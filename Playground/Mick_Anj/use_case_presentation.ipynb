{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**DELETE BEFORE PUBLISHING**_\n",
    "\n",
    "_This is a template also containing the style guide for use cases. The styling uses the use-case css when uploaded to the website, which will not be visible on your local machine._\n",
    "\n",
    "_Change any text marked with {} and delete any cells marked DELETE_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".usecase-title, .usecase-duration, .usecase-section-header {\n",
       "    padding-left: 15px;\n",
       "    padding-bottom: 8px;\n",
       "    padding-top: 8px;\n",
       "    padding-right: 15px;\n",
       "    background-color: #0f9295;\n",
       "    color: #fff;\n",
       "}\n",
       "\n",
       ".usecase-title {\n",
       "    font-size: 1.7em;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".usecase-authors, .usecase-level, .usecase-skill {\n",
       "    padding-left: 15px;\n",
       "    padding-bottom: 6px;\n",
       "    padding-top: 6px;\n",
       "    background-color: #baeaeb;\n",
       "    font-size: 1.4em;\n",
       "    color: #121212;\n",
       "}\n",
       "\n",
       ".usecase-level-skill  {\n",
       "    display: flex;\n",
       "}\n",
       "\n",
       ".usecase-level, .usecase-skill {\n",
       "    width: 50%;\n",
       "}\n",
       "\n",
       ".usecase-duration, .usecase-skill {\n",
       "    text-align: right;\n",
       "    padding-right: 15px;\n",
       "    padding-bottom: 6px;\n",
       "    font-size: 1.4em;\n",
       "}\n",
       "\n",
       ".usecase-section-header {\n",
       "    font-weight: bold;\n",
       "    font-size: 1.5em;\n",
       "}\n",
       "\n",
       ".usecase-subsection-header, .usecase-subsection-blurb {\n",
       "    font-weight: bold;\n",
       "    font-size: 1.2em;\n",
       "    color: #121212;\n",
       "}\n",
       "\n",
       ".usecase-subsection-blurb {\n",
       "    font-size: 1em;\n",
       "    font-style: italic;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DELETE BEFORE PUBLISHING\n",
    "# This is just here so you can preview the styling on your local machine\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".usecase-title, .usecase-duration, .usecase-section-header {\n",
    "    padding-left: 15px;\n",
    "    padding-bottom: 8px;\n",
    "    padding-top: 8px;\n",
    "    padding-right: 15px;\n",
    "    background-color: #0f9295;\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".usecase-title {\n",
    "    font-size: 1.7em;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".usecase-authors, .usecase-level, .usecase-skill {\n",
    "    padding-left: 15px;\n",
    "    padding-bottom: 6px;\n",
    "    padding-top: 6px;\n",
    "    background-color: #baeaeb;\n",
    "    font-size: 1.4em;\n",
    "    color: #121212;\n",
    "}\n",
    "\n",
    ".usecase-level-skill  {\n",
    "    display: flex;\n",
    "}\n",
    "\n",
    ".usecase-level, .usecase-skill {\n",
    "    width: 50%;\n",
    "}\n",
    "\n",
    ".usecase-duration, .usecase-skill {\n",
    "    text-align: right;\n",
    "    padding-right: 15px;\n",
    "    padding-bottom: 6px;\n",
    "    font-size: 1.4em;\n",
    "}\n",
    "\n",
    ".usecase-section-header {\n",
    "    font-weight: bold;\n",
    "    font-size: 1.5em;\n",
    "}\n",
    "\n",
    ".usecase-subsection-header, .usecase-subsection-blurb {\n",
    "    font-weight: bold;\n",
    "    font-size: 1.2em;\n",
    "    color: #121212;\n",
    "}\n",
    "\n",
    ".usecase-subsection-blurb {\n",
    "    font-size: 1em;\n",
    "    font-style: italic;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "_**DELETE BEFORE PUBLISHING**_\n",
    "\n",
    "## Style guide for use cases\n",
    "\n",
    "### Headers\n",
    "\n",
    "For styling within your markdown cells, there are two choices you can use for headers.\n",
    "\n",
    "1) You can use HTML classes specific to the use case styling:\n",
    "\n",
    "```<p class=\"usecase-subsection-header\">This is a subsection header.</p>```\n",
    "\n",
    "<p style=\"font-weight: bold; font-size: 1.2em;\">This is a subsection header.</p>\n",
    "\n",
    "```<p class=\"usecase-subsection-blurb\">This is a blurb header.</p>```\n",
    "\n",
    "<p style=\"font-weight: bold; font-size: 1em; font-style:italic;\">This is a blurb header.</p>\n",
    "\n",
    "\n",
    "2) Or if you like you can use the markdown header styles:\n",
    "\n",
    "```# for h1```\n",
    "\n",
    "```## for h2```\n",
    "\n",
    "```### for h3```\n",
    "\n",
    "```#### for h4```\n",
    "\n",
    "```##### for h5```\n",
    "\n",
    "## Plot colour schemes\n",
    "\n",
    "General advice:\n",
    "1. Use the same colour or colour palette throughout your notebook, unless variety is necessary\n",
    "2. Select a palette based on the type of data being represented\n",
    "3. Consider accessibility (colourblindness, low vision)\n",
    "\n",
    "#### 1) If all of your plots only use 1-2 colors use one of the company style colors:\n",
    "\n",
    "| Light theme | Dark Theme |\n",
    "|-----|-----|\n",
    "|<p style=\"color:#2af598;\">#2af598</p>|<p style=\"color:#08af64;\">#08af64</p>|\n",
    "|<p style=\"color:#22e4ac;\">#22e4ac</p>|<p style=\"color:#14a38e;\">#14a38e</p>|\n",
    "|<p style=\"color:#1bd7bb;\">#1bd7bb</p>|<p style=\"color:#0f9295;\">#0f9295</p>|\n",
    "|<p style=\"color:#14c9cb;\">#14c9cb</p>|<p style=\"color:#056b8a;\">#056b8a</p>|\n",
    "|<p style=\"color:#0fbed8;\">#0fbed8</p>|<p style=\"color:#121212;\">#121212</p>|\n",
    "|<p style=\"color:#08b3e5;\">#08b3e5</p>||\n",
    "\n",
    "\n",
    "#### 2) If your plot needs multiple colors, choose an appropriate palette using either of the following tutorials:\n",
    "- https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "- https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "\n",
    "#### 3) Consider accessibility as well.\n",
    "\n",
    "For qualitative plotting Seaborn's 'colorblind' palette is recommended. For maps with sequential or diverging it is recommended to use one of the Color Brewer schemes which can be previewed at https://colorbrewer2.org/.\n",
    "\n",
    "If you want to design your own colour scheme, it should use the same principles as Cynthia Brewer's research (with variation not only in hue but also, saturation or luminance).\n",
    "\n",
    "### References\n",
    "\n",
    "Be sure to acknowledge your sources and any attributions using links or a reference list.\n",
    "\n",
    "If you have quite a few references, you might wish to have a dedicated section for references at the end of your document, linked using footnote style numbers.\n",
    "\n",
    "You can connect your in-text reference by adding the number with a HTML link: ```<a href=\"#fn-1\">[1]</a>```\n",
    "\n",
    "and add a matching ID in the reference list using the ```<fn>``` tag: ```<fn id=\"fn-1\">[1] Author (Year) _Title_, Publisher, Publication location.</fn>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-title\">Small Area Population Growth & Active Transport Needs Analysis</div>\n",
    "\n",
    "<div class=\"usecase-authors\"><b>Authored by: </b>Angie Hollingworth and Mick Wiedermann</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-duration\"><b>Duration:</b> 90 mins</div>\n",
    "\n",
    "<div class=\"usecase-level-skill\">\n",
    "    <div class=\"usecase-level\"><b>Level: </b>Intermediate</div>\n",
    "    <div class=\"usecase-skill\"><b>Pre-requisite Skills: </b>Python</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Scenario</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a future resident of Melbourne, I want to live close to active and/or public transport routes. I prefer not to use my car in and around the city, where shall I live?\n",
    "- As a city council, we wish to increase the sustainability of our city and reduce the number of motor vehicles coming and going to lower emissions. What infrastructure investment will help achieve this goal?\n",
    "- As a city council, we wish to see our highest areas of active transport to identify where we could increase services for our residents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Exploratory Data Analysis Objectives</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals for this analysis (Part A) are: \n",
    "- Analyse population growth at the suburb level to quantify the speed of growth of each suburb relative to one another.   \n",
    "- Analyse the existing active transportation routes’ current demand and access relative to the forecast growth of the population.  \n",
    "- Identify key areas where active transportation routes could experience higher demand therefore may require additional infrastructure. \n",
    "\n",
    "Population Growth & Public Transport Needs Analysis (Part B) will extend this analysis and include public transport, trams, buses, and trains.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Strategic Benefits for the City of Melbourne</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This use case and analysis in conjunction with Part B, can help Melbourne City meet strategic and sustainability goals in the following ways: \n",
    "- Support discussions with infrastructure-related partners for the location of new or upgraded, public and active transportation routes to reduce the use of motorised vehicles in turn reducing emissions helping to meet the climate and biodiversity emergency objective.\n",
    "- Encouraging additional purpose-designed bike paths in heavy use areas can remove bicycles from the road and reduce the number of bike-related injuries helping to meet the safety and well-being objective. \n",
    "- Identify areas of higher active transport traffic (foot/bicycle etc) in comparison with predicted population growth to establish a use-case for more resources to encourage a greater use of active transport paths and or bike lanes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Why Inner-City Transport Routes Matter </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melbourne City is the first in Australia to make a [Voluntary Local Review (VLR) Declaration](https://www.melbourne.vic.gov.au/about-council/vision-goals/Pages/united-nations-sustainable-development-goals.aspx) which is a United Nations initiative for local and regional governments worldwide to formally commit to and report their local progress toward the seventeen Sustainable Development Goals.\n",
    "\n",
    "By examining the active and public transport routes and usage within Melbourne City, in conjunction with the population growth forecasts, we hope to identify areas with existing and projected increased demand for additional active and public transport routes. \n",
    "\n",
    "The hope is that by ensuring that the appropriate sustainable transport options are available and easily accessible, we would discourage the use of motorised vehicles within Melbourne City reducing emissions while creating a more sustainable city.   \n",
    "\n",
    "This will help Melbourne City to achieve two of the UN sustainability goals namely sustainable cities and communities, and climate action, along with a key strategic objective, the [climate and biodiversity emergency](https://www.melbourne.vic.gov.au/about-council/vision-goals/Pages/council-plan.aspx) objective which prioritises the reduction of emissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Data Requirments</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melbourne Open Data Datasets\n",
    "### Population Growth Forecast Data\n",
    "Our first and arguably the most important dataset for this analysis is the [*City of Melbourne Population Forecasts by Small Area 2021-2041*](https://data.melbourne.vic.gov.au/People/City-of-Melbourne-Population-Forecasts-by-Small-Ar/sp4r-xphj) from Melbourne Open Data which provides population forecasts by single year for 2021 to 2041. Prepared by SGS Economics and Planning (Jan-Jun 2021), forecasts are available for the municipality and small areas, as well as by gender and 5-year age groups.\n",
    "\n",
    "### Super Tuesday Bike Count data\n",
    "To understand the current volume of cyclists on bike paths, we can use the [*Annual Bike Counts (Super Tuesday)*](https://data.melbourne.vic.gov.au/Transport/Annual-Bike-Counts-Super-Tuesday-/uyp8-7ii8)) dataset. In summary, the dataset contains observed bike counts from sites across the city and is part of Australia’s biggest annual commuter bike path count dataset. Later datasets for Super Tuesday include greater information about the types of bike path user (walker, bike rider, gender etc)\n",
    "\n",
    "### Bike Path Geospatial data\n",
    "For this analysis we are looking solely at the Active transport routes. For a visual representation of the bike paths/routes, we require geospatial data. The following dataset [*Bicycle routes, including informal, on-road and off-road routes*](https://data.melbourne.vic.gov.au/Transport/Bicycle-routes-including-informal-on-road-and-off-/24aw-nd3i) contains information about each of the paths along with the geospatial data. \n",
    "\n",
    "## Other Datasets\n",
    "### Victorian Suburbs Geospatial Data\n",
    "In order to visualise our population forecasts as a map overlay, we need the geographical coordinates of the suburbs we'll be examining. For this we'll use the *VIC Suburb/Locality Boundaries - PSMA Administrative Boundaries GeoJSON* dataset from the Australian Government site [data.gov.au](https://data.gov.au/dataset/ds-dga-af33dd8c-0534-4e18-9245-fc64440f742e/distribution/dist-dga-d467c550-fdf0-480f-85ca-79a6a30b700b/details?q=). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Importing the data</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing our datasets, we shall first import the necessary libraries to support our exploratory data analysis and visualisation.\n",
    "\n",
    "The following are the core packages required for this analysis:\n",
    "\n",
    "- {List each non-standard package and why briefly why you're using it. No need to list commonly used packages like numpy, maths,os, time, pandas}\n",
    "- GeoPandas: Allows us to plot patial data and overlay that data on maps. \n",
    "- Folium: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For importing the data and using API\n",
    "from sodapy import Socrata\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import zipfile as zf\n",
    "import requests\n",
    "from io import BytesIO \n",
    "\n",
    "# Working with the data\n",
    "from shapely.geometry import Polygon, Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "# Visualisation\n",
    "from IPython.display import IFrame, display, HTML\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Turn off warnings for report purposes (enable for debugging)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the *Melbourne Open Data Portal* we must establish a connection using the sodapy library by specifying a domain, being the website domain where the data is hosted, and an application access token that can be requested from the City of Melbourne Open Data portal by registering [here](https://data.melbourne.vic.gov.au/signup)\n",
    "\n",
    "For this exercise, we will access the domain without an application token. Each dataset in the Melbourne Open Data Portal has a unique identifier that can be used to retrieve the dataset using the sodapy library.\n",
    "\n",
    "The *City of Melbourne Population Forecasts by Small Area 2021-2041* dataset unique identifier is *sp4r-xphj*. We will pass this identifier into the sodapy command below to retrieve this data placing it into a Pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n",
      "WARNING:urllib3.connection:Certificate did not match expected hostname: data.melbourne.vic.gov.au. Certificate: {'subject': ((('commonName', '*.opendatasoft.com'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '04341C9E5453D1712569FE50A35FA0795427', 'notBefore': 'Dec  1 21:14:26 2022 GMT', 'notAfter': 'Mar  1 21:14:25 2023 GMT', 'subjectAltName': (('DNS', '*.opendatasoft.com'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='data.melbourne.vic.gov.au', port=443): Max retries exceeded with url: /resource/sp4r-xphj.json?%24offset=0 (Caused by SSLError(SSLCertVerificationError(\"hostname 'data.melbourne.vic.gov.au' doesn't match '*.opendatasoft.com'\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m                 )\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0m_match_hostname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_hostname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_match_hostname\u001b[0;34m(cert, asserted_hostname)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mmatch_hostname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masserted_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCertificateError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mmatch_hostname\u001b[0;34m(cert, hostname)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnsnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         raise CertificateError(\"hostname %r \"\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0;34m\"doesn't match %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: (\"hostname 'data.melbourne.vic.gov.au' doesn't match '*.opendatasoft.com'\",)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='data.melbourne.vic.gov.au', port=443): Max retries exceeded with url: /resource/sp4r-xphj.json?%24offset=0 (Caused by SSLError(SSLCertVerificationError(\"hostname 'data.melbourne.vic.gov.au' doesn't match '*.opendatasoft.com'\")))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/s19df3pj3515r5_vrmr7rst80000gn/T/ipykernel_48070/2943875572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Population Forecast Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpopulation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_data_unique_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpopulation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_dataclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36mget_all\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dataset_identifier, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_empty_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         response = self._perform_request(\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='data.melbourne.vic.gov.au', port=443): Max retries exceeded with url: /resource/sp4r-xphj.json?%24offset=0 (Caused by SSLError(SSLCertVerificationError(\"hostname 'data.melbourne.vic.gov.au' doesn't match '*.opendatasoft.com'\")))"
     ]
    }
   ],
   "source": [
    "apptoken = os.environ.get(\"SODAPY_APPTOKEN\") # Anonymous App Token\n",
    "domain = \"data.melbourne.vic.gov.au\"\n",
    "client = Socrata(domain, apptoken)           # Open Dataset Connection\n",
    "pop_data_unique_identifier = 'sp4r-xphj'   \n",
    "\n",
    "# Population Forecast Data\n",
    "population_data = pd.DataFrame.from_dict(client.get_all(pop_data_unique_identifier))\n",
    "population_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use the same app token, domain, and client to import the remaining datasets described above from the Melbourne Open Data Portal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bike Path Geographical Data\n",
    "bike_geo_data_unique_identifier = 'hmuz-nz6m'\n",
    "bike_path_data_url = 'https://'+ domain +'/api/geospatial/'+ bike_geo_data_unique_identifier +'?method=export&format=GeoJSON'\n",
    "with urlopen(bike_path_data_url) as result:\n",
    "    BIKE_PATHS = json.load(result) \n",
    "\n",
    "# Bike Count Data\n",
    "bike_count_data_unique_identifier = 'uyp8-7ii8'\n",
    "BIKE_USAGE_COUNT = pd.DataFrame.from_dict(client.get_all(bike_count_data_unique_identifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last Dataset we need to import is the *Victorian Suburbs/Locality Boundaries* from the Australian Government site, data.gov.au, which is freely available for download via the below URL. As the data includes geometric data, we will also import this data into a GeoPandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suburb Geographical Data\n",
    "suburb_geo_data_url = ('https://data.gov.au/geoserver/vic-suburb-locality-boundaries-psma-administrative-'\n",
    "    + 'boundaries/wfs?request=GetFeature&typeName=ckan_af33dd8c_0534_4e18_9245_fc64440f742e&outputFormat=json')\n",
    "\n",
    "vic_suburb_data = gpd.read_file(suburb_geo_data_url)\n",
    "VIC_SUBURBS = vic_suburb_data[['vic_loca_2', 'geometry']]\n",
    "VIC_SUBURBS = VIC_SUBURBS.rename(columns={'vic_loca_2':'suburb'})\n",
    "\n",
    "VIC_SUBURBS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Cleaning and Preparing Our Data</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from reading the data descriptions and dictionaries from each of the datasets, most have data that we don’t require. In this section, we will extract the data we need and join various sets ready for analysis. \n",
    "\n",
    "### Population Forecast and Suburb Geospatial Data\n",
    "Starting with the population and suburb geospatial data, we need to remove suburbs from the geospatial data that are not included in our population forecast data. Also, we need to match the City of Melbourne suburbs by name which require a merge of certain fields and renaming of others. \n",
    "\n",
    "By passing the population dataset to the following two functions, we can clean the dataset to return only the columns (information) that we require whilst also formatting the column names and data types. The second function will return a new data frame with either the population totals by suburb for the year specified or all years if the boolean parameter is set to true.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pop_data(dataset):\n",
    "    \"\"\"\n",
    "    Filters and cleans the Population dataset returning a new pandas dataframe.\n",
    "    \n",
    "        dataset: The City of Melbourne Population Forecasts by Small Area 2021-2041. \n",
    "    \"\"\"\n",
    "    # Excluding the population totals & average age\n",
    "    dataset = dataset.loc[dataset['gender'] == 'Total']\n",
    "    dataset = dataset.loc[dataset['age'] != 'Average age']\n",
    "    \n",
    "    # Extract the colomns of interest into \"summary\" and rename geography.\n",
    "    summary = dataset[['geography', 'year', 'value', 'age']]\n",
    "    summary = summary.rename(columns={'geography':'suburb'})\n",
    "    \n",
    "    # Convert datatypes\n",
    "    summary = summary.astype({'year':int, 'value':float, 'suburb':'string'})\n",
    "\n",
    "    # Consolidating and updating suburb names to match the Geospatial data.\n",
    "    summary['suburb'] = summary['suburb'].replace(['Melbourne (CBD)', 'Melbourne (Remainder)'], ['Melbourne', 'Melbourne'])\n",
    "    summary['suburb'] = summary['suburb'].replace(['West Melbourne (Residential)'], ['West Melbourne'])\n",
    "    \n",
    "    # Removing unrequired data.  \n",
    "    summary.drop(summary.index[summary['suburb'] == 'West Melbourne (Industrial)'], inplace=True)\n",
    "    summary.drop(summary.index[summary['suburb'] == 'City of Melbourne'], inplace=True)\n",
    "    summary.drop(summary.index[summary['suburb'] == 'Port Melbourne'], inplace=True)\n",
    "    \n",
    "    # Sorting the data and resetting the indexes.\n",
    "    summary.sort_values(['suburb'], inplace = True)\n",
    "    summary = summary.reset_index(drop=True)\n",
    "    '''\n",
    "    n.b Port Melbourne Population count is very small and doesn't seem to be acurate. This jumps in subsequent years skewing the data,\n",
    "        for that reason we have chosen to exclude port melbourne from the analisys. \n",
    "    Melbourne (Remainder) and Melbourne (CBD) were combined to match the Geospatial data.\n",
    "    '''\n",
    "    summary = gpd.GeoDataFrame(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the initial dataset to contain only the information we require\n",
    "POPULATION_DATA = prepare_pop_data(population_data)\n",
    "\n",
    "POPULATION_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_by_year(grouped_data, year, all_years=False):\n",
    "    \"\"\"\n",
    "    Returns Geo DataFrame of the population by suburb of the year specified or all years if set to True.\n",
    "    \n",
    "        dataset: The prepared \"POPULATION_DATA\" dataset. \n",
    "        year:    The desired year for the population totals.\n",
    "        all_years (bool, optional): If True, will return a summary of all years.\n",
    "    \"\"\"\n",
    "    grouped_data = grouped_data[['suburb', 'year', 'value']]\n",
    "    \n",
    "    if all_years:\n",
    "        grouped_data = grouped_data.groupby(['suburb', 'year'])['value'].sum()\n",
    "        grouped_data = grouped_data.reset_index()\n",
    "    else:\n",
    "        grouped_data = grouped_data.loc[grouped_data['year'] == year]\n",
    "        grouped_data = grouped_data.groupby(['suburb', 'year'])['value'].sum()\n",
    "        grouped_data = grouped_data.reset_index()\n",
    "        grouped_data = grouped_data.rename(columns={'value': str(year)})\n",
    "        grouped_data = grouped_data.drop(columns=['year'])\n",
    "        grouped_data = grouped_data.astype({'suburb':'string'})\n",
    "    \n",
    "    grouped_data = gpd.GeoDataFrame(grouped_data)\n",
    "    \n",
    "    return grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dataset size with the pop_data_by_year function above.\n",
    "population_2022 = population_by_year(POPULATION_DATA, 2022)\n",
    "\n",
    "population_2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll utilise our reduced dataset above to filter and extract the suburbs of interest from the geospatial data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the suburbs of interest that match the population_data into \"target_suburbs\".\n",
    "target_subs = population_2022['suburb'].str.upper()\n",
    "\n",
    "# Locate the index of the target suburbs and store as a list in \"subs\"\n",
    "subs = [VIC_SUBURBS.index[VIC_SUBURBS['suburb']==sub].tolist()[0] for sub in target_subs]\n",
    "\n",
    "#Remove unwanted rows and keep data in geo dataframe format\n",
    "CITY_SUBURBS = VIC_SUBURBS.take(list(subs))\n",
    "CITY_SUBURBS.reset_index(drop=True, inplace = True)\n",
    "CITY_SUBURBS['suburb'] = CITY_SUBURBS['suburb'].str.title()\n",
    "CITY_SUBURBS = CITY_SUBURBS.astype({'suburb':'string'}) \n",
    "\n",
    "CITY_SUBURBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame now only includes suburbs found in our population dataset and their geospatial data (suburb outline). These two datasets can now be combined and used together in the analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bike Count and Path Geospatial Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick preview of our bike data shows that we have far more attributes (columns) than required. Let's extract the columns of interest for the most recent year and remove any NaN fields. Further, we need to update the data types for the extracted columns and combine the latitude and longitude into a single column containing a Point object which we’ll call geometry.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the output from the bike_count dataset that we imported earlier\n",
    "\n",
    "BIKE_USAGE_COUNT.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset to only include columns needed for the analysis & mapping.\n",
    "bike_usage_data =  gpd.GeoDataFrame(BIKE_USAGE_COUNT[['latitude','longitude','total','year', 'description']])\n",
    "\n",
    "# Drop missing data.\n",
    "bike_usage_data.dropna(inplace=True)   \n",
    "\n",
    "# Get a list of years that the bike counts were completed. Convert values to an integer data type. \n",
    "years = [int(x) for x in bike_usage_data['year'].unique()]\n",
    "\n",
    "# Extracting the most recent year of data.\n",
    "bike_usage_data = bike_usage_data.loc[bike_usage_data['year'] == str(max(years))]\n",
    "\n",
    "# Updating the data types.\n",
    "degs = ['latitude','longitude']\n",
    "for col in degs:\n",
    "    bike_usage_data[col] = bike_usage_data[col].astype(float)\n",
    "bike_usage_data['total'] = bike_usage_data['total'].astype(int)\n",
    "bike_usage_data.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# Combine the latitude & longitude values to a GeoPandas Point object in a new column for mapping.\n",
    "df_geometry = [Point(xy) for xy in zip(bike_usage_data['latitude'], bike_usage_data['longitude'])]\n",
    "\n",
    "# Create a GeoPandasDataFrame with the above cleaned dataset.\n",
    "BIKE_USAGE_DATA = gpd.GeoDataFrame(bike_usage_data, crs = 4326, geometry = df_geometry)\n",
    "\n",
    "BIKE_USAGE_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the bike path data is contained within a dictionary rather than a Pandas DataFrame. In this instance the dataset only contains relevant fields, so we have nothing to remove.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the attributes of the bike path data we imported earlier.  \n",
    "\n",
    "print(BIKE_PATHS.keys())\n",
    "BIKE_PATHS['features'][0]['properties'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Analysing Our Datasets</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Growth Analysis\n",
    "To begin our analysis, we'll first take a look at the population forecast data in five-year intervals. We will utilise our population by year function to build a new data frame and then plot the data as a clustered bar chart.\n",
    "\n",
    "First, let's confirm the first and last year contained within the population forecast dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the earliest and latest years for the population forecast in our dataset\n",
    "start_year = min(POPULATION_DATA['year'])\n",
    "final_year = max(POPULATION_DATA['year'])\n",
    "\n",
    "# In this case we expect it to be 2021 and 2041. Let's print to confirm\n",
    "print(f'Earliest year: {start_year},  Latest year: {final_year}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll extract a summary for the first year and every fifth year thereafter to create a new dataframe for our clustered bar chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting each year of interest.\n",
    "population_start = population_by_year(POPULATION_DATA, start_year)\n",
    "population_2026 = population_by_year(POPULATION_DATA, 2026)\n",
    "population_2031 = population_by_year(POPULATION_DATA, 2031)\n",
    "population_2036 = population_by_year(POPULATION_DATA, 2036)\n",
    "population_final = population_by_year(POPULATION_DATA, final_year)\n",
    "\n",
    "# Combining the years into a single dataframe.\n",
    "population_5y_intervals = population_start.merge(population_2026, left_on='suburb', right_on='suburb')\n",
    "population_5y_intervals = population_5y_intervals.merge(population_2031, left_on='suburb', right_on='suburb')\n",
    "population_5y_intervals = population_5y_intervals.merge(population_2036, left_on='suburb', right_on='suburb')\n",
    "population_5y_intervals = population_5y_intervals.merge(population_final, left_on='suburb', right_on='suburb')\n",
    "\n",
    "# Converting the dataframe to a GeoPandas dataframe. \n",
    "population_5y_intervals = gpd.GeoDataFrame(population_5y_intervals)\n",
    "population_5y_intervals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate our clustered bar chart and take our first visual representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "ax = population_5y_intervals.plot(x= 'suburb', kind='bar', stacked=False, figsize = (15,8))\n",
    "plt.style.use('seaborn-colorblind')\n",
    "ax.set_xticklabels(population_5y_intervals.suburb, rotation=45)\n",
    "plt.title('Population Counts by 5 Year Intervals\\n', size=14)\n",
    "plt.ylabel('Population Counts\\n', size=12)\n",
    "plt.xlabel('City of Melbourne Suburbs', size=12)\n",
    "ax.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the bar chart, we can see the concentration of Melbourne City's population is contained within the central suburb of Melbourne itself, which also includes the CBD. Most suburbs show quite a steep growth forecast apart from East Melbourne and South Yarra which seem quite flat in comparison.   \n",
    "\n",
    "Let's take a closer look at each or the suburbs forecast population change from the first to the last year included in the dataset so we can better see the change relative to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a summary of our population data using the \"population_by_year\" function. \n",
    "population_all = population_by_year(POPULATION_DATA, start_year, True)\n",
    "\n",
    "# Limiting the extraction to the first and last years as defined above.\n",
    "population_21_41 = population_all.loc[population_all['year'].isin([start_year, final_year])]\n",
    "\n",
    "# Finaly, plotting the distribution of the two years as a comparison. \n",
    "pop_dist = sns.displot(population_21_41, x='value', bins=10, hue='suburb', multiple='stack', col='year')\n",
    "pop_dist.set_axis_labels('Population', 'Count', size=13);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see the population for South Yarra and East Melbourne having minimal growth, both remaining under 10,000 residents while suburbs such as Melbourne, Southbank, North Melbourne, Docklands and Carlton, leaping ahead with what looks like a doubling of their populations.  \n",
    "\n",
    "Let’s see how this change looks overlayed on a map of Melbourne City. We can use a choropleth map to visualise the change between each suburb to determine if the forecast growth is taking place in a cluster or sporadically across Melbourne City. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will return a GeoPandas DataFrame with our population and suburb geospatial datasets joined with the population for the two years passed and the difference between the two years as both a number and percentage. We can then plot the change between any two given years as a number or percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_change(population_df, suburbs_df, year_1, year_2):\n",
    "    \"\"\"\n",
    "    Returns Geo DataFrame of the population by suburb of the years specified along\n",
    "    with the growth as a number and a percentage between the two years.\n",
    "    \n",
    "        population_df:   The prepared \"POPULATION_DATA\" dataframe. \n",
    "        suburbs_df:      The prepared \"CITY_SUBURBS\" dataframe.\n",
    "        year_1 & year_2: The years of interest.\n",
    "    \"\"\"\n",
    "    # Utilising the \"population_by_year\" function to prepare a dataframe for each year of interest.\n",
    "    start_year = population_by_year(population_df, year_1)\n",
    "    end_year = population_by_year(population_df, year_2)\n",
    "\n",
    "    # Join our above two dataframes based on \"suburb\"\n",
    "    combined =  start_year.merge(end_year, left_on='suburb', right_on='suburb')    \n",
    "    \n",
    "    # calculate % and # of changes within the two years for each suburb\n",
    "    combined['growth #'] = combined[list(end_year)[1]] - combined[list(start_year)[1]]\n",
    "    combined['growth %'] = round((combined[list(combined)[3]] / combined[list(start_year)[1]])*100,2)\n",
    "    \n",
    "    # Add the geometry (for mapping) from our CITY_SUBURBS dataframe\n",
    "    combined['geometry'] = suburbs_df['geometry']\n",
    "    \n",
    "    # Convert our standard pandas dataframe to a GeoPandas data frame for map rendering\n",
    "    combined = gpd.GeoDataFrame(combined)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looking at the total growth between the start_year and final_year.\n",
    "pop_diff_2021_2041 = population_change(POPULATION_DATA, CITY_SUBURBS, start_year, final_year)\n",
    "\n",
    "# Display the head for the new Data Frame with changes in population growth\n",
    "pop_diff_2021_2041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe that captures the total growth, let’s look at the change as a choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_change_20y = pop_diff_2021_2041.explore(column ='growth %', \n",
    "    tiles='CartoDB positron', zoom_start=13, cmap='winter')\n",
    "\n",
    "# Display our map\n",
    "pop_change_20y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the map above we can clearly see North Melbourne will outpace all others over the period captured by the data. The next question that comes to mind is if this forecast growth is consistent across all years. Lets’ look at the change at each 5-year interval, to see if the growth rate matches the above choropleth map.  \n",
    "\n",
    "To do this we'll write two small functions, one to return the map as above, and a second to allow us to examine the maps side by side. \n",
    "\n",
    "### Visualising Multiple Years of Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the change with a folium map layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_map(population_data, title = \"Melbourne City\", zoom = 12): \n",
    "    \"\"\"\n",
    "    Returns a choropleth folium map layer.\n",
    "        population_data:   A prepared dataframe returned from \"population_change\". \n",
    "        title:  Optional - The title of the growth map. Default = Melbourne City.\n",
    "        zoom:   Optional - The prefered starting zoom. Default = 12.\n",
    "    \"\"\"\n",
    "    geo_layer = population_data.explore(column ='growth %', tiles='CartoDB positron', \n",
    "                                        cmap='winter', name = title, zoom_start=zoom)\n",
    "    folium.LayerControl().add_to(geo_layer)\n",
    "    \n",
    "    return geo_layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a HTML frame to contain two maps in one visualisation for ease of comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_comparison_maps(map_1, map_2):\n",
    "    \"\"\"\n",
    "    Returns an iframe object to display both maps passed side by side for ease of comparison.\n",
    "        map_1 & map_2: Map layers prepared with the \"growth_map\" function. \n",
    "    \"\"\"\n",
    "    htmlmap = HTML('<iframe srcdoc=\"{}\" style=\"float:left; width: {}px; height: {}px; display:inline-block; width: 49%; margin: 0 auto; border: 2px solid #0f9295\"></iframe>'\n",
    "       '<iframe srcdoc=\"{}\" style=\"float:right; width: {}px; height: {}px; display:inline-block; width: 49%; margin: 0 auto; border: 2px solid #0f9295\"></iframe>'\n",
    "       .format(map_1.get_root().render().replace('\"', '&quot;'),400,400,\n",
    "               map_2.get_root().render().replace('\"', '&quot;'),400,400))\n",
    "\n",
    "    return (htmlmap) #display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some 5-year buckets to compare the rate of population growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the first 6 year bucket \n",
    "# # We know form above our earliest year in the dataset is 2021, and 2026\n",
    "# pop_change_1 = population_change(POPULATION_DATA, CITY_SUBURBS, 2021, 2026) \n",
    "\n",
    "# # Create our map layer using our new function above\n",
    "# map_2021_2026 = growth_map(pop_change_1, title = 'Population growth 2021-2026')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View our first map to make sure everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Population Growth 2021 - 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_2021_2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above 3 more time to cover our full 20 year dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our dataframe with the \"population_change\" function. \n",
    "pop_change_1 = population_change(POPULATION_DATA, CITY_SUBURBS, 2021, 2026)\n",
    "\n",
    "# Create our map layer using our new function above.\n",
    "map_2021_2026 = growth_map(pop_change_1, title = 'Population growth 2021-2026')\n",
    "\n",
    "# Repeating the above to capture the entire dataset.\n",
    "pop_change_2 = population_change(POPULATION_DATA, CITY_SUBURBS, 2026, 2031) \n",
    "map_2026_2031 = growth_map(pop_change_2, title = 'Population growth 2026-2031')\n",
    "\n",
    "pop_change_3 = population_change(POPULATION_DATA, CITY_SUBURBS, 2031, 2036) \n",
    "map_2031_2036 = growth_map(pop_change_3, title = 'Population growth 2031-2036')\n",
    "\n",
    "pop_change_4 = population_change(POPULATION_DATA, CITY_SUBURBS, 2036, 2041) \n",
    "map_2036_2041 = growth_map(pop_change_4, title = 'Population growth 2036-2041')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our two map comparison HTML elements\n",
    "growth_5y_10y = create_html_comparison_maps(map_2021_2026, map_2026_2031)\n",
    "growth_15y_20y = create_html_comparison_maps(map_2031_2036, map_2036_2041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Population Growth 6 year buckets, 2021- 2041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's view our maps side by side\n",
    "display(growth_5y_10y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above maps we can see that the highest % of growth is initially within the inner suburbs, Melbourne CBD, Southbank, Docklands, East Melbourne all at around 25% increase in population over a 5-year period. \n",
    "\n",
    "However, after a 10-year period we can see that that increase shifts Northwest to West Melbourne and North Melbourne with around a 30% increase in population. \n",
    "\n",
    "We can also see that the population growth in South Yarra is beginning to slow from 24% to 18%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(growth_15y_20y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead at the next 10-year buckets, we can see that South Yarra population growth not only comes to an almost halt for the 2031-2036 period but is predicted to decline by 2041 (in comparison to 2036). \n",
    "  \n",
    "North Melbourne remains a steady state of predicted increase at around 25% throughout the above 10-year period. \n",
    "\n",
    "Carlton drops from 27% growth, to 25% then down to less than 8% predicted growth. \n",
    "  \n",
    "There is enough change in the predicted growth rates above, for us to have one more look at our 20-year period of growth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same maps as our above datasets\n",
    "pop_change_5 = population_change(POPULATION_DATA, CITY_SUBURBS, 2021, 2041) \n",
    "map_2021_2041 = growth_map(pop_change_5, title = 'Population Difference (2021 - 2041)', zoom=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(map_2021_2041)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the prior four 5-year buckets, we can confirm that North Melbourne is predicted to have the highest increase in population growth at around 145% on the population totals in 2021. \n",
    "\n",
    "The initial inner cities that we saw with the initial population growth increases are all next in line at around a 100% increase (or double) in the residents of those suburbs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Transport Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisiting our bike data, lets have a look at what we have.\n",
    "print('Bike path usage:')\n",
    "print(BIKE_USAGE_DATA.keys(), type(BIKE_USAGE_DATA))\n",
    "\n",
    "print('\\nBike paths:')\n",
    "print(BIKE_PATHS.keys(), type(BIKE_PATHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having another look at bike usage, we can see that this has a lat, long pair for mapping.\n",
    "BIKE_USAGE_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our bike path data is in JSON format, so it doesn't print in a table. \n",
    "# We can see that we have a collection of lat,long pairs which make a path (features, geometry, coordinates)\n",
    "print(BIKE_PATHS['features'][0].keys())\n",
    "print(BIKE_PATHS['features'][0]['type'])\n",
    "print(BIKE_PATHS['features'][0]['properties'])\n",
    "print(BIKE_PATHS['features'][0]['geometry'].keys())\n",
    "print(BIKE_PATHS['features'][0]['geometry']['type'], BIKE_PATHS['features'][0]['geometry']['coordinates'][0][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To visualise this data, we need to be able to combine them so that we can see where the usage counts took place along the bike routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bike_path_types(bike_paths):\n",
    "    #Create an empty list to store our bike path types\n",
    "    bike_paths_types = []\n",
    "\n",
    "    # Iterate through the bike path data and look at features\n",
    "    for feature in bike_paths['features']:\n",
    "\n",
    "        # Set the route feature type to a variable\n",
    "        r_type = feature['properties']['type']\n",
    "\n",
    "        # check if the type is already in the list of unique featers (bike_paths_types)\n",
    "        if bike_paths_types.count(r_type) == 0:\n",
    "\n",
    "            # Add unique path type to list\n",
    "            bike_paths_types.append(feature['properties']['type'])\n",
    "    \n",
    "    return  bike_paths_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_path_type_list = get_bike_path_types(BIKE_PATHS)\n",
    "\n",
    "#let's look at the output\n",
    "print(bike_path_type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The function below will take the bike path JSON file, and return map layers for every type of bike path in the dataset (using the above function to get the list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_layer_with_path_features(bike_routes):    \n",
    "    '''Create Folium map layers with the different bike path types'''\n",
    "    \n",
    "    # We can use the above function to get a list of route types\n",
    "    bike_paths_types = get_bike_path_types(bike_routes)\n",
    "    \n",
    "    # Create a list of hex colours to match our theme that has enough values for all of our path types\n",
    "    colours = ['#056b8a', '#14a38e', '#2af598', '#08b3e5', '#08af64'] \n",
    "    \n",
    "    # Create an empty list to hold our map layers for each path type\n",
    "    map_routes = []\n",
    "    \n",
    "    # Loop through the bike path types (from above)\n",
    "    for i, route_type in enumerate(bike_paths_types): # We use enumerate to get the value and index of the loop\n",
    "        \n",
    "        # create a copy of the bike path json dataset\n",
    "        route_json = bike_routes.copy()\n",
    "        \n",
    "        # remove any features (bike routes) that do not match the route type for the loop\n",
    "        route_json['features'] = [path for path in route_json['features'] if path['properties']['type'] == route_type]\n",
    "        \n",
    "        # Add colour property to dataset\n",
    "        # for each feature in the new json file, add a __colour property and add the colour value [i]\n",
    "        for data in route_json['features']:\n",
    "            data['properties']['__colour'] = colours[i]\n",
    "        \n",
    "        # create the folium map layer for this route type\n",
    "        g = folium.GeoJson(\n",
    "                    route_json,# Our copy of our bike route data for this one path type\n",
    "                    name=f'{route_type}',     \n",
    "            \n",
    "                    # This is a lambda function for Folium that applies the colour label to each feature\n",
    "                    style_function=lambda x:{ \n",
    "                        \"color\": x[\"properties\"][\"__colour\"],\n",
    "                        \"fillColor\": x[\"properties\"][\"__colour\"],}, \n",
    "            \n",
    "                    # Add some features to popup when you hover over a point on the map\n",
    "                    tooltip=folium.features.GeoJsonTooltip(fields=['name','direction','type','notes'])\n",
    "                    )\n",
    "        \n",
    "        # add the map layer to the list of layers\n",
    "        map_routes.extend([g])\n",
    "        \n",
    "    return map_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the above function does to our json data before we use it in the next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_map_route_layers = create_map_layer_with_path_features(BIKE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the output from the above function, we can see we get a list of folium features (layers) for our map\n",
    "bike_map_route_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our bike path map layers, we need to create a map that has both the bike routes, and the usage counts so that we can visualise where people are mostly using the bikepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatMap(data, bikepath, city_map, colour = 'blue'):\n",
    "    '''Function for creating a Folium map with layers for the two bike datasets'''\n",
    "    \n",
    "    # Create a base map layer with our city map\n",
    "    m =  city_map.explore(name = 'Melbourne City Suburbs',style_kwds = dict(color= '#22e4ac', fillOpacity = 0.4, opacity = 0.4),\n",
    "                         zoom_start=13)\n",
    "    \n",
    "    #call the above function to create the coloured bike path layers  \n",
    "    bike_routes = create_map_layer_with_path_features(bikepath) \n",
    "    # Add each bike path type map layer to our base map\n",
    "    for r in bike_routes:\n",
    "        r.add_to(m)\n",
    "        \n",
    "    # Create Bike count heat map layer - this will allow us to see hot spots for usage\n",
    "    labels = data['description']\n",
    "    \n",
    "    #Convert data to list for heat map rendering\n",
    "    data = list(map(list, zip(data['latitude'], data['longitude'],data['total'])))        \n",
    "    # Add our formatted data as a heat map layer to our base map\n",
    "    HeatMap(data, name='Bike Counts').add_to(m)\n",
    "    \n",
    "    # Create markers with the bike count data so we can see what the \"heat\" relates to\n",
    "    for i, location in enumerate(data):\n",
    "        # From our table view abovem we know that [0] and [1] are our latitude and longitude values\n",
    "        folium.Marker(location=[location[0], location[1]],\n",
    "                      popup=f'<strong>{labels[i]}</strong>',\n",
    "                      tooltip=f'Bike Count: {location[2]}',\n",
    "                      # create an empty icon so that our map isn't cluttered\n",
    "                      icon = folium.DivIcon(html =f\"\"\"<div style=\"color: {colour};\">  </div>\"\"\")\n",
    "                     ).add_to(m)\n",
    "        \n",
    "    # Add layer control to switch on and off map features\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m # Map with layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above function to create our  map of layers with both sets of bike data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map object\n",
    "bike_map = draw_heatMap(BIKE_USAGE_DATA,BIKE_PATHS, CITY_SUBURBS, colour = 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise our data in a map\n",
    "bike_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring back our final map with all the years so that we can compare to the active routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_population_maps = create_html_comparison_maps(bike_map, map_2021_2041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_population_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our two maps, cooncentrating on North Melbourne (being the area with the projected highest populatioin growth), we can see that we had quite a few bike bike path counts with the highest values being where the on-road and informal bike paths interesect. This is often triple the usage on the roads.\n",
    "From our metadata (or data description) of our bike count data, we know that these values were counted within a 2 hour window. \n",
    "\n",
    "Which infers that there (at times) are over 1,000 people riding bikes through/ around the North Melbourne area. Not every bike user will have passed by every count station, so that number is most likely significantly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
