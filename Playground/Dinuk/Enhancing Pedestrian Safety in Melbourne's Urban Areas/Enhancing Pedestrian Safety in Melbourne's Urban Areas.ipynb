{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-title\">Enhancing Pedestrian Safety in Melbourne's Urban Areas</div>\n",
    "\n",
    "<div class=\"usecase-authors\"><b>Authored by: </b> Dinuk</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-duration\"><b>Duration:</b> {90} mins</div>\n",
    "\n",
    "<div class=\"usecase-level-skill\">\n",
    "    <div class=\"usecase-level\"><b>Level: </b>{Intermediate}</div>\n",
    "    <div class=\"usecase-skill\"><b>Pre-requisite Skills: </b>{Python, Machine Learning }</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\r\n",
    "The main goal of this project is to analyze traffic, pedestrian data, and weather conditions to identify patterns and factors that contribute to pedestrian safety. This analysis will help in enhancing safety measures and improving walking conditions.\r\n",
    "\r\n",
    "## Acceptance Criteria\r\n",
    "\r\n",
    "### Data Collection and Integration\r\n",
    "- **Sources**: The system must integrate data from various sources including:\r\n",
    "  - Weather conditions (temperature, UV index, rainfall).\r\n",
    "  - Pedestrian counts.\r\n",
    "  - Specific geographic locations.\r\n",
    "  - Detailed topographical data to assess the steepness of pedestrian paths.\r\n",
    "- **Timeliness**: Data should be updated over time to reflect the most current information available, ideally covering the past several months.\r\n",
    "\r\n",
    "### Data Analysis and Reporting\r\n",
    "- **Regression Analysis**: Implement a regression model to understand how various factors, such as weather conditions and specific locations, impact pedestrian safety.\r\n",
    "- **Correlation Analysis**: Use correlation matrices to identify variables that are highly correlated to address potential issues of multicollinearity.\r\n",
    "- **Pathway Calculation**: Develop algorithms to calculate the safest and most efficient pathways, minimizing steepness and exposure to potential hazards.\r\n",
    "\r\n",
    "### Route Optimization and Mapping\r\n",
    "- **GIS Technology**: Utilize Geographic Information Systems technology to map out optimized safety routes based on model findings.\r\n",
    "- **Alternative Routes**: Provide alternative routes that balance steepness with environmental and urban factors, catering to personal preferences.\r\n",
    "\r\n",
    "### Model Optimization\r\n",
    "- **Dimensionality Reduction**: Apply PCA to manage data efficiency and complexity.\r\n",
    "- **Regularization Methods**: Incorporate Ridge or Lasso regularization to handle multicollinearity and improve model performance.\r\n",
    "- **Feature Selection**: Develop a feature selection strategy to eliminate redundant or irrelevant features to enhance model accuracy.\r\n",
    "\r\n",
    "### Visualization and Decision Support\r\n",
    "- **Visualization Dashboard**: Develop a dashboard to display traffic and pedestrian safety metrics across different times and locations, including heatmaps to highlight key correlations and trends.\r\n",
    "- **Interactive Map**: Create an interactive map or application that provides route recommendations based on model insights.\r\n",
    "\r\n",
    "### Feedback and Iteration\r\n",
    "- **Feedback Loops**: Implement feedback loops to monitor the outcomes of safety measures, re-analyze data, and refine models based on effectiveness.\r\n",
    "\r\n",
    "### Technical Notes\r\n",
    "- **Data Privacy and Security**: Ensure the privacy and security of data, especially with real-time data integration.\r\n",
    "- **Scalability**: Consider the scalability of the data processing infrastructure to handle increasing data volume.\r\n",
    "- **Data Accuracy**: Ensure that the steepness data is accurate and regularly updated to reflect current pathway conditions.\r\n",
    "- **Accessibility**: Consider the needs of all users, including those with disabilities, to ensure that routes are universally accessible.\r\n",
    ".lysis and feedback.\n",
    " it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this use case, I will have demonstrated a broad range of skills essential for data-driven urban planning and public safety enhancement. These include Data Integration, where I'll show the ability to merge and utilise data from diverse sources such as weather conditions, pedestrian counts, and geographic specifics in real-time or near-real-time. In Statistical Analysis and Modeling, I'll apply statistical techniques and regression models to dissect the impact of various environmental and urban factors on pedestrian safety, tackling issues like multicollinearity and data dimensionality using methods like PCA and regularisation.\n",
    "\r\n",
    "M  work in Geospatial Analysis willhighlight hm  proficiency with GIS technology, enabling you to assess andoptimisee pedestrian routes based on topographical data like route steepness. In the realm of Machine Learning and Predictive ModelingIou'll refine predictive models to anticipate pedestrian traffic patterns and identify risk factors, enhancing model accuracy through careful feature selection.\r\n",
    "\r\n",
    "Software Development skills will come into pyin developing interactive applications that advise users on safe pedestrian routes, integrating complex backend analytics with user-friendly interfaces. My focus on User-Centric Design and Feedback processes ensuresrat these tools are accessible and practical, incorporating user feedback for continuous improvemen\n",
    "\r\n",
    "Promanagement and collaboration skills will be crucial to coordinating with stakeholders, including government bodies and public safety organisations, and ons, effectively communicating technical findings to inform and shape polFinallyFinMy, your understanding of Ethical and Privacy Considerations ensures that all data handling is conducted with the utmost respect for privacy and compliance with legal standards, establishing solutions that are not only effective but also ethically sound and securndards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Introduction or background relating to problem</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modern urban environments, pedestrian safety is a crucial concern for city planners and public officials. As cities grow and traffic increases, the challenge of ensuring safe and accessible pedestrian pathways becomes increasingly complex. Addressing this issue requires a comprehensive understanding of the various factors that influence pedestrian safety, including geographic features, traffic patterns, and environmental conditions such as weather.\r\n",
    "\r\n",
    "The use of data-driven approaches to urban planning offers a powerful tool to enhance pedestrian safety. By integrating and analyzing data from diverse sources—such as weather stations for real-time weather conditions, traffic sensors for vehicle and pedestrian counts, and GIS data for detailed geographic and topographical information—planners can identify high-risk areas, predict potential safety issues, and implement effective interventions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet sets up the necessary libraries and configurations for my project on analyzing pedestrian safety data. It includes tools for data manipulation, visualization, and statistical analysis. I also configure caching for my data requests to enhance performance and set up environment variables to manage sensitive data securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "import openmeteo_requests\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from dotenv import load_dotenv\n",
    "from folium.plugins import HeatMap\n",
    "from IPython.display import HTML\n",
    "from io import StringIO\n",
    "from ipywidgets import interact, widgets\n",
    "from retry_requests import retry\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from shapely.geometry import shape\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import euclidean\n",
    "from shapely.geometry import Polygon, Point\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up requests cache\n",
    "requests_cache.install_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footpath Steepness dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, I'm utilizing environment variables to securely access an API key and fetch data regarding footpath steepness from an open data platform. I use a GET request to retrieve the data in CSV format, then load it into a pandas DataFrame for analysis. This approach ensures that my data analysis is based on the most current information available, aligning with my project's requirements to enhance pedestrian safety through informed decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'footpath-steepness'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "#GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    footpath_steepness = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(footpath_steepness.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footpath_steepness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microclimate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'microclimate-sensors-data'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "#GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    microclimate_data = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(microclimate_data.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " microclimate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedestrian monthly Counts per hour dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, I retrieve hourly pedestrian count data from an API, which I then load into a pandas DataFrame for processing. This data is crucial for analysing pedestrian traffic patterns. I ensure the completeness of the time series by filling in any missing timestamps and replacing missing data with zeros. This preparation is essential for accurate analysis and modelling in my project aimed at enhancing pedestrian safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'pedestrian-counting-system-monthly-counts-per-hour'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    pedestrian_count = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "\n",
    "    pedestrian_count['timestamp'] = pd.to_datetime(pedestrian_count['timestamp'])\n",
    "    all_hours = pd.date_range(start=pedestrian_count['timestamp'].min(), end=pedestrian_count['timestamp'].max(), freq='1H')\n",
    "    all_hours_df = pd.DataFrame({'timestamp': all_hours})\n",
    "    \n",
    "    # Merge with original DataFrame to fill in missing rows\n",
    "    pedestrian_count = pd.merge(all_hours_df, pedestrian_count, on='timestamp', how='left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    pedestrian_count.fillna(0, inplace=True)\n",
    "    \n",
    "    print(pedestrian_count)\n",
    "\n",
    "    print(pedestrian_count.sample(10, random_state=999)) # Test\n",
    "    \n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_count = (pedestrian_count == 0).sum()\n",
    "zero_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedestrian Counting System - Past Hour (counts per minute) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'pedestrian-counting-system-past-hour-counts-per-minute'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "#GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    pedestrian_count_min = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(pedestrian_count_min.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the minute and second are zer\n",
    "pedestrian_count_min['sensing_datetime'] = pd.to_datetime(pedestrian_count_min['sensing_datetime'])\n",
    "hourly_df = pedestrian_count_min[pedestrian_count_min['sensing_datetime'].dt.minute == 0]\n",
    "hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column to datetime\n",
    "pedestrian_count_min['sensing_datetimep'] = pd.to_datetime(pedestrian_count_min['sensing_datetime'])\n",
    "\n",
    "earliest_timestamp = pedestrian_count_min['sensing_datetime'].min()\n",
    "latest_timestamp = pedestrian_count_min['sensing_datetime'].max()\n",
    "\n",
    "print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "print(\"Latest Timestamp:\", latest_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedestrian Couting System Locations dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, I access pedestrian sensor location data from an open data API to enhance my analysis of pedestrian traffic patterns. After successfully fetching the data, I convert it into a pandas DataFrame to facilitate further analysis, such as mapping sensor locations using GIS technology. This process is critical for accurately determining the distribution of pedestrian traffic and planning safety measures effectively in my data-driven urban planning project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'pedestrian-counting-system-sensor-locations'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    pedestrian_sensor_locations = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(pedestrian_sensor_locations.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Street Names Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this snippet, I utilize API data to fetch a list of street names in CSV format, which I load into a pandas DataFrame. This information is essential for associating geographic and traffic data with specific street locations, allowing for a more granular analysis of pedestrian safety across different areas. This method ensures that my urban planning project effectively utilizes real-time data for decision-making and planning interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'street-names'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    street_names = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(street_names.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge pedestrian counts and locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, I merge two datasets: pedestrian count data and sensor location data, using a common key (`locationid` from the pedestrian count data and `location_id` from the sensor location data). This merged DataFrame enables me to analyze pedestrian counts in the context of their specific locations, which is crucial for spatial analysis in my project. After merging, I sort the data in descending order by the `timestamp` to ensure that the most recent data is easily accessible for timely analysis and reporting. This step is integral for maintaining an up-to-date understanding of pedestrian traffic patterns and optimizing safety measures accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_merged_data = pd.merge(hourly_df, pedestrian_sensor_locations, left_on='location_id', right_on='location_id', how='inner')\n",
    "pedestrian_merged_data\n",
    "\n",
    "# pedestrian_merged_data.sort_values(by='timestamp',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Find the earliest and latest timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I convert the 'timestamp' column of our pedestrian data to a datetime format for easier analysis. Then, I extract and display the earliest and latest timestamps to assess the temporal range of the data, ensuring that our analysis is timely and relevant for current urban planning needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column to datetime\n",
    "pedestrian_merged_data['sensing_datetime'] = pd.to_datetime(pedestrian_merged_data['sensing_datetime'])\n",
    "\n",
    "earliest_timestamp = pedestrian_merged_data['sensing_datetime'].min()\n",
    "latest_timestamp = pedestrian_merged_data['sensing_datetime'].max()\n",
    "\n",
    "print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "print(\"Latest Timestamp:\", latest_timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data by date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Select the time duration here !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the start and end dates for filtering\n",
    "# start_date = pd.to_datetime('2024-02-18').date()\n",
    "# end_date = pd.to_datetime('2024-03-18').date()\n",
    "\n",
    "# # Extract date from 'timestamp' column\n",
    "# pedestrian_merged_data['date_only'] = pedestrian_merged_data['timestamp'].dt.date\n",
    "\n",
    "# # Dictionary to store filtered DataFrames\n",
    "# filtered_data_dfs = {}\n",
    "\n",
    "# # Iterate over dates in 7-day intervals and filter data\n",
    "# current_date = start_date\n",
    "# week_number = 1\n",
    "# while current_date <= end_date:\n",
    "#     # Define the end date of the current 7-day period\n",
    "#     period_end_date = current_date + pd.Timedelta(days=6)  # 6 days later\n",
    "    \n",
    "#     # Define the name for the DataFrame\n",
    "#     month_name = current_date.strftime('%B')\n",
    "#     df_name = f\"{month_name}_{week_number}\"\n",
    "    \n",
    "#     # Filter the combined data DataFrame by date range\n",
    "#     filtered_data_dfs[df_name] = pedestrian_merged_data[\n",
    "#         (pedestrian_merged_data['date_only'] >= current_date) & \n",
    "#         (pedestrian_merged_data['date_only'] <= period_end_date)\n",
    "#     ]\n",
    "    \n",
    "#     # Move to the next 7-day period\n",
    "#     current_date += pd.Timedelta(days=7)\n",
    "#     week_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the end date as today\n",
    "end_date = pd.Timestamp.today().date()\n",
    "\n",
    "# Define the start date as one month before the end date\n",
    "start_date = (pd.Timestamp.today() - pd.DateOffset(months=1)).date()\n",
    "\n",
    "# Extract date from 'timestamp' column\n",
    "pedestrian_merged_data['date_only'] = pedestrian_merged_data['sensing_datetime'].dt.date\n",
    "\n",
    "# Filter the combined data DataFrame by the last month\n",
    "filtered_data_last_month = pedestrian_merged_data[\n",
    "    (pedestrian_merged_data['date_only'] >= start_date) & \n",
    "    (pedestrian_merged_data['date_only'] <= end_date)\n",
    "]\n",
    "\n",
    "# Display the filtered data for the last month\n",
    "print(\"Filtered data for the last month:\")\n",
    "print(filtered_data_last_month.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_last_month.rename(columns={'sensing_datetime': 'timestamp'}, inplace=True)\n",
    "filtered_data_last_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make keys for every week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- In this snippet, I refine my analysis by ensuring that the keys representing weekly data segments are strictly within the specified date range. I loop through the dictionary keys, parse out the week's start date, and check if this week falls within the allowed date range. This process ensures that I only consider relevant data for further analysis, which is crucial for the accuracy and relevance of my findings in understanding pedestrian traffic patterns and planning safety interventions effectively.\n",
    "\r\n",
    "\r",
    " -->\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define an empty list to store keys within the date range\n",
    "# keys_within_date_range = []\n",
    "\n",
    "# # Iterate over the keys of filtered_data_dfs and select only those keys within the specified date range\n",
    "# for key in filtered_data_dfs.keys():\n",
    "#     month, week = key.split('_')\n",
    "#     week_start_date = datetime.datetime.strptime(f\"{month} {week.split()[0]} {start_date.year}\", '%B %d %Y').date()\n",
    "#     week_end_date = week_start_date + datetime.timedelta(days=6)\n",
    "#     if week_start_date >= start_date and week_end_date <= end_date:\n",
    "#         keys_within_date_range.append(key)\n",
    "\n",
    "# # Now, keys_within_date_range contains the keys representing month names and week numbers that fall within the specified date range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--In this code, I create a DataFrame from the keys of the dictionary that holds our filtered pedestrian data for each week. By converting these keys into a DataFrame with a single column named 'Keys', I can easily view and manage the identifiers for each data segment. This step facilitates organization and accessibility, allowing me to keep track of the different time periods analyzed in our study of pedestrian safety and traffic patterns-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Get all keys from the filtered_data_dfs dictionary\n",
    "# all_keys = list(filtered_data_dfs.keys())\n",
    "\n",
    "# # Create a DataFrame with the keys\n",
    "# keys_df = pd.DataFrame(all_keys, columns=['Keys'])\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(keys_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary used to store weekly DataFrame accessed by the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--In this code snippet, I iterate over each key listed in the `keys_df` DataFrame to retrieve and store each weekly segment of pedestrian data into a new dictionary called `week_dataframes`. This step efficiently organizes the weekly data into an easily accessible format, allowing for streamlined analysis and manipulation of each week's data as needed in the project. This method is particularly useful for comparing traffic patterns over successive weeks or applying specific analyses to individual time segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_dataframes = {}\n",
    "\n",
    "# # Loop through each key in the DataFrame keys_df\n",
    "# for index, row in keys_df.iterrows():\n",
    "#     week_key = row['Keys']\n",
    "    \n",
    "#     # Access the corresponding DataFrame from filtered_data_dfs using the week_key\n",
    "#     weekly_dataframe = filtered_data_dfs[week_key]\n",
    "    \n",
    "#     # Store the DataFrame in a new dictionary for easy access\n",
    "#     week_dataframes[week_key] = weekly_dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unwanted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code adjusts the structure of a weekly DataFrame by selecting and ordering important columns related to timestamps, location details, and pedestrian counts. This streamlined format enhances the analysis of pedestrian traffic patterns, which is essential for improving urban safety measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_dataframes[week_key] = week_dataframes[week_key].reindex(columns=['sensing_datetimep','location_id', 'latitude', 'longitude', 'direction_1_x', 'direction_2_x', 'total_of_directions', 'direction_1_y', 'direction_2_y', 'date_only'])\n",
    "# week_dataframes[week_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly Date Filtered Pedestrian data with location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty dictionary to store the DataFrames\n",
    "# filtered_pedestrian_merged_data = {}\n",
    "\n",
    "# # Iterate through each row in keys_df\n",
    "# for index, row in keys_df.iterrows():\n",
    "#     # Get the key value from the current row\n",
    "#     selected_key = row['Keys']\n",
    "    \n",
    "#     # Use the selected key to access the corresponding DataFrame from filtered_data_dfs\n",
    "#     filtered_pedestrian_merged_data[selected_key] = filtered_data_dfs[selected_key]\n",
    "\n",
    "# # Now you have a dictionary of DataFrames where keys are the keys from keys_df\n",
    "# # and values are corresponding DataFrames from filtered_data_dfs\n",
    "\n",
    "# # You can access each DataFrame using the key, for example:\n",
    "# filtered_pedestrian_merged_data[selected_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request climate data through API using latitude and longitude data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code extracts the first values of latitude and longitude from the pedestrian sensor locations dataset, storing them as variables. This step is typically used to set a reference point or initial map focus when visualizing geographic data related to pedestrian movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_latitude = pedestrian_sensor_locations['latitude'].values[0]\n",
    "pedestrian_longitude = pedestrian_sensor_locations['longitude'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet is designed to fetch and process climate data for multiple pedestrian sensor locations using the Open-Meteo API. It starts by setting up a cached and retry-enabled session to handle API requests efficiently. A function `get_climate_data` is defined to retrieve hourly climate data, including temperature, humidity, precipitation, and UV index, for given latitude and longitude coordinates. This data is then organized into a DataFrame. \r\n",
    "\r\n",
    "For each pedestrian sensor location, the function is called to gather climate data, which is then appended to a list. Finally, all individual DataFrames are concatenated into one comprehensive DataFrame, which is adjusted to rename the 'date' column to 'timestamp' for consistency with other data elements in the project. This integration enables a detailed analysis of how weather conditions correlate with pedestrian movement patterns, enhancing the project's insights into pedestrian safety under various environmental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def get_climate_data(latitude, longitude):\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": pedestrian_latitude,\n",
    "        \"longitude\": pedestrian_longitude,\n",
    "        \"current\": \"relative_humidity_2m\",\n",
    "        \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"showers\", \"weather_code\", \"uv_index\"],\n",
    "        \"past_days\": 92\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "\n",
    "    # Process hourly data\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_showers = hourly.Variables(4).ValuesAsNumpy()\n",
    "    hourly_weather_code = hourly.Variables(5).ValuesAsNumpy()\n",
    "    hourly_uv_index = hourly.Variables(6).ValuesAsNumpy()\n",
    "\n",
    "    hourly_data = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"temperature_2m\": hourly_temperature_2m,\n",
    "        \"relative_humidity_2m\": hourly_relative_humidity_2m,\n",
    "        \"precipitation\": hourly_precipitation,\n",
    "        \"rain\": hourly_rain,\n",
    "        \"showers\": hourly_showers,\n",
    "        \"weather_code\": hourly_weather_code,\n",
    "        \"uv_index\": hourly_uv_index\n",
    "    }\n",
    "\n",
    "    hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "    return hourly_dataframe\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all climate dataframes\n",
    "all_climate_data = []\n",
    "\n",
    "# Iterate over each location and retrieve climate data\n",
    "for index, row in pedestrian_sensor_locations.iterrows():\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "   \n",
    "    climate_data = get_climate_data(latitude, longitude)\n",
    "    all_climate_data.append(climate_data)\n",
    "\n",
    "# Concatenate all climate dataframes into a single dataframe\n",
    "climate_data_combined = pd.concat(all_climate_data, ignore_index=True)\n",
    "\n",
    "# Print the combined climate data\n",
    "\n",
    "climate_data_combined = climate_data_combined.rename(columns={'date': 'timestamp'})\n",
    "climate_data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' to datetime with timezone\n",
    "climate_data_combined['timestamp'] = pd.to_datetime(climate_data_combined['timestamp']).dt.tz_convert('UTC')\n",
    "\n",
    "# Define the end date as today\n",
    "end_date = pd.Timestamp.now(tz='UTC')\n",
    "\n",
    "# Define the start date as one month before the end date\n",
    "start_date = end_date - pd.DateOffset(months=1)\n",
    "\n",
    "# Filter the DataFrame for the last month\n",
    "filtered_climate_data_last_month = climate_data_combined[\n",
    "    (climate_data_combined['timestamp'] >= start_date) & \n",
    "    (climate_data_combined['timestamp'] <= end_date)\n",
    "]\n",
    "\n",
    "# Display the filtered data for the last month\n",
    "filtered_climate_data_last_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamp' column to datetime if needed\n",
    "filtered_climate_data_last_month['timestamp'] = pd.to_datetime(filtered_climate_data_last_month['timestamp'])\n",
    "\n",
    "# Find the earliest and latest timestamps\n",
    "earliest_timestamp = filtered_climate_data_last_month['timestamp'].min()\n",
    "latest_timestamp = filtered_climate_data_last_month['timestamp'].max()\n",
    "\n",
    "print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "print(\"Latest Timestamp:\", latest_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge pedestrian dataset with climate dataset on timestamp and location data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet efficiently merges pedestrian data with corresponding climate data on a weekly basis, ensuring that both datasets align by timestamp, latitude, and longitude. The process begins by ensuring timestamps are standardized to UTC in both the climate data and weekly pedestrian dataframes. \n",
    "\n",
    "For each week, identified by `week_key`, it filters the climate data to match the exact date range of the pedestrian data, then merges them based on shared columns (timestamp, latitude, and longitude). The resulting merged DataFrame for each week is then stored in a dictionary, allowing easy access to combined data for detailed analysis. This setup is particularly useful for examining the impact of weather conditions on pedestrian traffic patterns, enhancing the ability to draw meaningful conclusions and implement targeted safety measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Assuming climate_data_combined is already loaded and in the correct format\n",
    "# filtered_climate_data_last_month['timestamp'] = pd.to_datetime(filtered_climate_data_last_month['timestamp'], utc=True)\n",
    "\n",
    "# # Dictionary to store merged dataframes, initialized from previous steps if any\n",
    "# merged_dataframes = {}\n",
    "\n",
    "# # Iterate over the keys from keys_df DataFrame\n",
    "# for index, row in keys_df.iterrows():\n",
    "#     week_key = row['Keys']\n",
    "#     week_df = week_dataframes[week_key]  # Access the corresponding DataFrame using the week_key from keys_df\n",
    "\n",
    "#     # Convert timestamps in the week DataFrame\n",
    "#     week_df.loc[:, 'timestamp'] = pd.to_datetime(week_df['timestamp'], utc=True)\n",
    "\n",
    "\n",
    "#     # Filter climate data to match the date range of week_df\n",
    "#     start_date = week_df['timestamp'].min()\n",
    "#     end_date = week_df['timestamp'].max()\n",
    "#     filtered_climate_data = filtered_climate_data_last_month[(filtered_climate_data_last_month['timestamp'] >= start_date) & (filtered_climate_data_last_month['timestamp'] <= end_date)]\n",
    "\n",
    "#     # Join the data on timestamp and location (latitude and longitude)\n",
    "#     merged_data = pd.merge(filtered_climate_data, week_df, on=['timestamp', 'latitude', 'longitude'])\n",
    "\n",
    "#     # Store the merged DataFrame in the dictionary using the week_key\n",
    "#     merged_dataframes[week_key] = merged_data\n",
    "\n",
    "# # Now merged_dataframes contains all the merged data for each week_key as per keys_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(filtered_climate_data_last_month, filtered_data_last_month, on=['timestamp', 'latitude', 'longitude'])\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This line of code retrieves the column names of the DataFrame associated with the key 'March_3' from the dictionary merged_dataframes. This operation is useful for verifying the structure of the merged data, ensuring that all relevant columns from both the pedestrian and climate datasets have been successfully combined for the specified week. This helps in assessing the completeness of the data integration for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_dataframes['March_3'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code restructures the DataFrame corresponding to the last referenced week (`week_key`) by reordering and ensuring the presence of specific columns. It focuses on key geographical identifiers like latitude and longitude, timestamp, and location ID, alongside pedestrian traffic directions and various climate data elements such as temperature, humidity, precipitation levels, and UV index.\r\n",
    "\r\n",
    "This reorganization enhances clarity and ensures that all vital data aspects are readily accessible for analysis. The reindexed DataFrame allows for a more streamlined and focused approach when analyzing the interactions between pedestrian movement patterns and weather conditions, which is essential for developing effective pedestrian safety measures and urban planning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dataframes[week_key] = merged_dataframes[week_key].reindex(columns=['latitude', 'longitude','timestamp','location_id', 'direction_1_x', 'direction_2_x', 'total_of_directions', 'direction_1_y', 'direction_2_y','temperature_2m',\n",
    "#        'relative_humidity_2m', 'precipitation', 'rain', 'showers','weather_code', 'uv_index', ]) \n",
    "# merged_dataframes[week_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.reindex(columns=['latitude', 'longitude','timestamp','location_id', 'direction_1_x', 'direction_2_x', 'total_of_directions', 'direction_1_y', 'direction_2_y','temperature_2m',\n",
    "       'relative_humidity_2m', 'precipitation', 'rain', 'showers','weather_code', 'uv_index', ]) \n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dataframes[week_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest and latest timestamps\n",
    "earliest_timestamp = merged_data['timestamp'].min()\n",
    "latest_timestamp = merged_data['timestamp'].max()\n",
    "\n",
    "print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "print(\"Latest Timestamp:\", latest_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys_df.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_count = (merged_dataframes[week_key] == 0).sum()\n",
    "# zero_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a base map centered around Melbourne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet creates an interactive map centered on Melbourne using Folium, a Python library ideal for geographic visualizations. The map is initially set to a specific zoom level to provide a detailed view of the city. It then processes the DataFrame for the week labeled 'January_4' from your merged data collections, filtering out duplicate entries for latitude and longitude to ensure each location is marked uniquely on the map.\n",
    "\n",
    "Markers are added to the map for each unique coordinate, pinpointing the exact locations of pedestrian sensors. This visualization helps in understanding the geographic distribution of pedestrian traffic and can be crucial for identifying areas that may require additional safety measures. Although the code to save the map as an HTML file is commented out, executing that line would allow you to share or view the map independently of the Python environment. This map offers a powerful tool for both presentation and further analysis of pedestrian safety and traffic patterns in Melbourne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# melbourne_map = folium.Map(location=[-37.8136, 144.9631], zoom_start=15)\n",
    "\n",
    "# # Filter out duplicate latitude and longitude coordinates\n",
    "# unique_coordinates = merged_dataframes['March_3'][['latitude', 'longitude']].drop_duplicates().values.tolist()\n",
    "\n",
    "# # Add unique coordinates as markers on the map\n",
    "# for lat, lon in unique_coordinates:\n",
    "#     folium.Marker(location=[lat, lon]).add_to(melbourne_map)\n",
    "\n",
    "# # Save the map to an HTML file\n",
    "# # melbourne_map.save(\"melbourne_map_with_unique_coordinates.html\")\n",
    "# melbourne_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_map = folium.Map(location=[-37.8136, 144.9631], zoom_start=15)\n",
    "\n",
    "# Filter out duplicate latitude and longitude coordinates\n",
    "unique_coordinates = merged_data[['latitude', 'longitude']].drop_duplicates().values.tolist()\n",
    "\n",
    "# Add unique coordinates as markers on the map\n",
    "for lat, lon in unique_coordinates:\n",
    "    folium.Marker(location=[lat, lon]).add_to(melbourne_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "# melbourne_map.save(\"melbourne_map_with_unique_coordinates.html\")\n",
    "melbourne_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total of Directions Heat Map for selected date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code enables interactive visualization of pedestrian traffic data on a Melbourne map. Users can select a date, and a heatmap reflecting pedestrian traffic for that day is dynamically generated and displayed. The setup uses a date picker widget for easy date selection and a function to update the map accordingly, enhancing the analysis and planning of urban pedestrian safety measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to update the map based on the selected date\n",
    "# def update_map(selected_date):\n",
    "#     selected_day_df = merged_dataframes[week_key][merged_dataframes[week_key]['timestamp'].dt.date == selected_date]\n",
    "#     pedestrian_data = selected_day_df[['latitude', 'longitude', 'total_of_directions']].values.tolist()\n",
    "    \n",
    "#     # Create base map centered around Melbourne\n",
    "#     melbourne_map = folium.Map(location=[-37.8136, 144.9631], zoom_start=16)\n",
    "\n",
    "#     # Add heatmap layer using pedestrian data\n",
    "#     HeatMap(pedestrian_data).add_to(melbourne_map)\n",
    "\n",
    "#     # Save the map as HTML\n",
    "#     # melbourne_map.save(\"melbourne_heatmap.html\")\n",
    "#     display(melbourne_map)\n",
    "\n",
    "# # Create a widget to select the date\n",
    "# date_picker = widgets.DatePicker(description='Select Date', disabled=False)\n",
    "\n",
    "# # Display the time range\n",
    "# print(\"Time Range:\")\n",
    "# print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "# print(\"Latest Timestamp:\", latest_timestamp)\n",
    "\n",
    "# # Display the widget and the interactive map\n",
    "# #display(date_picker)\n",
    "# interact(update_map, selected_date=date_picker);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the map based on the selected date\n",
    "def update_map(selected_date):\n",
    "    selected_day_df = merged_data[merged_data['timestamp'].dt.date == selected_date]\n",
    "    pedestrian_data = selected_day_df[['latitude', 'longitude', 'total_of_directions']].values.tolist()\n",
    "    \n",
    "    # Create base map centered around Melbourne\n",
    "    melbourne_map = folium.Map(location=[-37.8136, 144.9631], zoom_start=16)\n",
    "\n",
    "    # Add heatmap layer using pedestrian data\n",
    "    HeatMap(pedestrian_data).add_to(melbourne_map)\n",
    "\n",
    "    # Save the map as HTML\n",
    "    # melbourne_map.save(\"melbourne_heatmap.html\")\n",
    "    display(melbourne_map)\n",
    "\n",
    "# Create a widget to select the date\n",
    "date_picker = widgets.DatePicker(description='Select Date', disabled=False)\n",
    "\n",
    "# Display the time range\n",
    "print(\"Pick a date last 30 days:\")\n",
    "\n",
    "# Display the widget and the interactive map\n",
    "#display(date_picker)\n",
    "interact(update_map, selected_date=date_picker);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HeatMap with Climate Data for selected date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This updated function enhances a Melbourne map by allowing users to select a date and dynamically view heatmaps of both pedestrian traffic and various climate conditions like humidity and precipitation. It features a base map setup, multiple heatmap layers for climate and pedestrian data, and layer control for toggling visibility. The code also handles data cleaning by dropping missing values and utilizes a date picker widget for easy date selection, improving the utility and interactivity of the map for urban planning analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to update the map based on the selected date\n",
    "def update_map(selected_date):\n",
    "    # Filter data for the selected date\n",
    "    selected_day_df = merged_data[merged_data['timestamp'].dt.date == selected_date]\n",
    "    \n",
    "    # Extract pedestrian data\n",
    "    pedestrian_data = selected_day_df[['latitude', 'longitude', 'total_of_directions']].values.tolist()\n",
    "    \n",
    "    # Create base map centered around Melbourne\n",
    "    melbourne_map = folium.Map(location=[-37.8136, 144.9631], zoom_start=16)\n",
    "    \n",
    "    # Add heatmap layer for pedestrian data\n",
    "    HeatMap(pedestrian_data, name='Pedestrian Heatmap').add_to(melbourne_map)\n",
    "    \n",
    "    # Add climate data layers\n",
    "    climate_layers = {\n",
    "        'Relative Humidity 2m': 'relative_humidity_2m',\n",
    "        'Precipitation': 'precipitation',\n",
    "        'Rain': 'rain',\n",
    "        'Showers': 'showers',\n",
    "        'UV Index': 'uv_index'\n",
    "    }\n",
    "    \n",
    "    for layer_name, layer_column in climate_layers.items():\n",
    "        climate_data = selected_day_df[['latitude', 'longitude', layer_column]].values.tolist()\n",
    "        HeatMap(climate_data, name=layer_name).add_to(melbourne_map)\n",
    "\n",
    "    # Add layer control\n",
    "    folium.LayerControl().add_to(melbourne_map)\n",
    "\n",
    "    # melbourne_map.save(\"melbourne_heatmap.html\")\n",
    "    display(melbourne_map)\n",
    "\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "# Get unique dates from the DataFrame\n",
    "unique_dates = merged_data['timestamp'].dt.date.unique()\n",
    "\n",
    "# Create a widget to select the date\n",
    "date_picker = widgets.DatePicker(description='Select Date', disabled=False)\n",
    "\n",
    "\n",
    "# Display the time range\n",
    "print(\"Time Range:\")\n",
    "print(\"Earliest Timestamp:\", earliest_timestamp)\n",
    "print(\"Latest Timestamp:\", latest_timestamp)\n",
    "\n",
    "# Display the widget and the interactive map\n",
    "#display(date_picker)\n",
    "interact(update_map, selected_date=date_picker);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code calculates and displays summary statistics for the missing values (NaN) across all columns in a specified weekly DataFrame. It helps in assessing the extent of missing data, which is crucial for planning data cleaning and preprocessing steps effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics of NaN values\n",
    "nan_dispersion = merged_data.isnull().sum().describe()\n",
    "print(nan_dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NaN values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet removes rows with missing values from a specified weekly DataFrame and then calculates summary statistics for the cleaned data. These statistics provide insights into the central tendency, dispersion, and shape of the dataset’s distribution, aiding in data analysis and decision-making. The summary is then printed, offering a detailed overview of the available data’s characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop NaN values\n",
    "# cleaned_df = merged_dataframes[week_key].dropna()\n",
    "\n",
    "# # Summary statistics of the available data\n",
    "# summary = cleaned_df.describe()\n",
    "\n",
    "# # Print the summary statistics\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "cleaned_df = merged_data.dropna()\n",
    "\n",
    "# Summary statistics of the available data\n",
    "summary = cleaned_df.describe()\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Traffic Volume for each week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet creates a series of bar charts representing the average pedestrian traffic volume for two directions at different hours of the day, each chart corresponding to a specific week. The setup includes:\n",
    "\n",
    "1. Subplot Configuration: It arranges the plots in a 3x3 grid, making comparing traffic patterns across different weeks easier.\n",
    "2. Data Processing: For each weekly dataset, the code converts the 'timestamp' to datetime, extracts the hour, and calculates the mean traffic volume per hour for both directions.\n",
    "3. Visualization: It plots these averages using bar charts within the subplots. Each subplot is clearly labeled with the hour and mean traffic volume, and includes a legend to distinguish between the two directions.\n",
    "4. Display Enhancements: The layout is adjusted for clarity and to prevent overlap, ensuring that each plot is easily readable.\n",
    "\n",
    "This visual representation provides a detailed comparison of pedestrian traffic patterns, allowing for insights into peak traffic times and variations across weeks, which is valuable for traffic management and urban planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the grid size for the subplots\n",
    "# rows, cols = (3, 2)\n",
    "# fig, axs = plt.subplots(rows, cols, figsize=(18, 12))  \n",
    "\n",
    "# # Flatten the array of axes for easy iteration\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# # Loop through each key in the DataFrame and plot the graphs\n",
    "# for i, (index, row) in enumerate(keys_df.iterrows()):\n",
    "#     week_key = row['Keys']\n",
    "    \n",
    "#     # Convert 'timestamp' column to datetime format\n",
    "#     merged_dataframes[week_key]['timestamp'] = pd.to_datetime(merged_dataframes[week_key]['timestamp'])\n",
    "#     # Extract hour from timestamp\n",
    "#     merged_dataframes[week_key]['hour'] = merged_dataframes[week_key]['timestamp'].dt.hour\n",
    "#     # Group data by hour and calculate mean traffic volume for both directions\n",
    "#     hourly_traffic_dir1 = merged_dataframes[week_key].groupby('hour')['direction_1_x'].mean()\n",
    "#     hourly_traffic_dir2 = merged_dataframes[week_key].groupby('hour')['direction_2_x'].mean()\n",
    "\n",
    "#     # Check if we haven't exceeded the number of subplots\n",
    "#     if i < rows * cols:\n",
    "#         ax = axs[i]\n",
    "#         # Plot bar chart for direction 1 on the ith subplot\n",
    "#         ax.bar(hourly_traffic_dir1.index, hourly_traffic_dir1.values, color='skyblue', label='Direction 1')\n",
    "#         # Plot bar chart for direction 2 on the ith subplot\n",
    "#         ax.bar(hourly_traffic_dir2.index, hourly_traffic_dir2.values, color='lightgreen', alpha=0.5, label='Direction 2')\n",
    "#         ax.set_xlabel('Hour of the Day')\n",
    "#         ax.set_ylabel('Mean Traffic Volume')\n",
    "#         ax.set_title(f'Mean Traffic Volume by Hour for {week_key}')\n",
    "#         ax.legend()\n",
    "#         ax.set_xticks(hourly_traffic_dir1.index)\n",
    "\n",
    "# # Adjust the layout\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid size for the subplots\n",
    "rows, cols = (3, 2)\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(18, 12))  \n",
    "\n",
    "# Flatten the array of axes for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each key in the DataFrame and plot the graphs\n",
    "for i, (index, row) in enumerate(keys_df.iterrows()):\n",
    "    week_key = row['Keys']\n",
    "    \n",
    "    # Convert 'timestamp' column to datetime format\n",
    "    merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "    # Extract hour from timestamp\n",
    "    merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "    # Group data by hour and calculate mean traffic volume for both directions\n",
    "    hourly_traffic_dir1 = merged_data.groupby('hour')['direction_1_x'].mean()\n",
    "    hourly_traffic_dir2 = merged_data.groupby('hour')['direction_2_x'].mean()\n",
    "\n",
    "    # Check if we haven't exceeded the number of subplots\n",
    "    if i < rows * cols:\n",
    "        ax = axs[i]\n",
    "        # Plot bar chart for direction 1 on the ith subplot\n",
    "        ax.bar(hourly_traffic_dir1.index, hourly_traffic_dir1.values, color='skyblue', label='Direction 1')\n",
    "        # Plot bar chart for direction 2 on the ith subplot\n",
    "        ax.bar(hourly_traffic_dir2.index, hourly_traffic_dir2.values, color='lightgreen', alpha=0.5, label='Direction 2')\n",
    "        ax.set_xlabel('Hour of the Day')\n",
    "        ax.set_ylabel('Mean Traffic Volume')\n",
    "        ax.set_title(f'Mean Traffic Volume by Hour for {week_key}')\n",
    "        ax.legend()\n",
    "        ax.set_xticks(hourly_traffic_dir1.index)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Traffic Volume with Rain for each hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet processes and visualizes pedestrian traffic volume and rainfall data on an hourly basis for a specific week (`week_key`). It first converts timestamps to datetime format and extracts hours to group the data, calculating mean traffic volumes and rainfall for each hour. The visualization uses a dual-axis bar and line chart, with traffic volumes displayed as blue bars and rainfall represented by a red line. This setup enables a clear comparison to see how weather conditions, specifically rainfall, correlate with pedestrian traffic volumes throughout the day, providing valuable insights for urban planning and safety enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert 'timestamp' column to datetime format\n",
    "# merged_dataframes[week_key]['timestamp'] = pd.to_datetime(merged_dataframes[week_key]['timestamp'])\n",
    "\n",
    "# # Extract hour from timestamp\n",
    "# merged_dataframes[week_key]['hour'] = merged_dataframes[week_key]['timestamp'].dt.hour\n",
    "\n",
    "# # Group data by hour and calculate mean traffic volume and mean rain\n",
    "# hourly_data = merged_dataframes[week_key].groupby('hour').agg({'total_of_directions': 'mean', 'rain': 'mean'})\n",
    "\n",
    "# # Plot bar chart\n",
    "# fig, ax1 = plt.subplots()\n",
    "\n",
    "# # Bar for traffic volume\n",
    "# color = 'tab:blue'\n",
    "# ax1.set_xlabel('Hour of the Day')\n",
    "# ax1.set_ylabel('Mean Traffic Volume', color=color)\n",
    "# ax1.bar(hourly_data.index, hourly_data['total_of_directions'], color=color, alpha=0.7)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Create another y-axis for rain\n",
    "# ax2 = ax1.twinx()  \n",
    "# color = 'tab:red'\n",
    "# ax2.set_ylabel('Mean Rain', color=color)\n",
    "# ax2.plot(hourly_data.index, hourly_data['rain'], color=color, linestyle='-', marker='o')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Show plot\n",
    "# plt.title(f'Mean Traffic Volume and Rain by Hour  [{week_key}]')\n",
    "# plt.xticks(range(24))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "\n",
    "# Extract hour from timestamp\n",
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "\n",
    "# Group data by hour and calculate mean traffic volume and mean rain\n",
    "hourly_data = merged_data.groupby('hour').agg({'total_of_directions': 'mean', 'rain': 'mean'})\n",
    "\n",
    "# Plot bar chart\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Bar for traffic volume\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Hour of the Day')\n",
    "ax1.set_ylabel('Mean Traffic Volume', color=color)\n",
    "ax1.bar(hourly_data.index, hourly_data['total_of_directions'], color=color, alpha=0.7)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create another y-axis for rain\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Mean Rain', color=color)\n",
    "ax2.plot(hourly_data.index, hourly_data['rain'], color=color, linestyle='-', marker='o')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Show plot\n",
    "plt.title(f'Mean Traffic Volume and Rain by Hour')\n",
    "plt.xticks(range(24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Traffic Volume with weather variables for each hour and specific week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week_key = 'March_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code enhances the visualization of pedestrian traffic data by incorporating multiple weather variables alongside traffic volume on an hourly basis for a specified week (`week_key`). After converting timestamps and extracting hours, it aggregates average values for traffic and weather-related metrics such as rain, temperature, humidity, precipitation, showers, and UV index. The visualization employs a dual-axis plot, with a bar chart for traffic volume on one axis and line graphs for the weather variables on the other. Each weather metric is represented in a different color, enhancing clarity and comparison. A legend is added for easy identification of each variable. This detailed visualization enables a comprehensive analysis of how various weather conditions correlate with pedestrian traffic volumes, offering valuable insights for urban safety and planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert 'timestamp' column to datetime format\n",
    "# merged_dataframes[week_key]['timestamp'] = pd.to_datetime(merged_dataframes[week_key]['timestamp'])\n",
    "\n",
    "# # Extract hour from timestamp\n",
    "# merged_dataframes[week_key]['hour'] = merged_dataframes[week_key]['timestamp'].dt.hour\n",
    "\n",
    "# # Group data by hour and calculate mean traffic volume and mean weather variables\n",
    "# hourly_data = merged_dataframes[week_key].groupby('hour').agg({\n",
    "#     'total_of_directions': 'mean', \n",
    "#     'rain': 'mean', \n",
    "#     'temperature_2m': 'mean', \n",
    "#     'relative_humidity_2m': 'mean', \n",
    "#     'precipitation': 'mean', \n",
    "#     'showers': 'mean', \n",
    "#     'uv_index': 'mean'\n",
    "# })\n",
    "\n",
    "# # Plot bar chart\n",
    "# fig, ax1 = plt.subplots()\n",
    "\n",
    "# # Bar for traffic volume\n",
    "# color = 'tab:blue'\n",
    "# ax1.set_xlabel('Hour of the Day')\n",
    "# ax1.set_ylabel('Mean Traffic Volume', color=color)\n",
    "# ax1.bar(hourly_data.index, hourly_data['total_of_directions'], color=color, alpha=0.7)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Create another y-axis for weather variables\n",
    "# ax2 = ax1.twinx()  \n",
    "\n",
    "# # Line plot for rain\n",
    "# color = 'tab:red'\n",
    "# ax2.set_ylabel('Weather Variables', color=color)\n",
    "# ax2.plot(hourly_data.index, hourly_data['rain'], color=color, linestyle='-', marker='o', label='Rain')\n",
    "# ax2.plot(hourly_data.index, hourly_data['temperature_2m'], color='green', linestyle='-', marker='o', label='Temperature')\n",
    "# ax2.plot(hourly_data.index, hourly_data['relative_humidity_2m'], color='orange', linestyle='-', marker='o', label='Relative Humidity')\n",
    "# ax2.plot(hourly_data.index, hourly_data['precipitation'], color='purple', linestyle='-', marker='o', label='Precipitation')\n",
    "# ax2.plot(hourly_data.index, hourly_data['showers'], color='brown', linestyle='-', marker='o', label='Showers')\n",
    "# ax2.plot(hourly_data.index, hourly_data['uv_index'], color='blue', linestyle='-', marker='o', label='UV Index')\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# # Add legend\n",
    "# fig.tight_layout()\n",
    "# fig.legend(loc=\"upper left\", bbox_to_anchor=(0.15,0.88))\n",
    "\n",
    "# # Show plot\n",
    "# plt.title(f'Mean Traffic Volume and Weather Variables by Hour [{week_key}]')\n",
    "# plt.xticks(range(24))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "\n",
    "# Extract hour from timestamp\n",
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "\n",
    "# Group data by hour and calculate mean traffic volume and mean weather variables\n",
    "hourly_data = merged_data.groupby('hour').agg({\n",
    "    'total_of_directions': 'mean', \n",
    "    'rain': 'mean', \n",
    "    'temperature_2m': 'mean', \n",
    "    'relative_humidity_2m': 'mean', \n",
    "    'precipitation': 'mean', \n",
    "    'showers': 'mean', \n",
    "    'uv_index': 'mean'\n",
    "})\n",
    "\n",
    "# Plot bar chart\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Bar for traffic volume\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Hour of the Day')\n",
    "ax1.set_ylabel('Mean Traffic Volume', color=color)\n",
    "ax1.bar(hourly_data.index, hourly_data['total_of_directions'], color=color, alpha=0.7)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create another y-axis for weather variables\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "# Line plot for rain\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Weather Variables', color=color)\n",
    "ax2.plot(hourly_data.index, hourly_data['rain'], color=color, linestyle='-', marker='o', label='Rain')\n",
    "ax2.plot(hourly_data.index, hourly_data['temperature_2m'], color='green', linestyle='-', marker='o', label='Temperature')\n",
    "ax2.plot(hourly_data.index, hourly_data['relative_humidity_2m'], color='orange', linestyle='-', marker='o', label='Relative Humidity')\n",
    "ax2.plot(hourly_data.index, hourly_data['precipitation'], color='purple', linestyle='-', marker='o', label='Precipitation')\n",
    "ax2.plot(hourly_data.index, hourly_data['showers'], color='brown', linestyle='-', marker='o', label='Showers')\n",
    "ax2.plot(hourly_data.index, hourly_data['uv_index'], color='blue', linestyle='-', marker='o', label='UV Index')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add legend\n",
    "fig.tight_layout()\n",
    "fig.legend(loc=\"upper left\", bbox_to_anchor=(0.15,0.88))\n",
    "\n",
    "# Show plot\n",
    "plt.title(f'Mean Traffic Volume and Weather Variables by Hour')\n",
    "plt.xticks(range(24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean hourly precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet visualizes the distribution of various weather conditions on an hourly basis for a specified week (`week_key`). It first extracts the hour from the timestamp for each entry and then analyzes several weather variables, including rain, precipitation, showers, UV index, temperature, and relative humidity. A 3x3 grid of subplots is set up, with each subplot dedicated to a different weather variable, displaying histograms of their mean hourly values. Each histogram is colored green and includes labels for the mean values and frequency, providing a clear visual representation of the typical hourly weather conditions. Unused subplot spaces are hidden to maintain a clean layout. This visualization is crucial for understanding the patterns and potential impacts of weather conditions on pedestrian dynamics, which can inform urban planning and safety strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract hour from timestamp\n",
    "# merged_dataframes[week_key]['hour'] = merged_dataframes[week_key]['timestamp'].dt.hour\n",
    "\n",
    "# # Define precipitation measures\n",
    "# precipitation_measures = ['rain', 'precipitation', 'showers','uv_index','temperature_2m','relative_humidity_2m'] \n",
    "\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# # Flatten the axes array for easy iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over precipitation measures\n",
    "# for i, measure in enumerate(precipitation_measures):\n",
    "#     # Group data by hour and calculate mean for the current precipitation measure\n",
    "#     hourly_data = merged_dataframes[week_key].groupby('hour')[measure].mean()\n",
    "    \n",
    "#     # Plot histogram of mean hourly precipitation\n",
    "#     axes[i].hist(hourly_data, bins=20, color='green', alpha=0.7)\n",
    "#     axes[i].set_xlabel(f'Mean Hourly {measure.capitalize()}')\n",
    "#     axes[i].set_ylabel('Frequency')\n",
    "#     axes[i].set_title(f'Histogram of Mean Hourly {measure.capitalize()}')\n",
    "\n",
    "# # Hide empty subplots (if any)\n",
    "# for ax in axes[len(precipitation_measures):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from timestamp\n",
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "\n",
    "# Define precipitation measures\n",
    "precipitation_measures = ['rain', 'precipitation', 'showers','uv_index','temperature_2m','relative_humidity_2m'] \n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over precipitation measures\n",
    "for i, measure in enumerate(precipitation_measures):\n",
    "    # Group data by hour and calculate mean for the current precipitation measure\n",
    "    hourly_data = merged_data.groupby('hour')[measure].mean()\n",
    "    \n",
    "    # Plot histogram of mean hourly precipitation\n",
    "    axes[i].hist(hourly_data, bins=20, color='green', alpha=0.7)\n",
    "    axes[i].set_xlabel(f'Mean Hourly {measure.capitalize()}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Histogram of Mean Hourly {measure.capitalize()}')\n",
    "\n",
    "# Hide empty subplots (if any)\n",
    "for ax in axes[len(precipitation_measures):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Volume with other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet utilizes matplotlib to generate a series of scatter plots, each examining the relationship between pedestrian traffic volume and various weather conditions such as rain, UV index, and temperature. The function `plot_trend_line` is defined to calculate and plot a regression line for each scatter plot, providing a visual representation of the linear relationship and the strength of correlation (R² value). The plots are organized in a 3x3 grid, with each subplot corresponding to a different weather variable. Scatter plots are marked in red for visibility, with trend lines in blue and dashed style to distinguish them. Labels and titles are set for each axis and plot, respectively, enhancing readability. Unused subplot spaces are hidden to maintain a neat layout. This visualization is instrumental in identifying how different weather conditions impact traffic volume, aiding in more informed decision-making for urban planning and safety measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define function to calculate regression line\n",
    "\n",
    "# def plot_trend_line(x, y, ax):\n",
    "#     if len(np.unique(x)) > 1:\n",
    "#         slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "#         line = slope * x + intercept\n",
    "#         ax.plot(x, line, color='blue', linestyle='--', label=f'Trend Line (R²={r_value**2:.2f})')\n",
    "#         ax.legend()\n",
    "#     else:\n",
    "#         ax.text(0.5, 0.5, 'Insufficient variation in data for regression', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, color='red')\n",
    "\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# # Flatten the axes array for easy iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Iterate over precipitation measures\n",
    "# for i, measure in enumerate(precipitation_measures):\n",
    "#     # Scatter plot of traffic volume vs. the current precipitation measure\n",
    "#     axes[i].scatter(merged_dataframes[week_key][measure], merged_dataframes[week_key]['total_of_directions'], color='red', alpha=0.5)\n",
    "#     axes[i].set_xlabel(measure.capitalize())  # Set x-axis label\n",
    "#     axes[i].set_ylabel('Traffic Volume')  # Set y-axis label\n",
    "#     axes[i].set_title(f'Traffic Volume vs. {measure.capitalize()}')  # Set title\n",
    "    \n",
    "#     # Calculate and plot trend line\n",
    "#     plot_trend_line(merged_dataframes[week_key][measure], merged_dataframes[week_key]['total_of_directions'], axes[i])\n",
    "\n",
    "# # Hide empty subplots (if any)\n",
    "# for ax in axes[len(precipitation_measures):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to calculate regression line\n",
    "\n",
    "def plot_trend_line(x, y, ax):\n",
    "    if len(np.unique(x)) > 1:\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        line = slope * x + intercept\n",
    "        ax.plot(x, line, color='blue', linestyle='--', label=f'Trend Line (R²={r_value**2:.2f})')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Insufficient variation in data for regression', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, color='red')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over precipitation measures\n",
    "for i, measure in enumerate(precipitation_measures):\n",
    "    # Scatter plot of traffic volume vs. the current precipitation measure\n",
    "    axes[i].scatter(merged_data[measure], merged_data['total_of_directions'], color='red', alpha=0.5)\n",
    "    axes[i].set_xlabel(measure.capitalize())  # Set x-axis label\n",
    "    axes[i].set_ylabel('Traffic Volume')  # Set y-axis label\n",
    "    axes[i].set_title(f'Traffic Volume vs. {measure.capitalize()}')  # Set title\n",
    "    \n",
    "    # Calculate and plot trend line\n",
    "    plot_trend_line(merged_data[measure], merged_data['total_of_directions'], axes[i])\n",
    "\n",
    "# Hide empty subplots (if any)\n",
    "for ax in axes[len(precipitation_measures):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Display the data types of all columns\n",
    "# print(all_weeks_combined.dtypes)\n",
    "# # Initialize the MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # List of columns to normalize\n",
    "# columns_to_normalize = ['total_of_directions', 'temperature_2m', 'relative_humidity_2m',\n",
    "#                         'precipitation', 'rain', 'showers', 'weather_code', 'uv_index']\n",
    "\n",
    "# # Fit the scaler on the data and transform it\n",
    "# all_weeks_combined[columns_to_normalize] = scaler.fit_transform(all_weeks_combined[columns_to_normalize])\n",
    "\n",
    "# # Print the scaled DataFrame\n",
    "# print(all_weeks_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = merged_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# List of columns to normalize\n",
    "columns_to_normalize = ['total_of_directions', 'temperature_2m', 'relative_humidity_2m',\n",
    "                        'precipitation', 'rain', 'showers', 'weather_code', 'uv_index']\n",
    "\n",
    "# Fit the scaler on the data and transform it, keeping the DataFrame structure\n",
    "normalized_data[columns_to_normalize] = scaler.fit_transform(normalized_data[columns_to_normalize])\n",
    "\n",
    "# Print the scaled DataFrame\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_values = all_weeks_combined['direction_1_y'].unique()\n",
    "# print(unique_values)\n",
    "# unique_values2 = all_weeks_combined['direction_2_y'].unique()\n",
    "# print(unique_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = normalized_data['direction_1_y'].unique()\n",
    "print(unique_values)\n",
    "unique_values2 = normalized_data['direction_2_y'].unique()\n",
    "print(unique_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of all columns\n",
    "print(normalized_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encorder for direction_1y and direction_2y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet initialises a `OneHotEncoder` to transform categorical data into a format that machine learning algorithms can more easily use. The encoder is set to produce a dense numpy array output. It first checks if the columns `direction_1_y` and `direction_2_y` exist in the `all_weeks_combined` DataFrame to prevent a KeyError during the encoding process.\r\n",
    "\r\n",
    "If the columns are present, the encoder fits and transforms the data in these columns, converting the categorical variables into a series of binary columns, one for each category. It then retrieves the names of these new features, which reflect the original column names and their corresponding category values.\r\n",
    "\r\n",
    "A new DataFrame, `encoded_df`, is created from the encoded data with these feature names as column headers. This encoded DataFrame is then concatenated with the original DataFrame, excluding the original categorical columns to avoid redundancy. The resulting DataFrame is printed to show the first few entries, allowing you to verify that the encoding was successful and the DataFrame now includes the newly encoded features.\r\n",
    "\r\n",
    "This transformation is essential for preparing the data for machine learning models that require numerical input, ensuring that categorical attributes like directions are appropriately represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)  # sparse=False ensures output is a numpy array\n",
    "\n",
    "# Check if columns exist to avoid KeyError\n",
    "if {'direction_1_y', 'direction_2_y'}.issubset(merged_data.columns):\n",
    "    # Fit and transform the data\n",
    "    encoded_data = encoder.fit_transform(normalized_data[['direction_1_y', 'direction_2_y']])\n",
    "    \n",
    "    # Get the feature names from the encoder\n",
    "    encoded_feature_names = encoder.get_feature_names_out(['direction_1_y', 'direction_2_y'])\n",
    "    \n",
    "    # Create a DataFrame with the encoded data and the generated feature names\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names)\n",
    "    \n",
    "    # Concatenate the encoded data back to the original DataFrame\n",
    "    normalized_data = pd.concat([normalized_data.drop(['direction_1_y', 'direction_2_y'], axis=1), encoded_df], axis=1)\n",
    "    print(normalized_data.head())\n",
    "else:\n",
    "    print(\"Columns 'direction_1_y' or 'direction_2_y' are not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.columns)  # This will list all column names in the DataFrame\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlation only weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['temperature_2m', 'relative_humidity_2m', 'precipitation', 'showers', 'uv_index']  \n",
    "correlation_matrix2 = normalized_data[selected_columns].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Map for weather features only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix2, annot=True, fmt=\".2f\", cmap='coolwarm',annot_kws={\"size\": 8})\n",
    "plt.title('Correlation Matrix2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check VIF for weather features to check multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code snippet is designed to evaluate multicollinearity among several predictors in the `all_weeks_combined` DataFrame, specifically focusing on weather-related variables such as temperature, UV index, showers, rain, and relative humidity. Here's a step-by-step breakdown of the process:\n",
    "\n",
    "Selection of Predictors: The first step involves selecting specific columns from the DataFrame that are likely to be used as predictors in a regression model.\n",
    "\n",
    "Adding a Constant: The `sm.add_constant` function is used to add a constant term to the predictor variables. This is necessary for models that require an intercept term, ensuring that the regression has a baseline to work from.\n",
    "Calculating Variance Inflation Factor (VIF):\n",
    "   - A new DataFrame `vif_data` is created to store the results.\n",
    "   - The Variance Inflation Factor is calculated for each predictor using the `variance_inflation_factor` function from the `statsmodels` library. This function assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated.\n",
    "   - The VIF for each variable is computed within a list comprehension, iterating over each column's index in the `predictors` DataFrame.\n",
    "\n",
    "Output: Finally, the VIF values along with the feature names are printed. VIF values greater than 10 typically suggest high multicollinearity, which may warrant further investigation or adjustments in the model, such as dropping variables or applying dimensionality reduction techniques.\n",
    "\n",
    "This approach is crucial for ensuring that the regression model built on these predictors will be reliable and not unduly influenced by multicollinearity, thus maintaining the validity of statistical inferences drawn from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Assuming df is your dataframe with the predictors\n",
    "# predictors = all_weeks_combined[['temperature_2m',  'uv_index', 'showers', 'rain','relative_humidity_2m']]\n",
    "# # Add a constant term for intercept\n",
    "# predictors = sm.add_constant(predictors)\n",
    "\n",
    "# # Calculating VIF for each variable\n",
    "# vif_data = pd.DataFrame()\n",
    "# vif_data[\"feature\"] = predictors.columns\n",
    "\n",
    "# vif_data[\"VIF\"] = [variance_inflation_factor(predictors.values, i)\n",
    "#                    for i in range(len(predictors.columns))]\n",
    "\n",
    "# print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming df is your dataframe with the predictors\n",
    "predictors = normalized_data[['temperature_2m',  'uv_index', 'showers', 'rain','relative_humidity_2m']]\n",
    "# Add a constant term for intercept\n",
    "predictors = sm.add_constant(predictors)\n",
    "\n",
    "# Calculating VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = predictors.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(predictors.values, i)\n",
    "                   for i in range(len(predictors.columns))]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Model with only weather Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--The `perform_regression` function is designed to conduct a regression analysis on a dataset, using weather-related predictors like temperature, UV index, showers, and rain to predict pedestrian traffic volumes (`total_of_directions`). The process begins by splitting the data into training and testing sets, ensuring a robust model evaluation. It incorporates a constant for intercept in the model, employs the Ordinary Least Squares (OLS) method for regression, and outputs a detailed summary including regression coefficients, statistical tests, and diagnostics to assess model performance. Additionally, it calculates and displays the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to evaluate the model's fit, aiding in the selection of an optimal model for predicting pedestrian traffic based on weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def perform_regression(df):\n",
    "    # Make sure to replace these with the actual columns from your dataframe\n",
    "    X = df[['temperature_2m',  'uv_index', 'showers', 'rain']]\n",
    "    y = df['total_of_directions']\n",
    "    \n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Adding a constant to the model for the intercept\n",
    "    X_train_with_const = sm.add_constant(X_train)\n",
    "    X_test_with_const = sm.add_constant(X_test)\n",
    "    \n",
    "    # Creating an OLS model with statsmodels\n",
    "    model = sm.OLS(y_train, X_train_with_const).fit()\n",
    "    \n",
    "    # Output the summary of the model\n",
    "    print(model.summary())\n",
    "    \n",
    "    # You can directly access AIC and BIC from the model object\n",
    "    print(f'AIC: {model.aic}')\n",
    "    print(f'BIC: {model.bic}')\n",
    "\n",
    "# Assuming df is your dataframe with all necessary data\n",
    "perform_regression(normalized_data)\n",
    "\n",
    "# Uncomment the last line and replace `df` with your actual dataframe variable when you run this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode the location_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code transforms the categorical `location_id` data in the `all_weeks_combined` DataFrame into dummy variables for use in numerical analysis and modeling. It uses `pd.get_dummies` to create a new DataFrame `location_dummies`, where each unique location ID is converted into a binary column prefixed with 'location'. These dummy variables are then appended to the original DataFrame using `pd.concat`, expanding it to include these new columns. The updated DataFrame's columns are then listed, allowing you to confirm that the dummy variables have been successfully integrated, which is crucial for subsequent data processing or modeling steps involving location-specific analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_dummies = pd.get_dummies(all_weeks_combined['location_id'], prefix='location')\n",
    "# all_weeks_combined = pd.concat([all_weeks_combined, location_dummies], axis=1)\n",
    "# all_weeks_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dummies = pd.get_dummies(normalized_data['location_id'], prefix='location')\n",
    "merged_data_encoded = pd.concat([normalized_data, location_dummies], axis=1)\n",
    "merged_data_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the correlation matrix for the numerical columns of interest + Categorical location ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--The code snippet prepares to analyze the correlations between various weather-related variables in the `all_weeks_combined` DataFrame. Initially, it attempts to drop several columns related to timestamps and geographical coordinates to focus on relevant predictors for analysis. Although the code to select specific columns like 'temperature_2m' and 'relative_humidity_2m' for correlation analysis is commented out, the intention is to examine the relationships between these environmental factors. The correlation matrix is then computed for these selected variables, aiming to reveal how closely these variables are related, which can help in understanding their combined effects on pedestrian traffic patterns. This correlation analysis is crucial for identifying potential multicollinearity or for developing more informed models that predict pedestrian behavior based on weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = merged_data_encoded.drop([ 'timestamp','longitude','latitude','direction_1_x',\n",
    "       'direction_2_x', 'total_of_directions'], axis=1)\n",
    "#selected_columns = ['temperature_2m', 'relative_humidity_2m', 'precipitation', 'rain', 'showers', 'uv_index']  \n",
    "correlation_matrix = merged_data_encoded[selected_columns].corr()\n",
    "#correlation_matrix = selected_data.corr()\n",
    "# Display the correlation matrix\n",
    "correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_encoded[  'total_of_directions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_encoded.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(merged_data_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = merged_data_encoded.select_dtypes(include=['object']).columns\n",
    "string_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data_encoded = merged_data_encoded.drop([ 'sensor_name_x',   'location_x', 'location_id', 'sensor_description', 'sensor_name_y', 'installation_date', 'note', 'location_type', 'status', 'location_y', 'date_only', 'hour', 'direction_1_y_East', 'direction_1_y_North', 'direction_2_y_South', 'direction_2_y_West'], axis=1)\n",
    "# print(merged_data_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--This code defines a function `find_column_with_value` that searches for a specific value within the object-type columns of a DataFrame. The function iterates over each column, checking if any of the entries match the given value, `Que85_T`. If a match is found, it returns the name of the column containing the value. After defining the function, it is executed using the `all_weeks_combined` DataFrame. The result indicates whether the value 'Que85_T' is found and in which column. If the value is not found, it outputs a message stating that no column contains 'Que85_T'. This functionality is particularly useful for quickly identifying the presence and location of specific entries within large datasets, aiding in data verification or cleaning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def find_column_with_value(df, value):\n",
    "#     for column in df.columns:\n",
    "#         if df[column].dtype == object:  # Check only object type columns to simplify\n",
    "#             if (df[column] == value).any():\n",
    "#                 return column\n",
    "#     return None\n",
    "\n",
    "# # Call the function to find the column name\n",
    "# column_name = find_column_with_value(all_weeks_combined, 'Que85_T')\n",
    "# if column_name:\n",
    "#     print(f\"'Que85_T' is found in the column: {column_name}\")\n",
    "# else:\n",
    "#     print(\"No column contains 'Que85_T'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_column_with_value(df, value):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == object:  # Check only object type columns to simplify\n",
    "            if (df[column] == value).any():\n",
    "                return column\n",
    "    return None\n",
    "\n",
    "# Call the function to find the column name\n",
    "column_name = find_column_with_value(merged_data_encoded, 'Que85_T')\n",
    "if column_name:\n",
    "    print(f\"'Que85_T' is found in the column: {column_name}\")\n",
    "else:\n",
    "    print(\"No column contains 'Que85_T'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with weather features + locationid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--The `perform_regression` function is meticulously designed to conduct a linear regression analysis, focusing on predicting pedestrian traffic volumes based on various environmental and situational predictors within a data frame. Initially, the function removes any rows containing NaN values to ensure the data quality used in the analysis. It then selects the target variable `total_of_directions` and discards non-predictive columns, setting up the predictor dataset. The data is split into training and testing sets, with 80% used for training to fit the model and 20% reserved for testing to evaluate the model's performance. A linear regression model is then trained on the training data. After training, the model makes predictions on the testing set, and its performance is evaluated using Mean Squared Error (MSE) and R-squared (R²), which are printed to give insights into the model's accuracy and explanatory power. This process is essential for understanding the impact of variables like weather conditions on pedestrian traffic, helping to inform decision-making in urban planning and safety management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_regression(df):\n",
    "    # Selecting the target variable and features\n",
    "    #X = df[['temperature_2m', 'relative_humidity_2m', 'uv_index','showers','rain']]  \n",
    "    # Mean Squared Error: 0.005115399601514251\n",
    "    # R^2 Score: 0.1291088971360574\n",
    "\n",
    "    df = df.dropna()  # Drops rows with any NaN values\n",
    "    X = df.drop(['timestamp','sensing_date', 'sensing_time', 'sensor_description', 'sensor_name',\n",
    "       'installation_date', 'note', 'location_type', 'status', 'location',\n",
    "       'date_only', 'longitude', 'latitude', 'direction_1_x', 'direction_2_x', 'total_of_directions',], axis=1)\n",
    "    y = df['total_of_directions']\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #  Creating a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R^2 Score: {r2}\")\n",
    "    \n",
    "# Call the function with your DataFrame\n",
    "perform_regression(merged_data_encoded)  # Adjust parameters as needed based on the correlation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weeks_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# def perform_advanced_regression(all_weeks_combined):\n",
    "#     # Selecting the target variable and features\n",
    "#     X = all_weeks_combined.drop(['timestamp', 'longitude', 'latitude', 'direction_1_x', 'direction_2_x', 'total_of_directions'], axis=1)\n",
    "#     y = all_weeks_combined['total_of_directions']\n",
    "\n",
    "#     # Adding polynomial features\n",
    "#     poly_model = make_pipeline(PolynomialFeatures(2), Ridge())\n",
    "\n",
    "#     # Cross-validation\n",
    "#     scores = cross_val_score(poly_model, X, y, cv=5, scoring='r2')\n",
    "#     print(f\"Cross-validated R^2 scores: {scores}\")\n",
    "#     print(f\"Average R^2 score: {scores.mean()}\")\n",
    "\n",
    "# # Call the function with your DataFrame\n",
    "# perform_advanced_regression(all_weeks_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def perform_random_forest_regression(df):\n",
    "#     X = all_weeks_combined[['temperature_2m', ]]\n",
    "#     y = all_weeks_combined['total_of_directions']\n",
    "    \n",
    "#     # Random Forest model\n",
    "#     rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "#     # Fitting the model\n",
    "#     rf_model.fit(X, y)\n",
    "    \n",
    "#     # Cross-validation\n",
    "#     scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')\n",
    "#     print(f\"Cross-validated R^2 scores: {scores}\")\n",
    "#     print(f\"Average R^2 score: {scores.mean()}\")\n",
    "\n",
    "#     # Feature importance\n",
    "#     importances = rf_model.feature_importances_\n",
    "#     feature_names = X.columns\n",
    "#     importance_dict = dict(zip(feature_names, importances))\n",
    "#     print(\"Feature importances:\", importance_dict)\n",
    "\n",
    "# # Call the function\n",
    "# perform_random_forest_regression(all_weeks_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def perform_xgboost_regression(df):\n",
    "#     X = dfall_weeks_combined[['temperature_2m', 'relative_humidity_2m', 'uv_index', 'showers', 'rain']]\n",
    "#     y = all_weeks_combined['total_of_directions']\n",
    "\n",
    "#     # XGBoost model\n",
    "#     xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    \n",
    "#     # Fitting the model\n",
    "#     xgb_model.fit(X, y)\n",
    "    \n",
    "#     # Cross-validation\n",
    "#     scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')\n",
    "#     print(f\"Cross-validated R^2 scores: {scores}\")\n",
    "#     print(f\"Average R^2 score: {scores.mean()}\")\n",
    "\n",
    "#     # Feature importance\n",
    "#     importances = xgb_model.feature_importances_\n",
    "#     feature_names = X.columns\n",
    "#     importance_dict = dict(zip(feature_names, importances))\n",
    "#     print(\"Feature importances:\", importance_dict)\n",
    "\n",
    "# # Call the function\n",
    "# perform_xgboost_regression(all_weeks_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footpath Steepness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to embed an interactive map into a Jupyter Notebook using HTML. It defines a string `html_code` that contains the HTML code for an iframe. This iframe links to a specific dataset hosted on the Melbourne data portal, displaying a map that highlights footpath steepness in the city. The map is set to display with specific dimensions: 1100 pixels wide and 600 pixels high, without any border around the frame. The last line of the code uses the `HTML` function from the IPython.display library to render this HTML content directly within the notebook. This functionality is particularly useful for integrating dynamic data visualizations directly into data analysis workflows, providing a visual context that complements the statistical analysis conducted in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HTML code for the map\n",
    "html_code = \"\"\"\n",
    "<iframe src=\"https://data.melbourne.vic.gov.au/explore/embed/dataset/footpath-steepness/map/?location=16,-37.81284,144.95249&basemap=mbs-7a7333\" width=\"1100\" height=\"600\" frameborder=\"0\"></iframe>\n",
    "\"\"\"\n",
    "\n",
    "# Display the map in the notebook\n",
    "HTML(html_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(footpath_steepness.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footpath_steepness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import footpath steepness dataset as geojson file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet is designed to load geographic data from a GeoJSON file named 'footpath-steepness.geojson' into a GeoDataFrame using the GeoPandas library. This process begins with reading the file and storing its contents in a variable gdf. To better understand the dataset, the script prints the first few entries of the GeoDataFrame, providing a quick glance at the data structure, including spatial attributes and geometry. Following the initial inspection, the code visualizes the data by plotting it directly. This visual representation helps in assessing the spatial distribution of footpath steepness across the dataset, offering a clear, immediate understanding of the geographic characteristics present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON into a GeoDataFrame\n",
    "gdf = gpd.read_file('footpath-steepness.geojson')\n",
    "\n",
    "# Check the first few records to understand what the data looks like\n",
    "print(gdf.head())\n",
    "\n",
    "# Perform a quick plot to visualize\n",
    "gdf.plot()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, you are refining a GeoDataFrame `gdf` by removing rows that lack data across a specific subset of columns related to footpath steepness. The columns of interest include 'grade1in', 'gradepc', 'deltaz', 'rlmax', 'rlmin', and 'distance'. By using the `dropna` method with the `subset` parameter, the code efficiently discards any rows where all of these columns simultaneously have missing values. This operation helps ensure that the remaining data in `gdf_cleaned` maintains a minimum level of completeness necessary for reliable analysis or modeling. After cleaning the data, you display the first few rows of the cleaned DataFrame to check the results of this operation, confirming that the rows with complete data in the specified columns are retained. This step is crucial for preparing the data for more detailed geographical analysis or visualization tasks that require consistent data availability across these key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['grade1in', 'gradepc', 'deltaz', 'rlmax', 'rlmin', 'distance']\n",
    "\n",
    "# Drop rows where all the specified columns are missing\n",
    "gdf_cleaned = gdf.dropna(subset=columns_to_check, how='all')\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame to verify the operation\n",
    "gdf_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "descriptive_stats = gdf_cleaned[['grade1in', 'gradepc']].describe()\n",
    "\n",
    "# Identifying missing values\n",
    "missing_values = gdf_cleaned[['grade1in', 'gradepc']].isnull().sum()\n",
    "\n",
    "# Outputting the results\n",
    "print(descriptive_stats)\n",
    "print('-----------------')\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of grade Inclination + Percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet creates histograms to visualize the distribution of 'grade1in' and 'gradepc' values in the cleaned GeoDataFrame `gdf_cleaned`. Each attribute is plotted separately, with the first histogram showing the 'grade1in' values, which represent the incline of footpaths measured as '1 in X'. The second histogram displays the 'gradepc', which is the grade percentage. Both histograms are designed with 50 bins to detail the spread of values and are colored distinctly—sky blue for 'grade1in' and green for 'gradepc'. The histograms are framed with black edges for better visual distinction and labeled appropriately with titles, x-axis labels denoting the measurement units, and y-axis labels indicating the frequency of data points, providing a clear and informative visualization of the distribution characteristics of these footpath steepness measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for grade1in\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(gdf_cleaned['grade1in'].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Grade Inclination (1 in X)')\n",
    "plt.xlabel('Grade Inclination (1 in X)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for gradepc\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(gdf_cleaned['gradepc'].dropna(), bins=50, color='green', edgecolor='black')\n",
    "plt.title('Distribution of Grade Percentage (%)')\n",
    "plt.xlabel('Grade Percentage (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade Percentage vs Grade Inclination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet visualizes the relationship between two key variables in the cleaned GeoDataFrame `gdf_cleaned`: 'grade1in', which measures the grade inclination, and 'gradepc', which represents the grade percentage. Using a scatter plot, it effectively illustrates how these two measures of footpath steepness correlate with each other. The plot is configured with a size of 5x3 inches, and points are semi-transparent (alpha set at 0.5) to better display areas of density where data points overlap. The axes are clearly labeled with the respective units of measurement, and the plot is titled \"Grade Percentage vs Grade Inclination\" to provide immediate context. This visualization is valuable for assessing the consistency between different measures of grade and identifying any trends or anomalies in the data related to footpath steepness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(gdf_cleaned['grade1in'], gdf_cleaned['gradepc'], alpha=0.5)\n",
    "plt.title('Grade Percentage vs Grade Inclination')\n",
    "plt.xlabel('Grade Inclination (1 in X)')\n",
    "plt.ylabel('Grade Percentage (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepness by grade percentage Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, I am using the `geopandas` library to visualize geographic data from the `gdf_cleaned` GeoDataFrame, specifically focusing on the 'gradepc' column, which represents the grade percentage of footpaths. The plot is initialised with a figure size of 10x10 inches, providing ample space for detailed visualisation. The footpath data is then plotted using a colour scheme based on quantiles to differentiate areas by steepness effectively. This method divides the data into categories with an equal number of data points, which helps understand the distribution of steepness across the region.\n",
    "\n",
    "The plot includes a legend with the title \"Steepness by Grade Percentage (%)\", positioned in the lower right of the plot, which aids in interpreting the colour scale. The axis labels are also turned off to emphasise the geographic visualisation without unnecessary distractions. This approach highlights spatial patterns in footpath steepness, offering insights into the topographical challenges within the area. The use of quantiles in colouring ensures that the steepness variations are clearly visible, making it an effective tool for urban planning and infrastructure analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot the data\n",
    "gdf_cleaned.plot(column='gradepc', scheme='quantiles', ax=ax, legend=True,\n",
    "                 legend_kwds={'title': \"Steepness by Grade Percentage (%)\", 'loc': 'lower right'})\n",
    "\n",
    "# Set the axis off\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map with selected Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cleaned = gdf_cleaned.dropna(subset=['streetid'])\n",
    "gdf_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cleaned.geo_point_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cleaned = gdf_cleaned.dropna(subset=['address'])\n",
    "gdf_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet leverages the Folium library to visualize specific geographic data within a defined bounding box on an interactive map. Initially, it sets the bounds of the bounding box using specified latitude and longitude values and constructs a Polygon object to represent this area. A Folium map is then initialized, centered on the midpoint of these bounds, with a zoom level optimized for detailed viewing. The bounding box is visually outlined on the map using a red PolyLine. The code processes each row of a cleaned GeoDataFrame (gdf_cleaned), checking if the centroid of the geometric data falls within the bounding box. If it does, a marker is placed at the centroid's coordinates, which includes a popup displaying these coordinates. This method of visualization is particularly effective for detailed spatial analysis within a specific area, allowing for precise identification and display of data points relevant to urban planning or geographic studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define bounds and create a bounding box polygon\n",
    "bounds = [-37.825, -37.820, 144.95, 144.96]\n",
    "bounding_box = Polygon([(bounds[2], bounds[0]), (bounds[3], bounds[0]), \n",
    "                        (bounds[3], bounds[1]), (bounds[2], bounds[1])])\n",
    "\n",
    "# Create a map centered around the middle of the bounds\n",
    "m = folium.Map(location=[(bounds[0] + bounds[1]) / 2, (bounds[2] + bounds[3]) / 2], zoom_start=16)\n",
    "\n",
    "# Add the bounding box as a rectangle on the map\n",
    "rectangle = [\n",
    "    [bounds[0], bounds[2]],  # (min_lat, min_lon)\n",
    "    [bounds[0], bounds[3]],  # (min_lat, max_lon)\n",
    "    [bounds[1], bounds[3]],  # (max_lat, max_lon)\n",
    "    [bounds[1], bounds[2]],  # (max_lat, min_lon)\n",
    "    [bounds[0], bounds[2]]   # close the loop\n",
    "]\n",
    "folium.PolyLine(rectangle, color=\"red\", weight=2).add_to(m)\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "for idx, row in gdf_cleaned.iterrows():\n",
    "    if row['geometry'] is not None:  # Check if geometry is not None\n",
    "        geometry = row['geometry']\n",
    "        centroid = geometry.centroid\n",
    "\n",
    "        # Check if centroid is within the bounding box\n",
    "        if bounding_box.contains(centroid):\n",
    "            # Add a marker for the location within the bounding box\n",
    "            folium.Marker([centroid.y, centroid.x],  # folium uses lat, lon order\n",
    "                          popup=f\"Coordinate: {centroid.y}, {centroid.x}\").add_to(m)\n",
    "\n",
    "m  # Display the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet processes the gdf_cleaned GeoDataFrame by extracting latitude and longitude coordinates from a column named geo_point_2d, which presumably contains dictionary-like objects with 'lat' and 'lon' keys. The lambda function applied to each row of the geo_point_2d column retrieves these values individually for latitude and longitude, and assigns them to new columns in the GeoDataFrame named 'lat' and 'lon' respectively. This operation effectively separates the combined geographical coordinates into distinct latitude and longitude columns, simplifying further geographic analyses or visualizations that require these coordinates to be accessed separately. This step is crucial for enhancing the usability of the data, particularly when interfacing with geographic libraries or systems that expect separate latitude and longitude inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cleaned['lat'] = gdf_cleaned['geo_point_2d'].apply(lambda x: x['lat'])\n",
    "gdf_cleaned['lon'] = gdf_cleaned['geo_point_2d'].apply(lambda x: x['lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract geo points in selected area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet filters entries in the `gdf_cleaned` GeoDataFrame to include only those within a specified geographic bounding box defined by the `bounds` list. The filtering criteria ensure that only entries with latitude and longitude values lying between the defined minimum and maximum bounds are included in the resulting `visible_area_df` DataFrame. The bounds list contains latitude and longitude limits in the order of minimum latitude, maximum latitude, minimum longitude, and maximum longitude. After applying these conditions, the snippet outputs the number of entries that fall within the defined visible area by accessing the `shape` attribute of the DataFrame, which provides the count of rows (entries). This operation is essential for focusing analyses on specific geographic regions, ensuring that subsequent processes, analyses, or visualizations are restricted to the most relevant spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame\n",
    "visible_area_df = gdf_cleaned[(gdf_cleaned['lat'] >= bounds[0]) & \n",
    "                              (gdf_cleaned['lat'] <= bounds[1]) &\n",
    "                              (gdf_cleaned['lon'] >= bounds[2]) & \n",
    "                              (gdf_cleaned['lon'] <= bounds[3])]\n",
    "\n",
    "# Show the number of rows filtered or any specific information\n",
    "print(\"Number of entries in visible area:\", visible_area_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use clustering method to findout the closest footpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a series of operations to analyze and cluster footpath data within a specific geographic area, using hierarchical clustering based on their coordinates to identify groups of closely located paths. Here’s how it works: The code first filters the `visible_area_df` DataFrame to include only those entries identified as 'Road Footway', thereby isolating footpaths. It then extracts the latitude and longitude coordinates and converts them into a NumPy array for distance calculations. Using the `pdist` function from the `scipy.spatial.distance` module, a condensed distance matrix is computed using the Euclidean metric, which measures the straight-line distance between points. Hierarchical clustering is performed on this distance matrix using the `linkage` method from `scipy.cluster.hierarchy`, with 'ward' method to minimize the variance within each cluster. To form distinct clusters, a distance threshold (`max_d`) is set, and the `fcluster` function creates flat clusters based on this threshold. The code then identifies the largest cluster and filters the footpaths to include only those within this cluster, limiting the result to the closest 100 paths if there are more available. This approach effectively groups footpaths that are geographically close to each other, facilitating targeted analysis or interventions in densely populated footpath regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include footpaths\n",
    "footpaths = visible_area_df[visible_area_df['asset_type'] == 'Road Footway']\n",
    "\n",
    "# Prepare coordinates for distance calculation\n",
    "coords = footpaths[['lat', 'lon']].to_numpy()\n",
    "\n",
    "# Calculate the condensed distance matrix\n",
    "dist_condensed = pdist(coords, metric='euclidean')\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(dist_condensed, method='ward')\n",
    "\n",
    "# Flat clusters: specify a threshold to form clusters\n",
    "# The threshold can be adjusted based on the dataset to control the size of clusters\n",
    "max_d = 10  # Adjust this threshold based on your specific needs\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "\n",
    "# Find the cluster with the closest 100 paths or adjust the selection based on the number of paths\n",
    "cluster_labels, counts = np.unique(clusters, return_counts=True)\n",
    "target_cluster_label = cluster_labels[np.argsort(-counts)[:1]]  # Getting the largest cluster\n",
    "\n",
    "# Filter rows belonging to the target cluster and limit to 100 if there are more\n",
    "closest_footpaths = footpaths[clusters == target_cluster_label[0]].head(100)\n",
    "\n",
    "# Output the result\n",
    "closest_footpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet efficiently generates an interactive map centered on the mean latitude and longitude of a cluster of closely located footpaths, aiming to visualize their distribution and individual characteristics. It begins by calculating the average coordinates of these footpaths, which are used to set the central point of the map for an optimal view. The map is created using Folium with a suitable zoom level to ensure detailed visibility. Each footpath in the cluster is marked on the map using a loop that places a marker at the respective geographic coordinates of each path. The markers are equipped with pop-ups that display the grade percentage of each footpath, formatted to two decimal places for precision. This format not only enhances the map's utility for spatial analysis but also provides valuable data on footpath steepness at a glance, which can be particularly useful for urban planning and pedestrian safety initiatives. The map is then displayed, offering an interactive tool for exploring the geographic layout and specific data points of the footpaths within the targeted cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_lat = closest_footpaths['lat'].mean()\n",
    "mean_lon = closest_footpaths['lon'].mean()\n",
    "m = folium.Map(location=[mean_lat, mean_lon], zoom_start=16)  # You might adjust zoom_start as needed\n",
    "\n",
    "# Loop through the DataFrame indices \n",
    "for idx in closest_footpaths.index:\n",
    "    lat = closest_footpaths.loc[idx, 'lat']\n",
    "    lon = closest_footpaths.loc[idx, 'lon']\n",
    "    gradepc = closest_footpaths.loc[idx, 'gradepc']  \n",
    "\n",
    "    # Create a marker for each location in the route\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=f\"Grade PC: {gradepc:.2f}%\",  # Format grade percent to 2 decimal places\n",
    "        icon=folium.Icon(color=\"blue\", icon=\"info-sign\")  # Blue icon with info sign\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet uses Folium to construct an interactive map centered on the average coordinates of a cluster of footpaths, effectively visualizing the path sequence and highlighting significant points. The map is initialized centered around the calculated mean latitude and longitude of the `closest_footpaths` DataFrame to ensure it focuses directly on the relevant area, with a zoom level of 14 for detailed observation. Markers are placed at the first and last points of the DataFrame, designated as the start and end points, and are distinguished by green and red icons, respectively. These markers display the grade percentage at each point, providing immediate data on the steepness at the start and end of the route. The paths between consecutive points are connected with blue lines, enhancing the visual representation of the route's continuity and direction. This setup not only aids in navigating through the path visually but also adds depth to the analysis of footpath connectivity and accessibility within the area, making it a valuable tool for urban planning and related studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a map centered around the average coordinates of the points\n",
    "map_center = [closest_footpaths['lat'].mean(), closest_footpaths['lon'].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=16)\n",
    "\n",
    "# Assuming the first and last points in your DataFrame are the start and end points\n",
    "start_point = closest_footpaths.iloc[0]\n",
    "end_point = closest_footpaths.iloc[-1]\n",
    "\n",
    "# Add markers for start and end points\n",
    "folium.Marker(\n",
    "    [start_point['lat'], start_point['lon']],\n",
    "    popup='Start Point: {}'.format(start_point['gradepc']),\n",
    "    icon=folium.Icon(color=\"green\", icon=\"play\")\n",
    ").add_to(m)\n",
    "\n",
    "folium.Marker(\n",
    "    [end_point['lat'], end_point['lon']],\n",
    "    popup='End Point: {}'.format(end_point['gradepc']),\n",
    "    icon=folium.Icon(color=\"red\", icon=\"stop\")\n",
    ").add_to(m)\n",
    "\n",
    "# Connect the points with lines\n",
    "for i in range(len(closest_footpaths) - 1):\n",
    "    folium.PolyLine([\n",
    "        [closest_footpaths.iloc[i]['lat'], closest_footpaths.iloc[i]['lon']],\n",
    "        [closest_footpaths.iloc[i + 1]['lat'], closest_footpaths.iloc[i + 1]['lon']]\n",
    "    ], color=\"blue\").add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interactive map reveals some potential issues with the visual representation of footpath data in Melbourne. First, the clustering of lines near the central point suggests either a high concentration of footpaths in a very specific area or possibly overlapping data that could indicate errors in data entry or processing. The spread of the blue lines, representing the routes or connections between points, seems somewhat chaotic and could confuse users trying to interpret paths or patterns. Additionally, the markers for the start and end points, indicated by green and red icons, are closely positioned, which might suggest either a short route or an inaccurate depiction of the footpath extents. This concentration and apparent clutter could hinder the map's usability for effective navigation or planning without further refinement or clearer demarcation of distinct paths and important locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problematic output in the visualization of the footpath steepness dataset can be attributed to several key issues related to data characteristics and processing methods:\n",
    "\n",
    "Comprehensive Inclusion of Footpaths: The dataset includes every single footpath segment, which, while thorough, can lead to visual clutter and overlap when all segments are displayed simultaneously. This level of detail, although useful for exhaustive analysis, can obscure higher-level trends or patterns when not selectively filtered or aggregated.\n",
    "Difficulty in Identifying Specific Streets: Due to the inclusion of all footpath segments without clear differentiation, it becomes challenging to distinguish individual streets or pathways. This can hinder efforts to focus on specific areas or to perform targeted analysis, especially in densely populated or complex urban layouts.\n",
    "Route Calculation Complexity: The route calculations that include every part of the footpath complicate the understanding of the overall footpath network. When paths are calculated without considering logical or habitual pedestrian routes (like direct paths or commonly used routes), the resulting visual output can appear chaotic and less practical for real-world applications, such as urban planning or pedestrian traffic management.\n",
    "\n",
    "Addressing these issues could involve applying more selective criteria for visualizing data, utilizing spatial hierarchies or clustering algorithms to simplify the presentation, or enhancing the map with interactive features that allow users to focus on specific layers or segments of interest. Such improvements would make the data more accessible and actionable for decision-makers and analysts looking to optimize urban spaces for pedestrian use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified nearest neighbor path function with max_distance parameter for finding the best route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code includes two functions: calculate_distance, which computes the Euclidean distance between two given points using their coordinates, and nearest_neighbor_path, which determines a path through a DataFrame of geographical points starting from a given index. This path is constructed by connecting to the nearest unvisited neighbor that is within a specified maximum distance. The function ensures proper handling of edge cases, such as an empty DataFrame or an out-of-bound start index, by returning an empty DataFrame or raising a ValueError respectively. The example usage resets the index of the DataFrame closest_footpaths for continuity and applies nearest_neighbor_path to visualize a feasible route under defined distance constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Euclidean distance\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return np.sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
    "\n",
    "# Modified nearest neighbor path function with max_distance parameter\n",
    "def nearest_neighbor_path(df, start_index=0, max_distance=0.01):  # max_distance in coordinate degrees\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if input is empty\n",
    "\n",
    "    if start_index >= len(df):\n",
    "        raise ValueError(\"Start index is out of the bounds of the DataFrame.\")\n",
    "\n",
    "    path = [start_index]  # Start at the given index\n",
    "    visited = set(path)\n",
    "\n",
    "    while len(visited) < len(df):\n",
    "        last_index = path[-1]\n",
    "        next_index = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for i in df.index:\n",
    "            if i not in visited:\n",
    "                current_distance = calculate_distance((df.iloc[last_index]['lat'], df.iloc[last_index]['lon']), (df.iloc[i]['lat'], df.iloc[i]['lon']))\n",
    "                if current_distance < min_distance and current_distance <= max_distance:\n",
    "                    min_distance = current_distance\n",
    "                    next_index = i\n",
    "\n",
    "        if next_index is None:\n",
    "            break  # Exit loop if no unvisited nodes are left or no nodes are within max_distance\n",
    "        visited.add(next_index)\n",
    "        path.append(next_index)\n",
    "\n",
    "    return df.iloc[path]\n",
    "\n",
    "# Example usage assuming gdf_cleaned is already defined and has 'lat' and 'lon' columns\n",
    "# Reset DataFrame index to ensure it's continuous\n",
    "closest_footpaths.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate the path starting from the first index with a maximum allowable distance\n",
    "path_df1 = nearest_neighbor_path(closest_footpaths, max_distance=0.005)  # Adjust max_distance as needed\n",
    "print(path_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map object centered at the mean of the points\n",
    "m1 = folium.Map(location=[gdf_cleaned['lat'].mean(), gdf_cleaned['lon'].mean()], zoom_start=16)\n",
    "\n",
    "# Plot each point on the map\n",
    "for idx, row in path_df1.iterrows():\n",
    "    folium.CircleMarker(location=[row['lat'], row['lon']],\n",
    "                        radius=5,\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        fill_color='blue',\n",
    "                        popup=f'Lat: {row[\"lat\"]}, Lon: {row[\"lon\"]}'\n",
    "                       ).add_to(m1)\n",
    "\n",
    "# Add lines between points\n",
    "for i in range(len(path_df1) - 1):\n",
    "    folium.PolyLine(locations=[(path_df1.iloc[i]['lat'], path_df1.iloc[i]['lon']),\n",
    "                               (path_df1.iloc[i + 1]['lat'], path_df1.iloc[i + 1]['lon'])],\n",
    "                    color='red').add_to(m1)\n",
    "\n",
    "# Display the map\n",
    "m1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDTree method for efficient nearest neighbor search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is designed to optimize the process of finding a path between geographic points using a KDTree for efficient nearest neighbor searches. It starts by resetting the index of a DataFrame, `gdf_cleaned`, which contains latitude ('lat') and longitude ('lon') columns. A KDTree is then created from these coordinates, configured to use the Euclidean distance metric. The function `nearest_neighbor_path_kdtree` is defined to determine a path starting from a specified index, utilizing the KDTree to find the nearest neighbors within a given maximum distance (specified in degrees, approximately 550 meters at the equator). The function ensures that each point is visited only once by maintaining a set of visited indices and iteratively adding the nearest valid neighbor to the path until no further valid neighbors are found or all points are visited. The path determined by this function is finally printed. This approach is particularly useful for applications needing rapid calculations over geographic data, like routing or spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming gdf_cleaned is a DataFrame with 'lat' and 'lon' columns\n",
    "gdf_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create KDTree for efficient nearest neighbor search\n",
    "coords = gdf_cleaned[['lat', 'lon']].to_numpy()\n",
    "tree = KDTree(coords, metric='euclidean')\n",
    "\n",
    "def nearest_neighbor_path_kdtree(df, start_index=0, max_distance=0.005):\n",
    "    # max_distance is in degrees, roughly equivalent to ~550 meters at the equator\n",
    "    # Adjust the value based on your specific needs and local geographical context\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    num_points = len(df)\n",
    "    path = [start_index]\n",
    "    visited = set(path)\n",
    "\n",
    "    while len(visited) < num_points:\n",
    "        last_index = path[-1]\n",
    "        # Query the tree for the nearest neighbors\n",
    "        distances, indices = tree.query([df.iloc[last_index][['lat', 'lon']].to_numpy()], k=num_points)\n",
    "        found_next = False\n",
    "        \n",
    "        for dist, index in zip(distances[0], indices[0]):\n",
    "            if index not in visited and dist <= max_distance:\n",
    "                visited.add(index)\n",
    "                path.append(index)\n",
    "                found_next = True\n",
    "                break\n",
    "        \n",
    "        if not found_next:  # Break if no unvisited neighbor within max_distance\n",
    "            break\n",
    "\n",
    "    return df.iloc[path]\n",
    "\n",
    "# Calculate the path starting from the first index\n",
    "path_df2 = nearest_neighbor_path_kdtree(gdf_cleaned)\n",
    "print(path_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with Folium\n",
    "m = folium.Map(location=[gdf_cleaned['lat'].mean(), gdf_cleaned['lon'].mean()], zoom_start=14)\n",
    "for idx, row in path_df.iterrows():\n",
    "    folium.CircleMarker([row['lat'], row['lon']],\n",
    "                        radius=3,\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        popup=f\"({row['lat']}, {row['lon']})\").add_to(m)\n",
    "\n",
    "# Connecting points with lines, checking the distance condition\n",
    "for i in range(len(path_df) - 1):\n",
    "    if calculate_distance((path_df.iloc[i]['lat'], path_df.iloc[i]['lon']), (path_df.iloc[i+1]['lat'], path_df.iloc[i+1]['lon'])) <= 0.005:\n",
    "        folium.PolyLine(\n",
    "            locations=[(path_df.iloc[i]['lat'], path_df.iloc[i]['lon']), \n",
    "                       (path_df.iloc[i+1]['lat'], path_df.iloc[i+1]['lon'])],\n",
    "            color='red').add_to(m)\n",
    "\n",
    "# Show the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Networkx to find best route considering gradepc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, a graph is constructed using the NetworkX library to represent footpaths as nodes and the distances between them as weighted edges. Each node in the graph corresponds to a footpath, identified by its index, and contains attributes such as latitude, longitude, and grade percentage (`gradepc`). Edges between nodes are then added based on the Euclidean distance between each pair of footpaths, with weights calculated by adjusting the raw distance by the grade percentage of the starting node. Specifically, the weight is computed by dividing the distance by the normalized grade percentage (converted from percentage to a proportion), effectively incorporating the steepness of the path into the cost of travel between nodes. This model allows for the analysis of path connectivity under conditions influenced by physical geography, potentially useful for optimizing routing in urban planning or accessibility studies where elevation changes are a significant factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for i, row in closest_footpaths.iterrows():\n",
    "    G.add_node(i, lat=row['lat'], lon=row['lon'], gradepc=row['gradepc'])\n",
    "\n",
    "# Add edges with weights\n",
    "for i in closest_footpaths.index:\n",
    "    for j in closest_footpaths.index:\n",
    "        if i != j:\n",
    "            dist = euclidean([closest_footpaths.loc[i, 'lat'], closest_footpaths.loc[i, 'lon']], [closest_footpaths.loc[j, 'lat'], closest_footpaths.loc[j, 'lon']])\n",
    "            weight = dist / (closest_footpaths.loc[i, 'gradepc'] / 100.0)  # Using gradepc as part of the weight calculation\n",
    "            G.add_edge(i, j, weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is designed to verify the presence of specified start and end nodes in a graph constructed from footpath data and to find the shortest path between these nodes using weighted edges. The process begins by identifying the minimum and maximum indices from the `closest_footpaths` DataFrame, which are assumed to represent the start and end nodes of the route, respectively. It then checks whether both these nodes are present in the graph `G`. If both nodes are found, the code employs NetworkX's `shortest_path` function to calculate the shortest path between them, considering the edge weights that incorporate both distance and footpath grade percentage. This approach ensures that the path calculated takes into account not just the direct distance but also the effort required to traverse each segment, which is particularly useful in scenarios where terrain and elevation are critical factors. If either the start or end node is missing, the code raises a `ValueError`, alerting to the issue that not all intended points are included in the graph, which is crucial for ensuring the reliability of any route or network analysis performed with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify nodes are in the graph and find the shortest path\n",
    "start_node = closest_footpaths.index.min()  # Assuming the first index is the start\n",
    "end_node = closest_footpaths.index.max()  # Assuming the last index is the end\n",
    "\n",
    "if start_node in G and end_node in G:\n",
    "    path = nx.shortest_path(G, source=start_node, target=end_node, weight='weight')\n",
    "else:\n",
    "    raise ValueError(\"Start or end node not found in the graph.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet utilizes Folium to create an interactive map showcasing the shortest path between nodes in a network of footpaths, with specific markers and lines highlighting the route. The map is initialized centered on the average latitude and longitude of the footpaths involved in the shortest path calculation, providing a focused view of the area of interest. Each node (footpath) along the path is marked on the map, with the starting node marked in green, the ending node in red, and all intermediate nodes in blue to distinguish the sequence of travel. Popup labels provide detailed information about each node, including its index and grade percentage, formatted for clarity. Lines connecting consecutive nodes are plotted to visually represent the path, with a red color to enhance visibility. This setup not only aids in visualizing the geographical layout of the shortest path but also enhances the user's ability to interact with the map to explore details about each footpath segment, making it an effective tool for urban planning, route optimization, or recreational trail mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map initialization\n",
    "m = folium.Map(location=[closest_footpaths['lat'].mean(), closest_footpaths['lon'].mean()], zoom_start=16)\n",
    "\n",
    "# Plotting each point on the map\n",
    "for idx in path:\n",
    "    folium.Marker(\n",
    "        [closest_footpaths.loc[idx, 'lat'], closest_footpaths.loc[idx, 'lon']],\n",
    "        popup=f\"Index: {idx}, Grad PC: {closest_footpaths.loc[idx, 'gradepc']:.2f}\",\n",
    "        icon=folium.Icon(color=\"blue\" if idx not in [start_node, end_node] else \"green\" if idx == start_node else \"red\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Connecting points with lines\n",
    "for i in range(len(path) - 1):\n",
    "    folium.PolyLine([\n",
    "        [closest_footpaths.loc[path[i], 'lat'], closest_footpaths.loc[path[i], 'lon']],\n",
    "        [closest_footpaths.loc[path[i + 1], 'lat'], closest_footpaths.loc[path[i + 1], 'lon']],\n",
    "    ], color=\"red\").add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
