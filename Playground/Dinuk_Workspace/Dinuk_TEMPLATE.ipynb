{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "def fetch_data(base_url, dataset, api_key, num_records=99, offset=0):\n",
    "    all_records = []\n",
    "    max_offset = 9900  # Maximum number of requests\n",
    "\n",
    "    while True:\n",
    "        # maximum limit check\n",
    "        if offset > max_offset:\n",
    "            break\n",
    "\n",
    "        # Create API request URL\n",
    "        filters = f'{dataset}/records?limit={num_records}&offset={offset}'\n",
    "        url = f'{base_url}{filters}&api_key={api_key_mop}'\n",
    "\n",
    "        # Start request\n",
    "        try:\n",
    "            result = requests.get(url, timeout=10)\n",
    "            result.raise_for_status()\n",
    "            records = result.json().get('results')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"API request failed: {e}\")\n",
    "        if records is None:\n",
    "            break\n",
    "        all_records.extend(records)\n",
    "        if len(records) < num_records:\n",
    "            break\n",
    "\n",
    "        # next cycle offset\n",
    "        offset += num_records\n",
    "\n",
    "    # DataFrame all data\n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df\n",
    "    \n",
    "API_KEY = os.environ.get(\"API_KEY_MOP\")\n",
    "BASE_URL = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microclimate Sensor Readings\n",
    "CLIMATE_DATASET = 'microclimate-sensor-readings'\n",
    "microclimate_readings = fetch_data(BASE_URL, CLIMATE_DATASET, API_KEY)\n",
    "microclimate_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Microclimate Sensor Locations\n",
    "SENSOR_LOCATIONS_DATASET = 'microclimate-sensor-locations'\n",
    "climate_sensor_locations = fetch_data(BASE_URL, SENSOR_LOCATIONS_DATASET, API_KEY)\n",
    "climate_sensor_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Footpath steepness\n",
    "FOOTPATH_STEEPNESS_DATASET = 'footpath-steepness'\n",
    "footpath_steepness = fetch_data(BASE_URL, FOOTPATH_STEEPNESS_DATASET, API_KEY)\n",
    "footpath_steepness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedestrian Counting System - Past Hour (counts per minute)\n",
    "PEDESTRIAN_DATASET = 'pedestrian-counting-system-past-hour-counts-per-minute'\n",
    "pedestraian_count = fetch_data(BASE_URL, PEDESTRIAN_DATASET, API_KEY)\n",
    "pedestraian_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pedestrian Counting System - Sensor Locations\n",
    "SENSOR_LOCATIONS_DATASET_2 = 'pedestrian-counting-system-sensor-locations'\n",
    "pedestrian_sensor_locations = fetch_data(BASE_URL, SENSOR_LOCATIONS_DATASET_2, API_KEY)\n",
    "pedestrian_sensor_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'microclimate-sensor-readings'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    microclimate_readings = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(microclimate_readings.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'footpath-steepness'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    footpath_steepness = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(footpath_steepness.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'pedestrian-counting-system-past-hour-counts-per-minute'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    pedestraian_count = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(pedestraian_count.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"API_KEY_MOP\")\n",
    "\n",
    "base_url = 'https://melbournetestbed.opendatasoft.com/api/explore/v2.1/catalog/datasets/'\n",
    "dataset_id = 'pedestrian-counting-system-sensor-locations'\n",
    "apikey = api_key\n",
    "dataset_id = dataset_id\n",
    "format = 'csv'\n",
    "\n",
    "params = {\n",
    "    'select': '*',\n",
    "    'limit': -1,  # all records\n",
    "    'lang': 'en',\n",
    "    'timezone': 'UTC',\n",
    "    'api_key': apikey\n",
    "}\n",
    "url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "# GET request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # StringIO to read the CSV data\n",
    "    url_content = response.content.decode('utf-8')\n",
    "    pedestrian_sensor_locations = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "    print(pedestrian_sensor_locations.sample(10, random_state=999)) # Test\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates -37.75째N 144.875째E\n",
      "Elevation 19.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n",
      "                          date  temperature_2m  relative_humidity_2m  \\\n",
      "0    2023-12-27 00:00:00+00:00       21.867001                  76.0   \n",
      "1    2023-12-27 01:00:00+00:00       22.017000                  76.0   \n",
      "2    2023-12-27 02:00:00+00:00       22.317001                  75.0   \n",
      "3    2023-12-27 03:00:00+00:00       22.717001                  73.0   \n",
      "4    2023-12-27 04:00:00+00:00       22.867001                  73.0   \n",
      "...                        ...             ...                   ...   \n",
      "2371 2024-04-03 19:00:00+00:00       11.467000                  78.0   \n",
      "2372 2024-04-03 20:00:00+00:00       11.217000                  80.0   \n",
      "2373 2024-04-03 21:00:00+00:00       11.417000                  79.0   \n",
      "2374 2024-04-03 22:00:00+00:00       12.516999                  72.0   \n",
      "2375 2024-04-03 23:00:00+00:00       14.117000                  62.0   \n",
      "\n",
      "      precipitation  rain  showers  weather_code  uv_index  \n",
      "0               0.0   0.0      0.0           2.0      3.90  \n",
      "1               0.0   0.0      0.0           2.0      3.65  \n",
      "2               0.0   0.0      0.0           2.0      4.30  \n",
      "3               0.0   0.0      0.0           2.0      9.15  \n",
      "4               0.0   0.0      0.0           3.0      7.00  \n",
      "...             ...   ...      ...           ...       ...  \n",
      "2371            0.0   0.0      0.0           1.0      0.00  \n",
      "2372            0.0   0.0      0.0           1.0      0.00  \n",
      "2373            0.0   0.0      0.0           1.0      0.00  \n",
      "2374            0.0   0.0      0.0           3.0      0.40  \n",
      "2375            0.0   0.0      0.0           3.0      1.00  \n",
      "\n",
      "[2376 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "\t\"latitude\": -37.814,\n",
    "\t\"longitude\": 144.965,\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"showers\", \"weather_code\", \"uv_index\"],\n",
    "\t\"past_days\": 92\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}째N {response.Longitude()}째E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_showers = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_weather_code = hourly.Variables(5).ValuesAsNumpy()\n",
    "hourly_uv_index = hourly.Variables(6).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "hourly_data[\"showers\"] = hourly_showers\n",
    "hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "hourly_data[\"uv_index\"] = hourly_uv_index\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(hourly_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_url = \"?dataset=global-summary-of-the-day\"\n",
    "\n",
    "#NOAA Dataset\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL and parameters\n",
    "base_url = \"https://www.ncei.noaa.gov/access/services/data/v1\"\n",
    "datasets_url = \"?dataset=daily-summaries\"\n",
    "station_url = \"&stations=ASN00083035,ASN00083052,ASN00083068,ASN00083090,ASN00084010,ASN00084016,ASN00084017,ASN00084022,ASN00084030,ASN00084059,ASN00084073,ASN00084084,ASN00084085,ASN00084114,ASN00084121,ASN00084143,ASN00084144,ASN00084145,ASN00085062,ASN00085072,ASN00085092,ASN00085093,ASN00085106,ASN00085142,ASN00085193,ASN00085242,ASN00085244,ASN00085247,ASN00085279,ASN00085280,ASN00085291,ASN00085296,ASN00086133,ASN00089037\"\n",
    "startdate_url = \"&startDate=2016-01-01\"\n",
    "enddate_url = \"&endDate=2020-12-31\"\n",
    "options_url = \"&options=includeAttributes:true\"\n",
    "data_format = \"&format=json\"\n",
    "\n",
    "# Construct the full URL\n",
    "full_url = f\"{base_url}{datasets_url}{station_url}{startdate_url}{enddate_url}{options_url}{data_format}\"\n",
    "token = \"vflMliolsDnHblAoiNDDQgmocEipDpTQ\"\n",
    "\n",
    "# Make the API call\n",
    "headers = {\"Authorization\": api_key}\n",
    "response = requests.get(full_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the response content to JSON\n",
    "    api_json = response.json()\n",
    "    \n",
    "    # Convert JSON to DataFrame\n",
    "    api_df = pd.DataFrame(api_json)\n",
    "    \n",
    "    # Select desired columns\n",
    "    api_df = api_df[['DATE', 'STATION', 'TMIN', 'TMAX', 'PRCP']]\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(api_df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data from the API. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Dinuk Edited\n",
    "_**DELETE BEFORE PUBLISHING**_\n",
    "\n",
    "_This is a template also containing the style guide for use cases. The styling uses the use-case css when uploaded to the website, which will not be visible on your local machine._\n",
    "\n",
    "_Change any text marked with {} and delete any cells marked DELETE_\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE BEFORE PUBLISHING\n",
    "# This is just here so you can preview the styling on your local machine\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".usecase-title, .usecase-duration, .usecase-section-header {\n",
    "    padding-left: 15px;\n",
    "    padding-bottom: 10px;\n",
    "    padding-top: 10px;\n",
    "    padding-right: 15px;\n",
    "    background-color: #0f9295;\n",
    "    color: #fff;\n",
    "}\n",
    "\n",
    ".usecase-title {\n",
    "    font-size: 1.7em;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".usecase-authors, .usecase-level, .usecase-skill {\n",
    "    padding-left: 15px;\n",
    "    padding-bottom: 7px;\n",
    "    padding-top: 7px;\n",
    "    background-color: #baeaeb;\n",
    "    font-size: 1.4em;\n",
    "    color: #121212;\n",
    "}\n",
    "\n",
    ".usecase-level-skill  {\n",
    "    display: flex;\n",
    "}\n",
    "\n",
    ".usecase-level, .usecase-skill {\n",
    "    width: 50%;\n",
    "}\n",
    "\n",
    ".usecase-duration, .usecase-skill {\n",
    "    text-align: right;\n",
    "    padding-right: 15px;\n",
    "    padding-bottom: 8px;\n",
    "    font-size: 1.4em;\n",
    "}\n",
    "\n",
    ".usecase-section-header {\n",
    "    font-weight: bold;\n",
    "    font-size: 1.5em;\n",
    "}\n",
    "\n",
    ".usecase-subsection-header, .usecase-subsection-blurb {\n",
    "    font-weight: bold;\n",
    "    font-size: 1.2em;\n",
    "    color: #121212;\n",
    "}\n",
    "\n",
    ".usecase-subsection-blurb {\n",
    "    font-size: 1em;\n",
    "    font-style: italic;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-title\">{Use Case Name}</div>\n",
    "\n",
    "<div class=\"usecase-authors\"><b>Authored by: </b> {Author/s}</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-duration\"><b>Duration:</b> {90} mins</div>\n",
    "\n",
    "<div class=\"usecase-level-skill\">\n",
    "    <div class=\"usecase-level\"><b>Level: </b>{Intermediate}</div>\n",
    "    <div class=\"usecase-skill\"><b>Pre-requisite Skills: </b>{Python, and add any more skills needed}</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">Scenario</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Using User Story format, write a description of the problem you are trying to solve for this use case.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">What this use case will teach you</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this use case you will:\n",
    "- {list the skills demonstrated in your use case}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"usecase-section-header\">{Heading for introduction or background relating to problem}</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{Write your introduction here. Keep it concise. We're not after \"War and Peace\" but enough background information to inform the reader on the rationale for solving this problem or background non-technical information that helps explain the approach. You may also wish to give information on the datasets, particularly how to source those not being imported from the client's open data portal.}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "_**DELETE BEFORE PUBLISHING**_\n",
    "\n",
    "## Style guide for use cases\n",
    "\n",
    "### Headers\n",
    "\n",
    "For styling within your markdown cells, there are two choices you can use for headers.\n",
    "\n",
    "1) You can use HTML classes specific to the use case styling:\n",
    "\n",
    "```<p class=\"usecase-subsection-header\">This is a subsection header.</p>```\n",
    "\n",
    "<p style=\"font-weight: bold; font-size: 1.2em;\">This is a subsection header.</p>\n",
    "\n",
    "```<p class=\"usecase-subsection-blurb\">This is a blurb header.</p>```\n",
    "\n",
    "<p style=\"font-weight: bold; font-size: 1em; font-style:italic;\">This is a blurb header.</p>\n",
    "\n",
    "\n",
    "2) Or if you like you can use the markdown header styles:\n",
    "\n",
    "```# for h1```\n",
    "\n",
    "```## for h2```\n",
    "\n",
    "```### for h3```\n",
    "\n",
    "```#### for h4```\n",
    "\n",
    "```##### for h5```\n",
    "\n",
    "## Plot colour schemes\n",
    "\n",
    "General advice:\n",
    "1. Use the same colour or colour palette throughout your notebook, unless variety is necessary\n",
    "2. Select a palette based on the type of data being represented\n",
    "3. Consider accessibility (colourblindness, low vision)\n",
    "\n",
    "#### 1) If all of your plots only use 1-2 colors use one of the company style colors:\n",
    "\n",
    "| Light theme | Dark Theme |\n",
    "|-----|-----|\n",
    "|<p style=\"color:#2af598;\">#2af598</p>|<p style=\"color:#08af64;\">#08af64</p>|\n",
    "|<p style=\"color:#22e4ac;\">#22e4ac</p>|<p style=\"color:#14a38e;\">#14a38e</p>|\n",
    "|<p style=\"color:#1bd7bb;\">#1bd7bb</p>|<p style=\"color:#0f9295;\">#0f9295</p>|\n",
    "|<p style=\"color:#14c9cb;\">#14c9cb</p>|<p style=\"color:#056b8a;\">#056b8a</p>|\n",
    "|<p style=\"color:#0fbed8;\">#0fbed8</p>|<p style=\"color:#121212;\">#121212</p>|\n",
    "|<p style=\"color:#08b3e5;\">#08b3e5</p>||\n",
    "\n",
    "\n",
    "#### 2) If your plot needs multiple colors, choose an appropriate palette using either of the following tutorials:\n",
    "- https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "- https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "\n",
    "#### 3) Consider accessibility as well.\n",
    "\n",
    "For qualitative plotting Seaborn's 'colorblind' palette is recommended. For maps with sequential or diverging it is recommended to use one of the Color Brewer schemes which can be previewed at https://colorbrewer2.org/.\n",
    "\n",
    "If you want to design your own colour scheme, it should use the same principles as Cynthia Brewer's research (with variation not only in hue but also, saturation or luminance).\n",
    "\n",
    "### References\n",
    "\n",
    "Be sure to acknowledge your sources and any attributions using links or a reference list.\n",
    "\n",
    "If you have quite a few references, you might wish to have a dedicated section for references at the end of your document, linked using footnote style numbers.\n",
    "\n",
    "You can connect your in-text reference by adding the number with a HTML link: ```<a href=\"#fn-1\">[1]</a>```\n",
    "\n",
    "and add a matching ID in the reference list using the ```<fn>``` tag: ```<fn id=\"fn-1\">[1] Author (Year) _Title_, Publisher, Publication location.</fn>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
