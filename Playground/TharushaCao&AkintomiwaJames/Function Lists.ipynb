{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73df5f01-0229-4f48-b848-f5e28a9abee9",
   "metadata": {},
   "source": [
    "# Function Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ce2b0-3f39-4266-ab1e-13784a396825",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is designed to be a flexible and expandable template for developing and documenting functions for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d479697-12c3-4b26-b224-9b854f06f70e",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "- Data Reduction\n",
    "- Feature Selection\n",
    "- Dimensionality Reduction\n",
    "- Binning / Encoding\n",
    "- Feature Engineering\n",
    "- Text Handling\n",
    "- Data Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb815c75-287b-4a05-a116-1a67444bdc10",
   "metadata": {},
   "source": [
    "## NOTE : This is still in progress "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0775ee-c9c0-4030-aec6-d586099297c4",
   "metadata": {},
   "source": [
    "## Table of Contents\r",
    "1. [Configuration and Setup](#Configuration-and-Setpu)\n",
    "2. [API Get Dataset no key](#Get-Data-No-ApiKey)\n",
    "3. [Pre-Processing Functions](#Pre-Processing-Functions)\n",
    "    - [Dealing with NULL Values (#Finding Missing Data Count )](Dealing-with-NULL-Values-(-Finding-Missing-Data-Count-))\n",
    "    - [Remove(drop), mean , median , mode](#Remove(drop),-mean-,-median-,-mode)\n",
    "    - [Data Combining / Intergration](#Data-Combining-/-Intergration)\n",
    "    - [Normalizing / Feature Scaling](#Normalizing-/-Feature-Scaling)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f687b-2bd4-48f8-b328-65f7a201174d",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "Set up the environment with necessary libraries and configurations, Make sure you have all libraries installed under functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463d7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Libraries used:\n",
    "###################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from io import StringIO\n",
    "from geopy.distance import geodesic\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler,PowerTransformer,MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler,Normalizer,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from haversine import haversine\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784fac1-5169-4f9e-afbc-468d51655fff",
   "metadata": {},
   "source": [
    "## Get Data No ApiKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbcc85b-39a4-4454-ac23-30935b8399c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet unlimited data from the API Function \\n\\nParameters:\\ndatasetname (string): dataset name as from city of melbourn \\napikey (string): the current api Key ( this should be gotton via the below if api stored in current workspace / google drive ( refer to Te API)\\n\\nf = open(\"API.txt\",\"r\")\\napi_key = f.read()\\n\\nReturns:\\nCsv : Returns the csv dataset of the dataset name \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def API_Unlimited(datasetname): # pass in dataset name and api key\n",
    "    dataset_id = datasetname\n",
    "\n",
    "    base_url = 'https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/'\n",
    "    #apikey = api_key\n",
    "    dataset_id = dataset_id\n",
    "    format = 'csv'\n",
    "\n",
    "    url = f'{base_url}{dataset_id}/exports/{format}'\n",
    "    params = {\n",
    "        'select': '*',\n",
    "        'limit': -1,  # all records\n",
    "        'lang': 'en',\n",
    "        'timezone': 'UTC'\n",
    "    }\n",
    "\n",
    "    # GET request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # StringIO to read the CSV data\n",
    "        url_content = response.content.decode('utf-8')\n",
    "        datasetname = pd.read_csv(StringIO(url_content), delimiter=';')\n",
    "        print(datasetname.sample(10, random_state=999)) # Test\n",
    "        return datasetname \n",
    "    else:\n",
    "        return (print(f'Request failed with status code {response.status_code}'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Get unlimited data from the API Function \n",
    "\n",
    "Parameters:\n",
    "datasetname (string): dataset name as from city of melbourn \n",
    "apikey (string): the current api Key ( this should be gotton via the below if api stored in current workspace / google drive ( refer to Te API)\n",
    "\n",
    "f = open(\"API.txt\",\"r\")\n",
    "api_key = f.read()\n",
    "\n",
    "Returns:\n",
    "Csv : Returns the csv dataset of the dataset name \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa6891-cd80-4196-9759-2b040d43c72c",
   "metadata": {},
   "source": [
    "#### Testing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb040b-0288-4543-9794-349472114840",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id_1 = 'litter-traps'\n",
    "dataset_id_2 = 'public-barbecues'\n",
    "dataset_id_3 = 'cafes-and-restaurants-with-seating-capacity'\n",
    "dataset_id_4 = 'argyle-square-air-quality'\n",
    "litter_df = API_Unlimited(dataset_id_1)\n",
    "bbq_df = API_Unlimited(dataset_id_2)\n",
    "cafe_df = API_Unlimited(dataset_id_3)\n",
    "AirQuality_df = API_Unlimited(dataset_id_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be736e",
   "metadata": {},
   "source": [
    "# Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7e8ee-ec0c-4002-bc40-da9a2849663e",
   "metadata": {},
   "source": [
    "## Dealing with NULL Values ( Finding Missing Data Count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8201fb3d-ba98-4b46-94f1-d6f7b970abad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFunction to get column names with count of missing values \\n\\nParameters:\\ndatasetname (string): dataset name as from city of melbourn \\napikey (string): the current api Key ( this should be gotton via the below if api stored in current workspace / google drive ( refer to Te API)\\n\\nf = open(\"API.txt\",\"r\")\\napi_key = f.read()\\n\\nReturns:\\nCsv : Returns the csv dataset of the dataset name \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FindMissingVal(df):\n",
    "  #now lets have a array to store the feature with number of NAN values\n",
    "  MissingFeaturenValues = []\n",
    "  #now we check each column\n",
    "  for column in df.columns:\n",
    "    missingVals = np.sum(df[column].isnull()) # sum the number of NAN values into variable\n",
    "    MissingFeaturenValues.append({'Feature':column ,'Number of Missing Values':missingVals}) #the array consist of dictionary with feature and its missing values\n",
    "  return MissingFeaturenValues\n",
    "\n",
    "\"\"\"\n",
    "Function to get column names with count of missing values \n",
    "\n",
    "Parameters:\n",
    "datasetname (string): dataset name as from city of melbourn \n",
    "apikey (string): the current api Key ( this should be gotton via the below if api stored in current workspace / google drive ( refer to Te API)\n",
    "\n",
    "f = open(\"API.txt\",\"r\")\n",
    "api_key = f.read()\n",
    "\n",
    "Returns:\n",
    "Csv : Returns the csv dataset of the dataset name \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d098d9bf-ba59-494a-86a0-e232673d63e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Feature': 'asset_number', 'Number of Missing Values': 0},\n",
       " {'Feature': 'asset_description', 'Number of Missing Values': 0},\n",
       " {'Feature': 'construct_material_lupvalue', 'Number of Missing Values': 7},\n",
       " {'Feature': 'inspection_frequency', 'Number of Missing Values': 5},\n",
       " {'Feature': 'maintained_by', 'Number of Missing Values': 0},\n",
       " {'Feature': 'object_type_lupvalue', 'Number of Missing Values': 4},\n",
       " {'Feature': 'lat', 'Number of Missing Values': 0},\n",
       " {'Feature': 'lon', 'Number of Missing Values': 0},\n",
       " {'Feature': 'location', 'Number of Missing Values': 0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FindMissingVal(litter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a56f2-1adc-4fd5-9e4b-43ef4fa80d45",
   "metadata": {},
   "source": [
    "### Remove(drop), mean , median , mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba09b02-dff5-48dd-a4e0-566a0eebc05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHandling Missing Values Functions\\n\\nParameters:\\n\\ndataset(dataframe) -  Dataframe you want to deal null values \\ncolumns (array) - a array of all columns you want to handle missing values for the picked action\\nactions (string) - 'remove' , 'mode' , 'mean' , 'median' performs the said actions when selected ( can select one at a time )\\n\\nReturns:\\nDataframe : Returns Dataframe including handled values\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def handle_null_values(dataset, columns, action): # nested conditions\n",
    "    if action == 'remove':\n",
    "        modified_dataset = dataset.dropna(subset=columns)\n",
    "    elif action in ['mean', 'median', 'mode']:\n",
    "        for column in columns:\n",
    "            if dataset[column].isnull().any():  \n",
    "                if action == 'mean':\n",
    "                    fill_value = dataset[column].mean()\n",
    "                elif action == 'median':\n",
    "                    fill_value = dataset[column].median()\n",
    "                elif action == 'mode':\n",
    "                    fill_value = dataset[column].mode()[0]\n",
    "                dataset[column] = dataset[column].fillna(fill_value)\n",
    "        modified_dataset = dataset\n",
    "    else:\n",
    "        raise ValueError(\"Action must be 'remove', 'mean', 'median', or 'mode'\")\n",
    "    return modified_dataset\n",
    "\n",
    "\"\"\"\n",
    "Handling Missing Values Functions\n",
    "\n",
    "Parameters:\n",
    "\n",
    "dataset(dataframe) -  Dataframe you want to deal null values \n",
    "columns (array) - a array of all columns you want to handle missing values for the picked action\n",
    "actions (string) - 'remove' , 'mode' , 'mean' , 'median' performs the said actions when selected ( can select one at a time )\n",
    "\n",
    "Returns:\n",
    "Dataframe : Returns Dataframe including handled values\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1c782-7edd-47f0-8cf5-12f32f36c4b3",
   "metadata": {},
   "source": [
    "#### Testing - I made a array of all columns i want to use mode on and ran function , returns to a new df called modified_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632286ae-c79f-4b44-827d-5d01d9f8075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Usage Example\"\"\"\n",
    "columns=['inspection_frequency','construct_material_lupvalue']\n",
    "modified_mode = handle_null_values(litter_df,columns,'mode') #<========== Pass DATASET and Prefered Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9631f71e-6193-44be-9958-89fc76445990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Feature': 'asset_number', 'Number of Missing Values': 0},\n",
       " {'Feature': 'asset_description', 'Number of Missing Values': 0},\n",
       " {'Feature': 'construct_material_lupvalue', 'Number of Missing Values': 0},\n",
       " {'Feature': 'inspection_frequency', 'Number of Missing Values': 0},\n",
       " {'Feature': 'maintained_by', 'Number of Missing Values': 0},\n",
       " {'Feature': 'object_type_lupvalue', 'Number of Missing Values': 4},\n",
       " {'Feature': 'lat', 'Number of Missing Values': 0},\n",
       " {'Feature': 'lon', 'Number of Missing Values': 0},\n",
       " {'Feature': 'location', 'Number of Missing Values': 0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FindMissingVal(modified_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfcd19-30ba-4b41-9b35-a5050852c15c",
   "metadata": {},
   "source": [
    "### Data Combining / Intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18a1816-8bae-4454-864f-0f6493d166ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCombining multiple datasets\\n\\nParameters:\\n\\ndatasets-  Array of multiple datasets\\nMode - inner , outer , left , right    (JOIN) Default : outer\\n\\nReturns:\\nDataframe : Returns Dataframe combined\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def Combine_Dataset(datasets, mode='outer'):\n",
    "    # Check if no datset is given \n",
    "    if not datasets:\n",
    "        raise ValueError(\"No datasets provided for merging.\")\n",
    "    \n",
    "    #We check if there are any common columns\n",
    "    common_columns = set(datasets[0].columns) # making a SET\n",
    "    for dataset in datasets[1:]:\n",
    "        common_columns.intersection_update(dataset.columns) #Appending if we find any matching \n",
    "        \n",
    "    #Error if no common found \n",
    "    if not common_columns:\n",
    "        raise ValueError(\"No common columns available for combining the datasets. Please give datasets with common columns.\")\n",
    "\n",
    "    #Merge\n",
    "    combined_dataset = datasets[0]\n",
    "    for dataset in datasets[1:]:\n",
    "        combined_dataset = pd.merge(combined_dataset, dataset, how=mode, on=list(common_columns))# combine with mode ( default is outer) with the common columns\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "\"\"\"\n",
    "Combining multiple datasets\n",
    "\n",
    "Parameters:\n",
    "\n",
    "datasets-  Array of multiple datasets\n",
    "Mode - inner , outer , left , right    (JOIN) Default : outer\n",
    "\n",
    "Returns:\n",
    "Dataframe : Returns Dataframe combined\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e16b1-96be-4b51-9d63-b9b12925ec61",
   "metadata": {},
   "source": [
    "#### Testing - I pass a array of all datasets, i want to use mode=inner on and ran the function , returns to a new df called combinedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf67a6d0-d0ea-4d0b-9179-271da667b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = [litter_df,cafe_df] # passing 2 datasets As aRRAY HERE \n",
    "\n",
    "combinedDF = Combine_Dataset(Datasets, mode='outer') # using mode \"outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e4680e-e8e0-41cb-873e-a980202a5f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_number</th>\n",
       "      <th>asset_description</th>\n",
       "      <th>construct_material_lupvalue</th>\n",
       "      <th>inspection_frequency</th>\n",
       "      <th>maintained_by</th>\n",
       "      <th>object_type_lupvalue</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>location</th>\n",
       "      <th>census_year</th>\n",
       "      <th>...</th>\n",
       "      <th>building_address</th>\n",
       "      <th>clue_small_area</th>\n",
       "      <th>trading_name</th>\n",
       "      <th>business_address</th>\n",
       "      <th>industry_anzsic4_code</th>\n",
       "      <th>industry_anzsic4_description</th>\n",
       "      <th>seating_type</th>\n",
       "      <th>number_of_seats</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.77619476202889, 144.93912821315</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52-62 Cade Way PARKVILLE 3052</td>\n",
       "      <td>Parkville</td>\n",
       "      <td>Corner Cafe &amp; Convenience Store</td>\n",
       "      <td>Shop 1, Ground , 52 Cade Way PARKVILLE 3052</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>Cafes and Restaurants</td>\n",
       "      <td>Seats - Outdoor</td>\n",
       "      <td>6.0</td>\n",
       "      <td>144.939128</td>\n",
       "      <td>-37.776195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.77619476202889, 144.93912821315</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52-62 Cade Way PARKVILLE 3052</td>\n",
       "      <td>Parkville</td>\n",
       "      <td>Corner Cafe &amp; Convenience Store</td>\n",
       "      <td>Shop 1, Ground , 52 Cade Way PARKVILLE 3052</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>Cafes and Restaurants</td>\n",
       "      <td>Seats - Indoor</td>\n",
       "      <td>35.0</td>\n",
       "      <td>144.939128</td>\n",
       "      <td>-37.776195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.776194762058005, 144.93912821305003</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52-62 Cade Way PARKVILLE 3052</td>\n",
       "      <td>Parkville</td>\n",
       "      <td>Corner Cafe &amp; Convenience Store</td>\n",
       "      <td>Unit 1, 62-0 Cade Way PARKVILLE 3052</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>Cafes and Restaurants</td>\n",
       "      <td>Seats - Outdoor</td>\n",
       "      <td>6.0</td>\n",
       "      <td>144.939128</td>\n",
       "      <td>-37.776195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.776194762058005, 144.93912821305003</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52-62 Cade Way PARKVILLE 3052</td>\n",
       "      <td>Parkville</td>\n",
       "      <td>Corner Cafe &amp; Convenience Store</td>\n",
       "      <td>Unit 1, 62-0 Cade Way PARKVILLE 3052</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>Cafes and Restaurants</td>\n",
       "      <td>Seats - Indoor</td>\n",
       "      <td>35.0</td>\n",
       "      <td>144.939128</td>\n",
       "      <td>-37.776195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.776194762090775, 144.93912821290002</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52-62 Cade Way PARKVILLE 3052</td>\n",
       "      <td>Parkville</td>\n",
       "      <td>Corner Cafe &amp; Convenience Store</td>\n",
       "      <td>Unit 1, 62-0 Cade Way PARKVILLE 3052</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>Cafes and Restaurants</td>\n",
       "      <td>Seats - Indoor</td>\n",
       "      <td>35.0</td>\n",
       "      <td>144.939128</td>\n",
       "      <td>-37.776195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   asset_number asset_description construct_material_lupvalue  \\\n",
       "0           NaN               NaN                         NaN   \n",
       "1           NaN               NaN                         NaN   \n",
       "2           NaN               NaN                         NaN   \n",
       "3           NaN               NaN                         NaN   \n",
       "4           NaN               NaN                         NaN   \n",
       "\n",
       "  inspection_frequency maintained_by object_type_lupvalue  lat  lon  \\\n",
       "0                  NaN           NaN                  NaN  NaN  NaN   \n",
       "1                  NaN           NaN                  NaN  NaN  NaN   \n",
       "2                  NaN           NaN                  NaN  NaN  NaN   \n",
       "3                  NaN           NaN                  NaN  NaN  NaN   \n",
       "4                  NaN           NaN                  NaN  NaN  NaN   \n",
       "\n",
       "                                  location  census_year  ...  \\\n",
       "0      -37.77619476202889, 144.93912821315       2013.0  ...   \n",
       "1      -37.77619476202889, 144.93912821315       2013.0  ...   \n",
       "2  -37.776194762058005, 144.93912821305003       2010.0  ...   \n",
       "3  -37.776194762058005, 144.93912821305003       2010.0  ...   \n",
       "4  -37.776194762090775, 144.93912821290002       2011.0  ...   \n",
       "\n",
       "                building_address  clue_small_area  \\\n",
       "0  52-62 Cade Way PARKVILLE 3052        Parkville   \n",
       "1  52-62 Cade Way PARKVILLE 3052        Parkville   \n",
       "2  52-62 Cade Way PARKVILLE 3052        Parkville   \n",
       "3  52-62 Cade Way PARKVILLE 3052        Parkville   \n",
       "4  52-62 Cade Way PARKVILLE 3052        Parkville   \n",
       "\n",
       "                      trading_name  \\\n",
       "0  Corner Cafe & Convenience Store   \n",
       "1  Corner Cafe & Convenience Store   \n",
       "2  Corner Cafe & Convenience Store   \n",
       "3  Corner Cafe & Convenience Store   \n",
       "4  Corner Cafe & Convenience Store   \n",
       "\n",
       "                              business_address industry_anzsic4_code  \\\n",
       "0  Shop 1, Ground , 52 Cade Way PARKVILLE 3052                4511.0   \n",
       "1  Shop 1, Ground , 52 Cade Way PARKVILLE 3052                4511.0   \n",
       "2         Unit 1, 62-0 Cade Way PARKVILLE 3052                4511.0   \n",
       "3         Unit 1, 62-0 Cade Way PARKVILLE 3052                4511.0   \n",
       "4         Unit 1, 62-0 Cade Way PARKVILLE 3052                4511.0   \n",
       "\n",
       "  industry_anzsic4_description     seating_type  number_of_seats   longitude  \\\n",
       "0        Cafes and Restaurants  Seats - Outdoor              6.0  144.939128   \n",
       "1        Cafes and Restaurants   Seats - Indoor             35.0  144.939128   \n",
       "2        Cafes and Restaurants  Seats - Outdoor              6.0  144.939128   \n",
       "3        Cafes and Restaurants   Seats - Indoor             35.0  144.939128   \n",
       "4        Cafes and Restaurants   Seats - Indoor             35.0  144.939128   \n",
       "\n",
       "    latitude  \n",
       "0 -37.776195  \n",
       "1 -37.776195  \n",
       "2 -37.776195  \n",
       "3 -37.776195  \n",
       "4 -37.776195  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88bb69-a127-4638-9373-9d1193eca4ac",
   "metadata": {},
   "source": [
    "### Normalizing / Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a61d72-87e0-437b-9e95-f378012a6a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScaling Features in dataset\\n\\nParameters:\\n\\ndataframe-  Array of multiple datasets\\ncolumns - array of all columns/features to normalize or scale \\nmethod -  minmax , zscore , powertransformer , absscalar , robustscalar , normalizer , quantile . Default : minmax\\n\\nReturns:\\nDataframe : Returns Dataframe Scaled/Normalized\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def Scale_data(dataframe, columns, method='minmax'):\n",
    "\n",
    "    #Copy so we dont change original dataFrame\n",
    "    df_scaled = dataframe.copy()\n",
    "    \n",
    "    # Check if all specified columns exist in the DataFrame\n",
    "    if not all(col in df_scaled.columns for col in columns):\n",
    "        missing_cols = [col for col in columns if col not in df_scaled.columns]\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {missing_cols}\")\n",
    "    \n",
    "    # Select the normalization method nested if \n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'zscore':\n",
    "        scaler = PowerTransformer()\n",
    "    elif method == 'zscore':\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif method == 'zscore':\n",
    "        scaler = RobustScaler()\n",
    "    elif method == 'zscore':\n",
    "        scaler = Normalizer()\n",
    "    elif method == 'zscore':\n",
    "        scaler = QuantileTransformer()\n",
    "    else:\n",
    "        raise ValueError(\"Please Enter one scalar method : minmax , zscore , powertransformer , absscalar , robustscalar , normalizer , quantile\") #exception\n",
    "\n",
    "    # Use the selected scalar\n",
    "    df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "\"\"\"\n",
    "Scaling Features in dataset\n",
    "\n",
    "Parameters:\n",
    "\n",
    "dataframe-  Array of multiple datasets\n",
    "columns - array of all columns/features to normalize or scale \n",
    "method -  minmax , zscore , powertransformer , absscalar , robustscalar , normalizer , quantile . Default : minmax\n",
    "\n",
    "Returns:\n",
    "Dataframe : Returns Dataframe Scaled/Normalized\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1e557-9bd2-457e-a933-dd8f64e584af",
   "metadata": {},
   "source": [
    "### Get column names in a list ( Easy to copy over )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5c9ce3-32d6-4294-a38f-63e042e9247c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'dev_id',\n",
       " 'sensor_name',\n",
       " 'lat_long',\n",
       " 'averagespl',\n",
       " 'carbonmonoxide',\n",
       " 'humidity',\n",
       " 'ibatt',\n",
       " 'nitrogendioxide',\n",
       " 'ozone',\n",
       " 'particulateserr',\n",
       " 'particulatesvsn',\n",
       " 'peakspl',\n",
       " 'pm1',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'temperature',\n",
       " 'vbatt',\n",
       " 'vpanel']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(AirQuality_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2454c0-d47f-4cf0-b97a-aafba3771673",
   "metadata": {},
   "source": [
    "#### Testing : Adding columns I want to scale on AirQuality DF in function , selecting minmax scalar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ee4f6b-5e54-4761-a5ef-206b22c31e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>dev_id</th>\n",
       "      <th>sensor_name</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>averagespl</th>\n",
       "      <th>carbonmonoxide</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ibatt</th>\n",
       "      <th>nitrogendioxide</th>\n",
       "      <th>ozone</th>\n",
       "      <th>particulateserr</th>\n",
       "      <th>particulatesvsn</th>\n",
       "      <th>peakspl</th>\n",
       "      <th>pm1</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>temperature</th>\n",
       "      <th>vbatt</th>\n",
       "      <th>vpanel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-09T09:02:38+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.136969</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.733945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.075697</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.261421</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-09T11:17:37+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.115559</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.877953</td>\n",
       "      <td>0.322523</td>\n",
       "      <td>0.768807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.095618</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.225888</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-09T11:32:37+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.115559</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.869423</td>\n",
       "      <td>0.322523</td>\n",
       "      <td>0.768807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.215736</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-09T12:17:37+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.102704</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.339940</td>\n",
       "      <td>0.776147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.200508</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-09T13:47:36+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.115559</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.912730</td>\n",
       "      <td>0.333934</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.163347</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.180203</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-06-09T14:02:36+00:00</td>\n",
       "      <td>ems-ec8a</td>\n",
       "      <td>Air Quality Sensor 2</td>\n",
       "      <td>-37.802772, 144.9655513</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.094149</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.879921</td>\n",
       "      <td>0.333934</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.099602</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.182741</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time    dev_id           sensor_name  \\\n",
       "0  2020-06-09T09:02:38+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "1  2020-06-09T11:17:37+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "2  2020-06-09T11:32:37+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "3  2020-06-09T12:17:37+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "4  2020-06-09T13:47:36+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "5  2020-06-09T14:02:36+00:00  ems-ec8a  Air Quality Sensor 2   \n",
       "\n",
       "                  lat_long  averagespl  carbonmonoxide  humidity     ibatt  \\\n",
       "0  -37.802772, 144.9655513       0.100        0.136969  0.583333  0.866142   \n",
       "1  -37.802772, 144.9655513       0.075        0.115559  0.619048  0.877953   \n",
       "2  -37.802772, 144.9655513       0.075        0.115559  0.630952  0.869423   \n",
       "3  -37.802772, 144.9655513       0.125        0.102704  0.666667  0.874016   \n",
       "4  -37.802772, 144.9655513       0.075        0.115559  0.714286  0.912730   \n",
       "5  -37.802772, 144.9655513       0.125        0.094149  0.738095  0.879921   \n",
       "\n",
       "   nitrogendioxide     ozone  particulateserr  particulatesvsn   peakspl  \\\n",
       "0         0.299700  0.733945              0.0              1.0  0.241379   \n",
       "1         0.322523  0.768807              0.0              1.0  0.120690   \n",
       "2         0.322523  0.768807              0.0              1.0  0.224138   \n",
       "3         0.339940  0.776147              0.0              1.0  0.327586   \n",
       "4         0.333934  0.785321              0.0              1.0  0.120690   \n",
       "5         0.333934  0.785321              0.0              1.0  0.275862   \n",
       "\n",
       "        pm1      pm10      pm25  temperature  vbatt  vpanel  \n",
       "0  0.100000  0.075697  0.001002     0.261421   3.96     0.0  \n",
       "1  0.125000  0.095618  0.001296     0.225888   3.93     0.0  \n",
       "2  0.158333  0.115538  0.001414     0.215736   3.92     0.0  \n",
       "3  0.166667  0.115538  0.001709     0.200508   3.91     0.0  \n",
       "4  0.200000  0.163347  0.002121     0.180203   3.89     0.0  \n",
       "5  0.133333  0.099602  0.001296     0.182741   3.89     0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # minmax , zscore , powertransformer , absscalar , robustscalar , normalizer , quantile Are Current Supported \n",
    "\n",
    "Scaled_min_max_df = Scale_data(AirQuality_df, ['averagespl','carbonmonoxide','humidity','ibatt','nitrogendioxide','ozone','particulateserr','particulatesvsn','peakspl','pm1','pm10','pm25','temperature'], method='minmax')\n",
    "Scaled_min_max_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a8f70",
   "metadata": {},
   "source": [
    "## Point to point distance calculator minimum ( Thomas )( NEED FIXING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12444ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the minimum distance from a point to any point in a list\n",
    "\"\"\"\n",
    "Calculate the minimum geodesic distance from a point to any point in a given list.\n",
    "\n",
    "Parameters:\n",
    "point (tuple): A tuple representing the coordinates (latitude, longitude) of the point.\n",
    "list_of_points (list of tuples): A list of tuples, each representing coordinates (latitude, longitude) of points to compare against.\n",
    "\n",
    "Returns:\n",
    "float: The minimum Euclidean distance from the given point to the closest point in the list.\n",
    "\"\"\"\n",
    "\n",
    "def min_distance(point, list_of_points): \n",
    "    return min([geodesic(point, pt).meters for pt in list_of_points]) #get min dis\n",
    "\n",
    "#example :\n",
    "\n",
    "\n",
    "row = {'lat': 40.7128, 'lon': -74.0060}\n",
    "# Call the lambda function with the row as an argument\n",
    "value = lambda row: min_distance((row['lat'], row['lon']), bbq_coords)\n",
    "# Get the result by calling the lambda function\n",
    "result = value(row)\n",
    "# Print the result\n",
    "print(\"test distance in meters :\",result)\n",
    "\n",
    "# example used in dataset :\n",
    "\n",
    "\n",
    "litter_df['Nearest BBQ Distance (m)'] = litter_df.apply(lambda row: min_distance((row['lat'], row['lon']), bbq_coords), axis=1)\n",
    "#creates a new column for nearest distance to a point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c03b",
   "metadata": {},
   "source": [
    "## Point to point distance calculator maximum ( NEED FIXING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e45ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the maximum distance from a point to any point in a list\n",
    "\"\"\"\n",
    "Calculate the maximum geodesic distance from a point to any point in a given list.\n",
    "\n",
    "Parameters:\n",
    "point (tuple): A tuple representing the coordinates (latitude, longitude) of the point.\n",
    "list_of_points (list of tuples): A list of tuples, each representing coordinates (latitude, longitude) of points to compare against.\n",
    "\n",
    "Returns:\n",
    "float: The maximum Euclidean distance from the given point to the closest point in the list.\n",
    "\"\"\"\n",
    "\n",
    "def max_distance(point, list_of_points): \n",
    "    return max([geodesic(point, pt).meters for pt in list_of_points]) #get min dis\n",
    "\n",
    "#example : \n",
    "\n",
    "value = max_distance((row['lat'], row['lon']), bbq_coords) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0822ea",
   "metadata": {},
   "source": [
    "## Number of points in a given radius ( NEED FIXING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989be9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Number of points in a radius from a point \n",
    "\"\"\"\n",
    "Calculate the number of geodesic distances from a point to any point in a given list.\n",
    "\n",
    "Parameters:\n",
    "center_point (tuple): A tuple representing the coordinates (latitude, longitude) of the point.\n",
    "list_of_points (list of tuples): A list of tuples, each representing coordinates (latitude, longitude) of points to compare against.\n",
    "radius_meters\n",
    "\n",
    "Returns:\n",
    "INT: The Number of points in the radius given\n",
    "\"\"\"\n",
    "\n",
    "def count_points_in_radius(center_point, list_of_points, radius_meters):\n",
    "    count = sum(1 for pt in list_of_points if geodesic(center_point, pt).meters <= radius_meters)\n",
    "    return count\n",
    "\n",
    "#Example into dataset : \n",
    "\n",
    "#========Parameter 1 : Centur point \n",
    "#========Parameter 2 : all coordinate points [must be list form , see example ]\n",
    "#========Parameter 3 : radius \n",
    "\n",
    "radius = 100\n",
    "litter_df['Number of Nearby Points in Radius'] = litter_df.apply(lambda row: count_points_in_radius((row['lat'], row['lon']), bbq_coords + cafe_coords,radius),axis=1)\n",
    "\n",
    "# Example ( singular ) :\n",
    "\n",
    "values = count_points_in_radius((row['lat'], row['lon']),cafe_coords,radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9d3e1",
   "metadata": {},
   "source": [
    "## The Map using folium ( basic ) ( NEED FIXING )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a643f8-6b45-43cd-907a-bfd3ab3889bf",
   "metadata": {},
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Calculate the minimum geodesic distance from a point to any point in a given list.\n",
    "\n",
    "Parameters:\n",
    "dataframe : A datset representing the coordinates (latitude, longitude) of the index and also other values hence when\n",
    "using this we can also include other things from the dataset in the map , when using the html legend\n",
    "\n",
    "Returns:\n",
    "Map: The folium based map is returned\n",
    "\"\"\"\n",
    "\n",
    "def map_func(PointsDatasets,):\n",
    "    # Create a folium map centered at the mean coordinates of litter traps / intial setup\n",
    "    map_center = [PointsDatasets['lat'].mean(), PointsDatasets['lon'].mean()]\n",
    "    mymap = folium.Map(location=map_center, zoom_start=13)\n",
    "    \n",
    "    # Add circles for the points\n",
    "    for index, row in PointsDatasets.iterrows():\n",
    "        location = [row['lat'], row['lon']] \n",
    "        # Add a circle for the radius around the litter trap\n",
    "        folium.Circle(\n",
    "            location=location,\n",
    "            radius=30,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_opacity=0.2\n",
    "        ).add_to(mymap)\n",
    "    return mymap\n",
    "\n",
    "# Example usage ========================= Pass in your function =================\n",
    "\"\"\"Make sure your dataframe has a column with both lat and lon\"\"\"\n",
    "map_func(litter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c2e6c-d132-4f22-9bbb-100abd7a4111",
   "metadata": {},
   "source": [
    "### correlation heat map for spearman and pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44d81917-34fc-4561-a20c-2aaf3d5333b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def plot_correlation_heatmaps(data, labels, order=None):\n",
    "    \"\"\"\n",
    "    Plots Pearson and Spearman correlation heatmaps for the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array or DataFrame containing the data to analyze.\n",
    "    - labels: List of column names corresponding to the data.\n",
    "    - order: List of indices specifying the order of columns for aesthetic purposes in the heatmap.\n",
    "\n",
    "    The function creates a figure with two subplots: one for Pearson correlation and one for Spearman correlation.\n",
    "    \"\"\"\n",
    "    if order is None:\n",
    "        order = range(len(labels))  # Default order if none provided\n",
    "\n",
    "    # Compute Pearson correlation coefficients\n",
    "    R = np.corrcoef(data, rowvar=False)\n",
    "\n",
    "    # Compute Spearman's rank correlation\n",
    "    rho, pval = spearmanr(data, axis=0)\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot Pearson correlation heatmap\n",
    "    ax1.set_title('Pearson Correlation')\n",
    "    plt.sca(ax1)\n",
    "    corrheatmap(R[np.ix_(order, order)], np.array(labels)[order])\n",
    "\n",
    "    # Plot Spearman correlation heatmap\n",
    "    ax2.set_title('Spearman Correlation')\n",
    "    plt.sca(ax2)\n",
    "    corrheatmap(rho[np.ix_(order, order)], np.array(labels)[order])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def corrheatmap(R, labels):\n",
    "    \"\"\"\n",
    "    Helper function to draw a correlation heat map.\n",
    "    \"\"\"\n",
    "    k = len(labels)\n",
    "    plt.imshow(R, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    plt.xticks(np.arange(k), labels=labels, rotation=45)\n",
    "    plt.yticks(np.arange(k), labels=labels)\n",
    "    plt.colorbar()\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            plt.text(j, i, f\"{R[i, j]:.2f}\", ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if np.abs(R[i, j]) > 0.5 else \"black\")\n",
    "    plt.grid(False)\n",
    "\n",
    "# Usage example\n",
    "# data = np.random.rand(100, 5)  # Dummy data\n",
    "# labels = ['Var1', 'Var2', 'Var3', 'Var4', 'Var5']\n",
    "# order = [0, 1, 2, 3, 4]\n",
    "# plot_correlation_heatmaps(data, labels, order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b92dd29c-b94e-47e9-a104-7b3406c279cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def optimal_k_clusters(data, k_range):\n",
    "    \"\"\"\n",
    "    Determines the optimal number of clusters for K-means clustering based on silhouette scores.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset on which clustering is to be performed.\n",
    "    - k_range: A range of k values to test. Typically, this is a range object.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_k: The optimal number of clusters with the highest silhouette score.\n",
    "    - Plots the silhouette scores for each k in k_range.\n",
    "    \"\"\"\n",
    "    # List to store silhouette scores for each value of k\n",
    "    silh_scores = []\n",
    "\n",
    "    # Iterate over each value of k in the range provided\n",
    "    for k in k_range:\n",
    "        # Fit KMeans clustering model to the data with 'k' clusters\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10) # n_init=10 to ensure consistency across initializations\n",
    "        cluster_labels = kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate the silhouette score for the current number of clusters\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        silh_scores.append(silhouette_avg)\n",
    "\n",
    "    # Determine the value of k that has the maximum silhouette score\n",
    "    optimal_k = k_range[np.argmax(silh_scores)]\n",
    "    print(\"Optimal number of clusters (k):\", optimal_k)\n",
    "\n",
    "    # Plot the silhouette scores against the number of clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, silh_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score for Different Values of k')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Example of how to use the function\n",
    "# data = your_data_frame  # make sure to define your DataFrame\n",
    "# k_range = range(2, 11)  # Setting a range from 2 to 10\n",
    "# optimal_k = optimal_k_clusters(data, k_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbded8f-ef52-44f9-99bf-00b73545800f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54da1c85-df69-464e-ac4f-1a115b9f34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "def find_optimal_clusters(data, k_range=(2, 12)):\n",
    "    \"\"\"\n",
    "    Determines the optimal number of clusters for K-means clustering using the elbow method and plots the results.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset on which clustering is to be performed, typically preprocessed (e.g., PCA-transformed).\n",
    "    - k_range: A tuple indicating the range of k values to test (inclusive). Default is (2, 12).\n",
    "\n",
    "    Returns:\n",
    "    - Plots the elbow plot showing the distortion for each k, helping to identify the optimal number of clusters.\n",
    "    \"\"\"\n",
    "    # Initialize the KMeans model with a fixed number of initializations to avoid random seed variability\n",
    "    model = KMeans(n_init=10)\n",
    "\n",
    "    # Initialize the KElbowVisualizer with the KMeans model, specifying the range of k and the metric 'distortion'\n",
    "    visualizer = KElbowVisualizer(\n",
    "        model, k=k_range, metric='distortion', timings=False\n",
    "    )\n",
    "\n",
    "    # Fit the visualizer to the data\n",
    "    visualizer.fit(data)\n",
    "\n",
    "    # Finalize and render the figure\n",
    "    visualizer.show()\n",
    "\n",
    "# Example of how to use the function\n",
    "# X_pca = your_pca_transformed_data  # Ensure your data is appropriately preprocessed, e.g., using PCA\n",
    "# find_optimal_clusters(X_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229c77c-b181-46fa-82c2-cadb09a4eadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
