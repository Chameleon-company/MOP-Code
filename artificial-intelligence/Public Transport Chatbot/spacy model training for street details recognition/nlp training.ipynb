{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc75b2-5a2c-4e7d-88f6-331573f5e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources = \"https://zen10.com.au/melbourne-suburb-list/\"\n",
    "#\"https://data.melbourne.vic.gov.au/explore/dataset/street-names/information/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71be500-0c65-43b6-8d9b-8586a22bc326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extracted_street_names.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r'C:\\Users\\logan\\Desktop\\Uni\\Team proj\\Mapping\\street-names.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Extract the street names column\n",
    "street_names = df['name'].unique()\n",
    "\n",
    "#Clean the street names to remove extra spaces or format issues\n",
    "cleaned_street_names = [name.strip() for name in street_names if pd.notnull(name)]\n",
    "\n",
    "#Save the street names to a text file\n",
    "output_file_path = 'extracted_street_names.txt'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for street in cleaned_street_names:\n",
    "        f.write(f\"{street}\\n\")\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aabb5916-3c09-4186-8746-a75e28ba9c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'annotated_sentences.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#Load street names and suburb names\n",
    "with open('extracted_street_names.txt', 'r') as f:\n",
    "    street_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open('melbourne_suburbs_list.txt', 'r') as f:\n",
    "    suburb_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "#templates for indicating current location\n",
    "current_location_templates = [\n",
    "    \"I am currently at {street_number} {street}.\",\n",
    "    \"I'm at {street_number} {street}, {suburb}.\",\n",
    "    \"I am here at {street_number} {street}.\",\n",
    "    \"Currently, I am on {street}.\",\n",
    "    \"I am at {street_number} {street} in {suburb}.\",\n",
    "    \"I'm on {street} in {suburb}.\",\n",
    "    \"I am here in {suburb}.\",\n",
    "    \"Iâ€™m at {street_number} {street}.\"\n",
    "]\n",
    "\n",
    "#templates for indicating destination\n",
    "destination_templates = [\n",
    "    \"I need to get to {street_number} {street}.\",\n",
    "    \"I need directions to {street} in {suburb}.\",\n",
    "    \"Can you guide me to {street_number} {street}, {suburb}?\",\n",
    "    \"How do I get to {street_number} {street}?\",\n",
    "    \"I need to go to {suburb}.\",\n",
    "    \"I'm heading towards {street_number} {street} in {suburb}.\",\n",
    "    \"I am trying to reach {street}.\",\n",
    "    \"I need to get here: {street_number} {street}.\",\n",
    "    \"Where is {street_number} {street} located?\"\n",
    "]\n",
    "\n",
    "#Function to generate random sentences with street numbers and annotations\n",
    "def generate_annotated_data(street_names, suburb_names, current_templates, destination_templates, num_samples=1500):\n",
    "    annotated_data = []\n",
    "    for _ in range(num_samples):\n",
    "        #Randomly choose to generate a current location or destination sentence\n",
    "        if random.choice([True, False]):\n",
    "            #Generate current location sentence\n",
    "            template = random.choice(current_templates)\n",
    "        else:\n",
    "            #Generate destination sentence\n",
    "            template = random.choice(destination_templates)\n",
    "\n",
    "        #Determine if placeholders exist in template\n",
    "        include_street_number = '{street_number}' in template\n",
    "        include_street = '{street}' in template\n",
    "        include_suburb = '{suburb}' in template\n",
    "\n",
    "        #Generate random values for placeholders\n",
    "        street_number = random.randint(1, 1000) if include_street_number else ''\n",
    "        street = random.choice(street_names) if include_street else ''\n",
    "        suburb = random.choice(suburb_names) if include_suburb else ''\n",
    "\n",
    "        #Format the sentence\n",
    "        sentence = template.format(street_number=street_number, street=street, suburb=suburb)\n",
    "        \n",
    "        #Create entity annotations\n",
    "        entities = []\n",
    "        if street:\n",
    "            start = sentence.index(street)\n",
    "            entities.append((start, start + len(street), \"STREET_NAME\"))\n",
    "        if suburb:\n",
    "            start = sentence.index(suburb)\n",
    "            entities.append((start, start + len(suburb), \"SUBURB\"))\n",
    "        if street_number:\n",
    "            start = sentence.index(str(street_number))\n",
    "            entities.append((start, start + len(str(street_number)), \"STREET_NUMBER\"))\n",
    "        \n",
    "        annotated_data.append((sentence, entities))\n",
    "    \n",
    "    return annotated_data\n",
    "\n",
    "#Generate annotated data and convert to df and save\n",
    "annotated_data = generate_annotated_data(street_names, suburb_names, current_location_templates, destination_templates, num_samples=1500)\n",
    "\n",
    "annotated_df = pd.DataFrame(annotated_data, columns=[\"sentence\", \"entities\"])\n",
    "\n",
    "output_csv_path = 'annotated_sentences.csv'\n",
    "annotated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "output_csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80c8bee8-58d7-4de3-8e0f-b08079c74c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\anaconda3\\envs\\rasa_env\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I need to get to 607 CL1056A.\" with entities \"[(21, 28, 'STREET_NAME'), (17, 20, 'STREET_NUMBER'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\logan\\anaconda3\\envs\\rasa_env\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I need to get to 691 CL1619  PRIV..\" with entities \"[(21, 34, 'STREET_NAME'), (17, 20, 'STREET_NUMBER'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 1989.923194047918}\n",
      "Epoch 2, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 86.93376884912736}\n",
      "Epoch 3, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 30.376093826680638}\n",
      "Epoch 4, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 30.54763240443067}\n",
      "Epoch 5, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 12.854508668442213}\n",
      "Epoch 6, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 14.610203881869209}\n",
      "Epoch 7, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 9.536862214620959}\n",
      "Epoch 8, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 3.9137060054542694}\n",
      "Epoch 9, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 0.9962643049841222}\n",
      "Epoch 10, Losses: {'tok2vec': 0.0, 'tagger': 0.0, 'parser': 0.0, 'ner': 5.2389285653010855}\n",
      "250 STREET_NUMBER\n",
      "Flinders Street SUBURB\n",
      "Richmond SUBURB\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Access the NER component of the pipeline\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "#Add new labels to the NER component\n",
    "for label in [\"STREET_NAME\", \"SUBURB\", \"STREET_NUMBER\"]:\n",
    "    if label not in ner.labels:\n",
    "        ner.add_label(label)\n",
    "\n",
    "#Load training data from CSV\n",
    "training_data = []\n",
    "df = pd.read_csv('annotated_sentences.csv')\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    entities = eval(row['entities']) \n",
    "    annotations = {\"entities\": entities}\n",
    "    training_data.append((sentence, annotations))\n",
    "\n",
    "#Convert the training data to SpaCy examples\n",
    "examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in training_data]\n",
    "\n",
    "#Training\n",
    "optimizer = nlp.resume_training()\n",
    "for epoch in range(10):  \n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    for batch in spacy.util.minibatch(examples, size=8):\n",
    "        nlp.update(batch, drop=0.5, sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch + 1}, Losses: {losses}\")\n",
    "\n",
    "#Save the model\n",
    "nlp.to_disk(\"fine_tuned_ner_model\")\n",
    "\n",
    "#Test the model\n",
    "nlp_test = spacy.load(\"fine_tuned_ner_model\")\n",
    "doc = nlp_test(\"I need directions to 250 Flinders Street, Richmond.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d7d3c6b-b199-48b2-ab0c-c0c405b31532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\logan\\anaconda3\\envs\\rasa_env\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I am at 300 Collins Street in Melbourne.\" with entities \"[(9, 12, 'STREET_NUMBER'), (13, 27, 'STREET_NAME')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\logan\\anaconda3\\envs\\rasa_env\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Can you guide me to 450 Bourke Street?\" with entities \"[(18, 21, 'STREET_NUMBER'), (22, 35, 'STREET_NAME'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'tag_acc': None,\n",
       " 'sents_p': None,\n",
       " 'sents_r': None,\n",
       " 'sents_f': None,\n",
       " 'dep_uas': None,\n",
       " 'dep_las': None,\n",
       " 'dep_las_per_type': None,\n",
       " 'pos_acc': None,\n",
       " 'morph_acc': None,\n",
       " 'morph_micro_p': None,\n",
       " 'morph_micro_r': None,\n",
       " 'morph_micro_f': None,\n",
       " 'morph_per_feat': None,\n",
       " 'lemma_acc': None,\n",
       " 'ents_p': None,\n",
       " 'ents_r': None,\n",
       " 'ents_f': None,\n",
       " 'ents_per_type': None,\n",
       " 'speed': 2239.502332878182}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validating output\n",
    "validation_data = [\n",
    "    (\"I am at 300 Collins Street in Melbourne.\", {\"entities\": [(9, 12, \"STREET_NUMBER\"), (13, 27, \"STREET_NAME\"), (31, 40, \"SUBURB\")]}),\n",
    "    (\"Can you guide me to 450 Bourke Street?\", {\"entities\": [(18, 21, \"STREET_NUMBER\"), (22, 35, \"STREET_NAME\")]}),\n",
    "]\n",
    "\n",
    "#Convert validation data to spacy examples\n",
    "validation_examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in validation_data]\n",
    "\n",
    "#Evaluate\n",
    "nlp.evaluate(validation_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5fad9ff-f2ae-4dd7-b3a6-cd19ee386618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: None\n",
      "Recall: None\n",
      "F1 Score: None\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and get metrics\n",
    "results = nlp.evaluate(validation_examples)\n",
    "\n",
    "print(\"Precision:\", results[\"ents_p\"])\n",
    "print(\"Recall:\", results[\"ents_r\"])\n",
    "print(\"F1 Score:\", results[\"ents_f\"])\n",
    "print(\"Accuracy:\", results[\"token_acc\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
