{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5ded5b-a560-4153-b4ff-dcd87fc7c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96936026-8bbd-4e67-8b04-5920a0bbef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 165ms/step - accuracy: 0.2928 - loss: 1.7601 - val_accuracy: 0.4223 - val_loss: 1.5227\n",
      "Epoch 2/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 156ms/step - accuracy: 0.4110 - loss: 1.5113 - val_accuracy: 0.4646 - val_loss: 1.4030\n",
      "Epoch 3/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 144ms/step - accuracy: 0.4588 - loss: 1.4099 - val_accuracy: 0.4891 - val_loss: 1.3549\n",
      "Epoch 4/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 146ms/step - accuracy: 0.4812 - loss: 1.3406 - val_accuracy: 0.5095 - val_loss: 1.3068\n",
      "Epoch 5/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 148ms/step - accuracy: 0.5083 - loss: 1.2792 - val_accuracy: 0.5098 - val_loss: 1.2763\n",
      "Epoch 6/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 147ms/step - accuracy: 0.5329 - loss: 1.2274 - val_accuracy: 0.5209 - val_loss: 1.2565\n",
      "Epoch 7/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 138ms/step - accuracy: 0.5465 - loss: 1.1764 - val_accuracy: 0.5245 - val_loss: 1.2405\n",
      "Epoch 8/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 148ms/step - accuracy: 0.5668 - loss: 1.1243 - val_accuracy: 0.5223 - val_loss: 1.2514\n",
      "Epoch 9/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 145ms/step - accuracy: 0.5815 - loss: 1.0894 - val_accuracy: 0.5351 - val_loss: 1.2375\n",
      "Epoch 10/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 151ms/step - accuracy: 0.5979 - loss: 1.0397 - val_accuracy: 0.5365 - val_loss: 1.2322\n",
      "Epoch 11/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 142ms/step - accuracy: 0.6207 - loss: 0.9985 - val_accuracy: 0.5389 - val_loss: 1.2370\n",
      "Epoch 12/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 142ms/step - accuracy: 0.6278 - loss: 0.9522 - val_accuracy: 0.5347 - val_loss: 1.2552\n",
      "Epoch 13/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 142ms/step - accuracy: 0.6467 - loss: 0.8986 - val_accuracy: 0.5382 - val_loss: 1.2752\n",
      "Epoch 14/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 146ms/step - accuracy: 0.6728 - loss: 0.8550 - val_accuracy: 0.5359 - val_loss: 1.3353\n",
      "Epoch 15/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 143ms/step - accuracy: 0.6764 - loss: 0.8213 - val_accuracy: 0.5454 - val_loss: 1.3120\n",
      "Epoch 16/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 145ms/step - accuracy: 0.6913 - loss: 0.7843 - val_accuracy: 0.5391 - val_loss: 1.3640\n",
      "Epoch 17/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 145ms/step - accuracy: 0.7041 - loss: 0.7484 - val_accuracy: 0.5419 - val_loss: 1.3507\n",
      "Epoch 18/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 145ms/step - accuracy: 0.7187 - loss: 0.7155 - val_accuracy: 0.5414 - val_loss: 1.4563\n",
      "Epoch 19/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 147ms/step - accuracy: 0.7244 - loss: 0.6902 - val_accuracy: 0.5450 - val_loss: 1.4739\n",
      "Epoch 20/20\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 145ms/step - accuracy: 0.7449 - loss: 0.6431 - val_accuracy: 0.5453 - val_loss: 1.5209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.5469 - loss: 1.5085\n",
      "Validation Accuracy: 54.53%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"fer2013_combined.csv\")\n",
    "\n",
    "# Convert pixels column to NumPy arrays\n",
    "X = np.array([np.fromstring(pixels, dtype=np.uint8, sep=' ').reshape(48, 48, 1) for pixels in df[\"pixels\"]])\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = to_categorical(df[\"emotion\"], num_classes=7)\n",
    "\n",
    "# Normalize pixel values (scale between 0 and 1)\n",
    "X = X / 255.0\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # 7 output classes (angry, disgust, fear, etc.)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"emotion_detection_model.h5\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb9224a-7e7a-4c4e-8c02-e6fd3b7e4bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Predicted Emotion: Surprise\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"emotion_detection_model.h5\")\n",
    "\n",
    "# Recompile the model (optional but recommended to avoid the warning)\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define emotion labels (to map the predictions to human-readable emotions)\n",
    "EMOTION_LABELS = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprise\"\n",
    "}\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize the image to 48x48 pixels\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "    \n",
    "    # Normalize pixel values (scale between 0 and 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Reshape to add batch dimension (1 image, 48x48 pixels, 1 color channel)\n",
    "    img = np.reshape(img, (1, 48, 48, 1))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Path to the test image\n",
    "image_path = r\"C:\\Users\\HP\\SAD.jpg\"  # Corrected image path\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image_path)\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(processed_image)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Get the emotion label corresponding to the predicted class\n",
    "predicted_emotion = EMOTION_LABELS[predicted_class]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22bea44-0d97-45bc-8c21-4198e9fe6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 625ms/step - accuracy: 0.2274 - loss: 2.2605 - val_accuracy: 0.2621 - val_loss: 1.9755 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 594ms/step - accuracy: 0.2999 - loss: 1.7182 - val_accuracy: 0.3344 - val_loss: 1.6625 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 613ms/step - accuracy: 0.3474 - loss: 1.6394 - val_accuracy: 0.4122 - val_loss: 1.5237 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 591ms/step - accuracy: 0.3861 - loss: 1.5593 - val_accuracy: 0.3656 - val_loss: 1.6195 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 599ms/step - accuracy: 0.4008 - loss: 1.5267 - val_accuracy: 0.4072 - val_loss: 1.4782 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 580ms/step - accuracy: 0.4089 - loss: 1.5007 - val_accuracy: 0.4599 - val_loss: 1.4317 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 532ms/step - accuracy: 0.4280 - loss: 1.4685 - val_accuracy: 0.4767 - val_loss: 1.3466 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 485ms/step - accuracy: 0.4350 - loss: 1.4415 - val_accuracy: 0.4980 - val_loss: 1.3183 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 503ms/step - accuracy: 0.4485 - loss: 1.4308 - val_accuracy: 0.3493 - val_loss: 1.8879 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 479ms/step - accuracy: 0.4557 - loss: 1.4092 - val_accuracy: 0.5116 - val_loss: 1.2719 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 456ms/step - accuracy: 0.4641 - loss: 1.3929 - val_accuracy: 0.5210 - val_loss: 1.2654 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 441ms/step - accuracy: 0.4720 - loss: 1.3811 - val_accuracy: 0.5026 - val_loss: 1.2699 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 443ms/step - accuracy: 0.4845 - loss: 1.3613 - val_accuracy: 0.5040 - val_loss: 1.3021 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419ms/step - accuracy: 0.4773 - loss: 1.3661\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 436ms/step - accuracy: 0.4773 - loss: 1.3661 - val_accuracy: 0.4887 - val_loss: 1.3972 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 439ms/step - accuracy: 0.4977 - loss: 1.3337 - val_accuracy: 0.5598 - val_loss: 1.1742 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 453ms/step - accuracy: 0.5079 - loss: 1.2923 - val_accuracy: 0.5610 - val_loss: 1.1852 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 437ms/step - accuracy: 0.5110 - loss: 1.2908 - val_accuracy: 0.5726 - val_loss: 1.1468 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 462ms/step - accuracy: 0.5114 - loss: 1.2818 - val_accuracy: 0.5475 - val_loss: 1.1847 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 474ms/step - accuracy: 0.5210 - loss: 1.2753 - val_accuracy: 0.5751 - val_loss: 1.1362 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 462ms/step - accuracy: 0.5216 - loss: 1.2585 - val_accuracy: 0.5587 - val_loss: 1.1663 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 473ms/step - accuracy: 0.5237 - loss: 1.2629 - val_accuracy: 0.5630 - val_loss: 1.1698 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 522ms/step - accuracy: 0.5313 - loss: 1.2433 - val_accuracy: 0.5809 - val_loss: 1.1331 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 546ms/step - accuracy: 0.5340 - loss: 1.2397 - val_accuracy: 0.5821 - val_loss: 1.1386 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 511ms/step - accuracy: 0.5300 - loss: 1.2426 - val_accuracy: 0.5130 - val_loss: 1.3509 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.5386 - loss: 1.2219\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 437ms/step - accuracy: 0.5386 - loss: 1.2219 - val_accuracy: 0.5680 - val_loss: 1.1406 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 518ms/step - accuracy: 0.5455 - loss: 1.2124 - val_accuracy: 0.5879 - val_loss: 1.1055 - learning_rate: 2.5000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 514ms/step - accuracy: 0.5472 - loss: 1.1979 - val_accuracy: 0.5674 - val_loss: 1.1691 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 456ms/step - accuracy: 0.5509 - loss: 1.1862 - val_accuracy: 0.5802 - val_loss: 1.1149 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.5543 - loss: 1.1832\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 468ms/step - accuracy: 0.5543 - loss: 1.1832 - val_accuracy: 0.5858 - val_loss: 1.1179 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 467ms/step - accuracy: 0.5578 - loss: 1.1832 - val_accuracy: 0.5950 - val_loss: 1.0862 - learning_rate: 1.2500e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.5907 - loss: 1.0994\n",
      "Validation Accuracy: 59.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"fer2013_combined.csv\")\n",
    "\n",
    "# Convert pixel strings to numpy arrays\n",
    "X = np.array([np.fromstring(pixels, dtype=np.uint8, sep=' ').reshape(48, 48, 1) for pixels in df[\"pixels\"]])\n",
    "y = to_categorical(df[\"emotion\"], num_classes=7)\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X / 255.0\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# improved CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=64),\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=30,\n",
    "          callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "# Save model\n",
    "model.save(\"improved_emotion_model.h5\")\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccd9f3b-2534-4987-b786-3bd808025d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "Predicted Emotion: Fear\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"emotion_detection_model.h5\")\n",
    "\n",
    "# Recompile the model (optional but recommended to avoid the warning)\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define emotion labels (to map the predictions to human-readable emotions)\n",
    "EMOTION_LABELS = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprise\"\n",
    "}\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image in grayscale\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize the image to 48x48 pixels\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "    \n",
    "    # Normalize pixel values (scale between 0 and 1)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Reshape to add batch dimension (1 image, 48x48 pixels, 1 color channel)\n",
    "    img = np.reshape(img, (1, 48, 48, 1))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Path to the test image\n",
    "image_path = r\"C:\\Users\\HP\\images.jpg\"  # Corrected image path\n",
    "\n",
    "# Preprocess the image\n",
    "processed_image = preprocess_image(image_path)\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(processed_image)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Get the emotion label corresponding to the predicted class\n",
    "predicted_emotion = EMOTION_LABELS[predicted_class]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
