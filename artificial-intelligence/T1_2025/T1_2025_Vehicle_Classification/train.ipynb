{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfVk1XFS4F7y"
      },
      "outputs": [],
      "source": [
        "# Full Vehicle Classification Training Pipeline using MobileNetV2\n",
        "# Includes augmentation, class weighting, fine-tuning, and saving as .keras\n",
        "\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgfK0dxqemaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSq3cfn69OIl",
        "outputId": "89eecf0a-5dcd-460a-89da-2663b1144fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwmy8k8CrLP5"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/vehicle_classification/datasets/'\n",
        "save_path = '/content/drive/MyDrive/vehicle_classification/processed_images/'\n",
        "split_path = '/content/drive/MyDrive/vehicle_classification/vehicle_classification_datasets/'\n",
        "base_dir = '/content/drive/MyDrive/vehicle_classification'\n",
        "processed_path = os.path.join(base_dir, 'processed_images')\n",
        "split_path = os.path.join(base_dir, 'vehicle_classification_datasets')\n",
        "log_dir = os.path.join(base_dir, 'logs_mobilenet')\n",
        "model_path = os.path.join(base_dir, 'vehicle_classifier_mobilenet2.keras')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac3ES-KXrJwa",
        "outputId": "b58f9381-d644-45c9-ccdb-48b879834fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3130 images belonging to 13 classes.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "#Load data with resizing and rescaling\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "data = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ4lEu9-WFvD",
        "outputId": "55ce8a43-c370-42b5-9e54-334a3e498f9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 3130 resized and normalized images to: /content/drive/MyDrive/vehicle_classification/processed_images/\n"
          ]
        }
      ],
      "source": [
        "img_count = 0\n",
        "for images, labels in data:\n",
        "    for i in range(images.shape[0]):\n",
        "        img = images[i] * 255.0\n",
        "        img = img.astype(np.uint8)\n",
        "        pil_img = array_to_img(img)\n",
        "\n",
        "        class_index = np.argmax(labels[i])\n",
        "        class_name = list(data.class_indices.keys())[class_index]\n",
        "\n",
        "        class_dir = os.path.join(save_path, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        img_filename = f\"{class_name}_{img_count:04d}.png\"\n",
        "        img_path = os.path.join(class_dir, img_filename)\n",
        "        pil_img.save(img_path)\n",
        "        img_count += 1\n",
        "\n",
        "    if img_count >= data.samples:\n",
        "        break\n",
        "\n",
        "print(f\"Saved {img_count} resized and normalized images to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kDpeufg6XZMN",
        "outputId": "187c9be3-6d2f-49ad-cba0-e898db3c0104"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'processed_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-545ebf35a6e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Create split folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'processed_path' is not defined"
          ]
        }
      ],
      "source": [
        "#  Create split folders\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for cls in os.listdir(processed_path):\n",
        "        os.makedirs(os.path.join(split_path, split, cls), exist_ok=True)\n",
        "\n",
        "#  Split the processed dataset\n",
        "for cls in os.listdir(processed_path):\n",
        "    class_dir = os.path.join(processed_path, cls)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    train_val, test = train_test_split(images, test_size=0.15, random_state=42)\n",
        "    train, val = train_test_split(train_val, test_size=0.176, random_state=42)  # 0.176 * 0.85 ≈ 15%\n",
        "\n",
        "    for split_name, split_files in zip(['train', 'val', 'test'], [train, val, test]):\n",
        "        for img_file in split_files:\n",
        "            src = os.path.join(class_dir, img_file)\n",
        "            dst = os.path.join(split_path, split_name, cls, img_file)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "print(\" Dataset split into train, val, and test folders\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOSYvA0ouvye"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz-4DlZTu4n6"
      },
      "outputs": [],
      "source": [
        "for folder in ['train', 'val', 'test']:\n",
        "    path = os.path.join(split_path, folder, 'class_13')\n",
        "    shutil.rmtree(path, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgf2O-qx9mBK",
        "outputId": "fca58a33-e00a-497f-daa0-9edbf125476e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4137 images belonging to 12 classes.\n",
            "Found 891 images belonging to 12 classes.\n",
            "Found 891 images belonging to 12 classes.\n"
          ]
        }
      ],
      "source": [
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.5, 1.5]\n",
        ")\n",
        "val_test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_gen.flow_from_directory(\n",
        "    os.path.join(split_path, 'train'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "val_data = val_test_gen.flow_from_directory(\n",
        "    os.path.join(split_path, 'val'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_data = val_test_gen.flow_from_directory(\n",
        "    os.path.join(split_path, 'test'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOhq74pW-zuV"
      },
      "outputs": [],
      "source": [
        "#  Compute class weights\n",
        "y_train = train_data.classes\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R4BfcCh-2ka",
        "outputId": "a8b70860-c22f-4304-c583-00c45fb5c533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#  Build model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(train_data.num_classes, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHgAhqLI-6yl"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    TensorBoard(log_dir=log_dir),\n",
        "    ModelCheckpoint(model_path, save_best_only=True, monitor='val_loss', mode='min'),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFKBWCXc_Brf",
        "outputId": "3ec4d965-633f-474e-a735-51286d48ff57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1774s\u001b[0m 13s/step - accuracy: 0.0862 - loss: 2.7220 - val_accuracy: 0.1515 - val_loss: 2.3955\n",
            "Epoch 2/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 4s/step - accuracy: 0.1587 - loss: 2.4252 - val_accuracy: 0.2974 - val_loss: 2.1525\n",
            "Epoch 3/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 4s/step - accuracy: 0.2771 - loss: 2.2100 - val_accuracy: 0.3793 - val_loss: 1.9301\n",
            "Epoch 4/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 4s/step - accuracy: 0.3441 - loss: 2.0550 - val_accuracy: 0.4602 - val_loss: 1.7155\n",
            "Epoch 5/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 4s/step - accuracy: 0.4211 - loss: 1.8659 - val_accuracy: 0.5309 - val_loss: 1.5304\n",
            "Epoch 6/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 4s/step - accuracy: 0.4944 - loss: 1.6903 - val_accuracy: 0.5915 - val_loss: 1.3784\n",
            "Epoch 7/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.5330 - loss: 1.6031 - val_accuracy: 0.6274 - val_loss: 1.2572\n",
            "Epoch 8/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 4s/step - accuracy: 0.5722 - loss: 1.4291 - val_accuracy: 0.6655 - val_loss: 1.1379\n",
            "Epoch 9/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 4s/step - accuracy: 0.5903 - loss: 1.3943 - val_accuracy: 0.6958 - val_loss: 1.0413\n",
            "Epoch 10/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 4s/step - accuracy: 0.6171 - loss: 1.2617 - val_accuracy: 0.7082 - val_loss: 0.9729\n",
            "Epoch 11/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 4s/step - accuracy: 0.6326 - loss: 1.2613 - val_accuracy: 0.7441 - val_loss: 0.8887\n",
            "Epoch 12/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 4s/step - accuracy: 0.6656 - loss: 1.0979 - val_accuracy: 0.7441 - val_loss: 0.8387\n",
            "Epoch 13/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.6894 - loss: 1.0460 - val_accuracy: 0.7576 - val_loss: 0.7913\n",
            "Epoch 14/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 4s/step - accuracy: 0.6898 - loss: 1.0200 - val_accuracy: 0.7800 - val_loss: 0.7433\n",
            "Epoch 15/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.6901 - loss: 1.0087 - val_accuracy: 0.7879 - val_loss: 0.7177\n",
            "Epoch 16/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.7192 - loss: 0.9110 - val_accuracy: 0.7991 - val_loss: 0.6874\n",
            "Epoch 17/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 4s/step - accuracy: 0.7144 - loss: 0.9280 - val_accuracy: 0.8013 - val_loss: 0.6623\n",
            "Epoch 18/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 4s/step - accuracy: 0.7413 - loss: 0.8265 - val_accuracy: 0.8103 - val_loss: 0.6374\n",
            "Epoch 19/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 4s/step - accuracy: 0.7501 - loss: 0.8176 - val_accuracy: 0.8126 - val_loss: 0.6214\n",
            "Epoch 20/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 4s/step - accuracy: 0.7753 - loss: 0.7803 - val_accuracy: 0.8227 - val_loss: 0.5915\n",
            "Epoch 21/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 4s/step - accuracy: 0.7704 - loss: 0.7538 - val_accuracy: 0.8249 - val_loss: 0.5674\n",
            "Epoch 22/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 4s/step - accuracy: 0.7731 - loss: 0.7339 - val_accuracy: 0.8316 - val_loss: 0.5625\n",
            "Epoch 23/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 4s/step - accuracy: 0.7853 - loss: 0.7041 - val_accuracy: 0.8429 - val_loss: 0.5502\n",
            "Epoch 24/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 4s/step - accuracy: 0.7945 - loss: 0.6595 - val_accuracy: 0.8305 - val_loss: 0.5464\n",
            "Epoch 25/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 4s/step - accuracy: 0.8057 - loss: 0.6540 - val_accuracy: 0.8395 - val_loss: 0.5329\n",
            "Epoch 26/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.8121 - loss: 0.6426 - val_accuracy: 0.8373 - val_loss: 0.5180\n",
            "Epoch 27/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 4s/step - accuracy: 0.8147 - loss: 0.6083 - val_accuracy: 0.8418 - val_loss: 0.5003\n",
            "Epoch 28/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 4s/step - accuracy: 0.8161 - loss: 0.6021 - val_accuracy: 0.8429 - val_loss: 0.4884\n",
            "Epoch 29/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 4s/step - accuracy: 0.8219 - loss: 0.5805 - val_accuracy: 0.8429 - val_loss: 0.4698\n",
            "Epoch 30/75\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.8375 - loss: 0.5361 - val_accuracy: 0.8485 - val_loss: 0.4607\n",
            "Epoch 31/75\n",
            "\u001b[1m115/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.8335 - loss: 0.5540"
          ]
        }
      ],
      "source": [
        "#  Train model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=75,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ywxPAY9EvY"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = load_model('/content/drive/MyDrive/vehicle_classification/vehicle_classifier_mobilenet.keras')\n",
        "\n",
        "test_data = val_test_gen.flow_from_directory(\n",
        "    os.path.join(split_path, 'test'),\n",
        "    target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False\n",
        ")\n",
        "loss, acc = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "#  Predict single image with unknown fallback\n",
        "class_labels = sorted(os.listdir(os.path.join(split_path, 'train')))\n",
        "\n",
        "def predict_vehicle_image(img_path, model, class_labels, threshold=0.6):\n",
        "    img = load_img(img_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    prediction = model.predict(img_array, verbose=0)\n",
        "    confidence = np.max(prediction)\n",
        "    class_idx = np.argmax(prediction)\n",
        "\n",
        "    if confidence < threshold:\n",
        "        label = \"Unknown Vehicle\"\n",
        "    else:\n",
        "        label = f\"{class_labels[class_idx]} ({confidence*100:.1f}%)\"\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Prediction: {label}\")\n",
        "    plt.show()\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY1DT0JKo7_c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}