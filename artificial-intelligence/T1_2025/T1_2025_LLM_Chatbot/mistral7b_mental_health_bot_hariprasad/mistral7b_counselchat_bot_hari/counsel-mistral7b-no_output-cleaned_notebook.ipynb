{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HDYaYgnUtRFj"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate datasets peft trl\n",
    "!pip install -U huggingface_hub\n",
    "!pip install -q evaluate bert-score\n",
    "!pip install rouge_score\n",
    "!pip install -q streamlit pyngrok\n",
    "!pip install -q flask flask-ngrok flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qw4Oj_qwp-2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0x0ttG1wszF"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggbDC6T8gCC6"
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(\"/content/drive/MyDrive/secrets/.env\")\n",
    "env_content = \"HF_TOKEN=ACTUAL_NGROK_TOKEN\"\n",
    "env_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "env_path.write_text(env_content)\n",
    "\n",
    "print(f\".env file created at {env_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iZWzKrAguBn"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Epq98bfm0IMk"
   },
   "source": [
    "# Dataset Counsel Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8HwilSzwyQt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#Load dataset\n",
    "file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/20200325_counsel_chat.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Preview dataset\n",
    "print(\"🧾 Columns:\", df.columns)\n",
    "print(\"\\n📌 Sample rows:\")\n",
    "display(df.head(3))\n",
    "\n",
    "#Check for missing values\n",
    "print(\"\\n🧼 Missing values per column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPyEtvqo0Tf1"
   },
   "source": [
    "# Step 2: Extract Best Q&A Pairs to JSONL (for LoRA Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoYkbj-t0UuE"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Group by questionID and select top upvoted answer\n",
    "top_answers_df = (\n",
    "    df.sort_values(\"upvotes\", ascending=False)\n",
    "      .groupby(\"questionID\", as_index=False)\n",
    "      .first()\n",
    ")\n",
    "\n",
    "#Format into JSONL format for instruction-output fine-tuning\n",
    "jsonl_data = []\n",
    "for _, row in top_answers_df.iterrows():\n",
    "    instruction = row[\"questionText\"].strip()\n",
    "    output = row[\"answerText\"].strip()\n",
    "\n",
    "    #Optional: Skip if too short\n",
    "    if len(instruction.split()) < 4 or len(output.split()) < 4:\n",
    "        continue\n",
    "\n",
    "    jsonl_data.append({\"instruction\": instruction, \"output\": output})\n",
    "\n",
    "#Save to JSONL file\n",
    "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Saved {len(jsonl_data)} cleaned Q&A pairs to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAhstEVv0ZIL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gKWui2D1Nvr"
   },
   "source": [
    "# Testing with Mistral + Counsel Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ru6fE__O1Sdr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "#Load base model (or your fine-tuned model path)\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "#Load your CounselChat cleaned JSONL\n",
    "jsonl_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
    "samples = []\n",
    "\n",
    "with open(jsonl_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        samples.append(data)\n",
    "\n",
    "#Test first 3 prompts\n",
    "for i in range(3):\n",
    "    user_question = samples[i]['instruction']\n",
    "    expected_answer = samples[i]['output']\n",
    "\n",
    "    #Prompt style to guide model\n",
    "    prompt = f\"\"\"<s>[INST] You are a supportive mental health assistant. Provide thoughtful guidance.\n",
    "\n",
    "User: {user_question}\n",
    "\n",
    "Respond with empathy and practical steps. [/INST]\"\"\"\n",
    "\n",
    "    #Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=200)\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n📌 Prompt: {user_question}\\n🔹 Expected: {expected_answer}\\n🤖 Model: {response}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYy9lDcc11K6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lQIuZhc30Qf"
   },
   "source": [
    "# Tuning -Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwTh9NvJ32ul"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\", split=\"train\")\n",
    "\n",
    "#Load tokenizer + Fix padding\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix padding token issue\n",
    "\n",
    "#Tokenization function\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    result = tokenizer(prompt, text_target=example[\"output\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    return result\n",
    "\n",
    "#Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#Load base model + apply LoRA\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "#Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "#Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "#Save model\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\")\n",
    "\n",
    "print(\"✅ Trial 1 fine-tuning complete and saved to Drive!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLFsGyBI8nXq"
   },
   "source": [
    "# Trial 1 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSqCKfcb6hLV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "#Load the evaluation dataset (1000 samples)\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
    "    split=\"train[:300]\"\n",
    ")\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "\n",
    "#Load evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Inference loop\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 1\"):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "#Compute metrics\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "#Print results\n",
    "print(\"\\n📊 Trial 1 Evaluation Results (1000 samples):\")\n",
    "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
    "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
    "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
    "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
    "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRvXJdDy83n8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2N-Fe8D6D6Ma"
   },
   "source": [
    "# Tuning trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSNbU5tkD8UM"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "#Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load dataset (~890 samples)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#Load model + tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#Tokenization function\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
    "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "#Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Training args (tested in Trial 9)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#Train\n",
    "trainer.train()\n",
    "\n",
    "#Save model + tokenizer\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\")\n",
    "\n",
    "print(\"✅ Trial 2 fine-tuning complete and saved to Drive!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi7rvAjMF9mv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe-bAMDYJ0JN"
   },
   "source": [
    "# Trial 2 - evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eraZ3CQpJ0sF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load evaluation dataset (300 or 1000 samples)\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
    "    split=\"train[:300]\"\n",
    ")\n",
    "\n",
    "#Load the PEFT config + base model\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\"  # 👈 Change here\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "#Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "#Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "#Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Inference loop\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 2\"):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "#Evaluation\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "#Print results\n",
    "print(\"\\n📊 Trial 2 Evaluation Results (300 samples):\")\n",
    "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
    "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
    "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
    "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
    "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQFK5gwjKHOQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHVquUAuWul8"
   },
   "source": [
    "# creating synthetic GPT samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NJD2f1XXYbu"
   },
   "source": [
    "🔁 Synthetic Sample Generation – CounselChat Augmentation (Trial 3)\n",
    "To enhance the performance of our Mistral 7B fine-tuning for the mental health chatbot, we generate 200 high-quality synthetic Q&A samples. These samples are designed to:\n",
    "\n",
    "Match the tone, format, and structure of the original CounselChat dataset\n",
    "\n",
    "Improve generalization and bigram-level coherence (ROUGE-2)\n",
    "\n",
    "Stay lightweight to avoid exhausting Colab Pro A100 resources\n",
    "\n",
    "Each synthetic entry consists of:\n",
    "\n",
    "A realistic user query about anxiety, depression, or emotional distress (instruction)\n",
    "\n",
    "A thoughtful, structured, empathetic multi-step response (output) based on known therapeutic practices (e.g., CBT, DBT)\n",
    "\n",
    "These 200 samples will be merged with ~890 real samples to create a new training set (counselchat_augmented_1090.jsonl) for Trial 3 fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dd96pEXtWz2y"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "#Expanded set of ~60 varied emotional/mental health prompts\n",
    "user_issues = [\n",
    "    \"I feel anxious about everyday situations, even simple ones like sending emails.\",\n",
    "    \"I can't stop overanalyzing conversations after they happen.\",\n",
    "    \"I'm always expecting the worst-case scenario, even when things seem fine.\",\n",
    "    \"My heart races whenever I have to speak in front of others.\",\n",
    "    \"I feel guilty when I try to take time for myself.\",\n",
    "    \"I’m scared to share how I feel with my family because they might judge me.\",\n",
    "    \"Sometimes I just feel numb, like I’m not really here.\",\n",
    "    \"I constantly compare myself to others and feel like I’m falling behind.\",\n",
    "    \"I feel overwhelmed by my responsibilities and don't know where to start.\",\n",
    "    \"I'm always trying to please others and end up neglecting myself.\",\n",
    "    \"Even when I achieve something, I feel like it's not good enough.\",\n",
    "    \"I can’t stop thinking about things I did wrong years ago.\",\n",
    "    \"I’m scared I’ll never feel truly happy again.\",\n",
    "    \"Sometimes I cry for no reason and I don't know how to explain it.\",\n",
    "    \"I get upset with myself when I’m not productive enough.\",\n",
    "    \"I feel like a burden to the people around me.\",\n",
    "    \"I hate asking for help because I don't want to seem weak.\",\n",
    "    \"I isolate myself even though I don’t want to be alone.\",\n",
    "    \"It feels like my mind is always racing and I can't shut it off.\",\n",
    "    \"I’ve been eating less lately because I feel stressed and anxious.\",\n",
    "    \"I feel like my life lacks purpose or direction.\",\n",
    "    \"I try to sleep but my brain keeps bringing up bad memories.\",\n",
    "    \"I always feel like I’m being judged, even when no one is around.\",\n",
    "    \"Sometimes I fake being okay because I don’t want others to worry.\",\n",
    "    \"I struggle to get out of bed in the mornings lately.\",\n",
    "    \"I don’t know how to express my emotions without feeling ashamed.\",\n",
    "    \"I keep doubting my self-worth, even when others praise me.\",\n",
    "    \"I feel stuck in a loop of negative thinking.\",\n",
    "    \"It’s hard for me to enjoy things I used to love.\",\n",
    "    \"I feel afraid of failing, so I avoid trying new things.\",\n",
    "    \"I’ve lost interest in socializing with others.\",\n",
    "    \"I get nervous even before small meetings or group calls.\",\n",
    "    \"I overthink everything I say and do.\",\n",
    "    \"I feel like I’m pretending to be okay all the time.\",\n",
    "    \"It feels like no one really understands me.\",\n",
    "    \"I panic when plans change unexpectedly.\",\n",
    "    \"I feel like I'm falling apart on the inside.\",\n",
    "    \"I worry a lot about things I can’t control.\",\n",
    "    \"I feel disconnected from my own life.\",\n",
    "    \"I’m trying to heal, but progress feels painfully slow.\",\n",
    "    \"I question if therapy is even helping me.\",\n",
    "    \"I want to trust people but I'm afraid of being hurt.\",\n",
    "    \"I avoid eye contact because I feel ashamed.\",\n",
    "    \"I feel anxious when someone compliments me.\",\n",
    "    \"I'm afraid people are pretending to like me.\",\n",
    "    \"I get angry at myself for feeling this way.\",\n",
    "    \"I often feel like an imposter, even with my accomplishments.\",\n",
    "    \"I’m overwhelmed and don’t know where to begin.\",\n",
    "    \"I worry that people secretly dislike me.\",\n",
    "    \"I sometimes wish I could just disappear for a while.\",\n",
    "    \"I get drained by social events, even short ones.\",\n",
    "    \"I feel like I can't be myself around others.\",\n",
    "    \"I don’t feel motivated to do anything lately.\",\n",
    "    \"I try to look strong, but inside I’m falling apart.\",\n",
    "    \"I’m scared of being vulnerable with people.\",\n",
    "    \"I feel emotionally exhausted by my own thoughts.\",\n",
    "    \"I want to feel better, but I don’t know how.\",\n",
    "    \"I get scared of being alone but also fear getting close to people.\"\n",
    "]\n",
    "\n",
    "# Same response components from earlier\n",
    "responses_intro = [\n",
    "    \"Thank you for being open and sharing this. You're not alone in feeling this way.\",\n",
    "    \"I hear you, and I want you to know that your feelings are valid.\",\n",
    "    \"What you're experiencing is difficult, and it's good that you're reaching out.\",\n",
    "    \"It takes courage to speak up about this — you're already taking a positive step.\",\n",
    "    \"I'm really glad you asked this. Many people go through similar emotions.\"\n",
    "]\n",
    "\n",
    "responses_body = [\n",
    "    \"Anxiety can often lead to overthinking and self-doubt. It helps to practice grounding techniques like focused breathing or mindful journaling.\",\n",
    "    \"Cognitive Behavioral Therapy (CBT) has proven effective in managing negative thought loops. You might consider exploring CBT worksheets or speaking with a therapist trained in it.\",\n",
    "    \"Try to notice when these thoughts come up and gently challenge them by asking yourself if they're based on facts or assumptions.\",\n",
    "    \"Consider keeping a self-compassion journal where you write down moments you were kind to yourself or others.\",\n",
    "    \"Sometimes, talking to a counselor or even joining a peer support group can provide relief and perspective.\"\n",
    "]\n",
    "\n",
    "responses_close = [\n",
    "    \"Remember, healing is not linear. Small steps matter.\",\n",
    "    \"Be patient with yourself. You deserve support and care.\",\n",
    "    \"You are worthy of kindness — from others and from yourself.\",\n",
    "    \"Don’t hesitate to seek professional help when needed. You’re not alone.\",\n",
    "    \"Keep going. You're doing better than you think.\"\n",
    "]\n",
    "\n",
    "# Generate and save 200 synthetic Q&A pairs\n",
    "synthetic_samples = []\n",
    "for _ in range(200):\n",
    "    instruction = random.choice(user_issues)\n",
    "    output = \"\\n\\n\".join([\n",
    "        random.choice(responses_intro),\n",
    "        random.choice(responses_body),\n",
    "        random.choice(responses_close)\n",
    "    ])\n",
    "    synthetic_samples.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"output\": output\n",
    "    })\n",
    "\n",
    "# Save as JSONL\n",
    "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_synthetic_200.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for item in synthetic_samples:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✅ 200 synthetic samples saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrHblaKRXq3I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlqN7WsuX7Y8"
   },
   "source": [
    "# Creating new augmented dataset - synthetic200 + counsel chat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIx7D2XzYEZX"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "#File paths\n",
    "real_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
    "synthetic_path = \"/content/dtasets/counselchat_augmented_1090.jsonl\"rive/MyDrive/chatbot/counsel/datasets/counselchat_synthetic_200.jsonl\"\n",
    "output_path = \"/content/drive/MyDrive/chatbot/counsel/da\n",
    "\n",
    "#Load real CounselChat data\n",
    "with open(real_path, \"r\") as f:\n",
    "    real_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "#Load synthetic samples\n",
    "with open(synthetic_path, \"r\") as f:\n",
    "    synthetic_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "#Combine and shuffle\n",
    "combined_data = real_data + synthetic_data\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "#Save merged dataset as JSONL\n",
    "with open(output_path, \"w\") as f:\n",
    "    for item in combined_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Merged dataset saved to: {output_path}\")\n",
    "print(f\"📊 Total samples: {len(combined_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShAuC8aqYHWp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LjBtIYTYf4T"
   },
   "source": [
    "# Trial 3 with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjxwg2OyYkYH"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "#Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load merged dataset (Counsel + 200 synthetic)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_augmented_1090.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#Tokenize\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
    "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "#Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "trainer.train()\n",
    "\n",
    "#Save model and tokenizer\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\")\n",
    "\n",
    "print(\"✅ Trial 3 fine-tuning complete and saved to Drive!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbqlO9z1Ymjc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggm_iDdeaO7c"
   },
   "source": [
    "# Trial 3 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMPysrNooULS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrOxFh6qaUv5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load evaluation dataset\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
    "    split=\"train[:300]\"  # Adjust to 1000 if needed\n",
    ")\n",
    "\n",
    "#Load the PEFT model and config\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "#Load base model and LoRA adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "#Load evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Run generation\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 3\"):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "#Compute metrics\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "#Print results\n",
    "print(\"\\n📊 Trial 3 Evaluation Results (300 samples):\")\n",
    "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
    "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
    "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
    "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
    "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdpW_y12ad0n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUDCnfTzGrNv"
   },
   "source": [
    "# Dataset Cleaning Updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA06ISrMVaTX"
   },
   "source": [
    "## 🧹 Smart Filtering of CounselChat Dataset (Explained)\n",
    "\n",
    "This dataset cleaning script was applied to the original CSV file `20200325_counsel_chat.csv` from CounselChat.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Step-by-Step Actions\n",
    "\n",
    "#### 1. **Loaded the original raw CSV**\n",
    "- File: `20200325_counsel_chat.csv`\n",
    "- Contains therapist responses to mental health questions.\n",
    "- Columns included: `questionText`, `answerText`, `topic`, `therapistInfo`, etc.\n",
    "\n",
    "#### 2. **Extracted Key Fields**\n",
    "Only two fields were kept to train the model:\n",
    "\n",
    "| CSV Field       | Transformed Field |\n",
    "|----------------|-------------------|\n",
    "| `questionText` | `instruction`     |\n",
    "| `answerText`   | `output`          |\n",
    "\n",
    "Example format:\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"I’ve been feeling anxious a lot lately. How can I manage it?\",\n",
    "  \"output\": \"Anxiety is common. You can begin with deep breathing and journaling...\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Applied Smart Filters**\n",
    "The goal was to remove low-quality or misleading samples.\n",
    "\n",
    "| Filter Rule                                      | Reason                                              |\n",
    "|--------------------------------------------------|-----------------------------------------------------|\n",
    "| `instruction` and `output` must not be empty     | Skips blank entries                                 |\n",
    "| `instruction` must have ≥ 5 words                | Too-short questions aren't useful                   |\n",
    "| `output` must have ≥ 15 words                    | Short answers don’t help the model learn enough     |\n",
    "| No phrases like “I am an AI”, “I don’t understand” | Removes hallucinated or chatbot-like answers        |\n",
    "| `instruction` must not appear inside the `output`| Avoid parroting                                     |\n",
    "\n",
    "This ensures cleaner and more meaningful learning data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Saved Cleaned Data**\n",
    "- Saved as JSONL to:\n",
    "  `/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl`\n",
    "- Each line contains one valid instruction-output pair\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why This Matters\n",
    "- Improves dataset quality for fine-tuning\n",
    "- Reduces junk samples and repetitive patterns\n",
    "- Leads to **better BLEU, ROUGE, and BERTScore results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mMhQy0hGsKf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "csv_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/20200325_counsel_chat.csv\"  # Update path if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"Total rows:\", len(df))\n",
    "\n",
    "def format_row(row):\n",
    "    instruction = row.get(\"questionText\", \"\").strip()\n",
    "    output = row.get(\"answerText\", \"\").strip()\n",
    "    return {\"instruction\": instruction, \"output\": output}\n",
    "\n",
    "def is_valid(example):\n",
    "    instr = example[\"instruction\"]\n",
    "    out = example[\"output\"]\n",
    "\n",
    "    # Basic checks\n",
    "    if not instr or not out:\n",
    "        return False\n",
    "    if len(instr.split()) < 5 or len(out.split()) < 15:\n",
    "        return False\n",
    "    if \"I am an AI\" in out or \"I don't understand\" in out:\n",
    "        return False\n",
    "    if instr.lower() in out.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered = []\n",
    "for _, row in df.iterrows():\n",
    "    formatted = format_row(row)\n",
    "    if is_valid(formatted):\n",
    "        filtered.append(formatted)\n",
    "\n",
    "print(f\"Total valid instruction-output pairs: {len(filtered)}\")\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for item in filtered:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"Cleaned JSONL saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTDsBrHZI73V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8oY57KlKQLl"
   },
   "source": [
    "# Tuning - Trial 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "k04Wps7YKUT2"
   },
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "#Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load filtered dataset (2,116 samples)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#Tokenization function\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
    "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#Train!\n",
    "trainer.train()\n",
    "\n",
    "#Save model and tokenizer\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\")\n",
    "\n",
    "print(\"✅ Trial 4 complete! Model saved to Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65PXSARyKbyT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzn6bNOaO_2b"
   },
   "source": [
    "# Trial 4 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQbzLHc2PDRw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Enable TF32 for faster matrix operations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "#Load smaller eval set (500 samples for quick test)\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
    "    split=\"train[:300]\"\n",
    ")\n",
    "\n",
    "#Load PEFT config and model\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "#Load evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Inference loop with timing\n",
    "predictions = []\n",
    "references = []\n",
    "token_lengths = []\n",
    "start_time = time.time()\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 4 (500 samples, fast mode)\"):\n",
    "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,  # ✅ Reduced from 200\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "    token_lengths.append(len(tokenizer.tokenize(pred)))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "#Evaluation metrics\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "#Performance logs\n",
    "avg_gen_time = round((end_time - start_time) / len(eval_dataset), 2)\n",
    "avg_token_len = round(sum(token_lengths) / len(token_lengths), 2)\n",
    "\n",
    "print(\"\\n📊 Trial 4 Evaluation Results (500 samples, Fast Mode):\")\n",
    "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
    "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
    "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
    "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
    "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n",
    "\n",
    "print(f\"\\n⏱️ Avg generation time per sample: {avg_gen_time} seconds\")\n",
    "print(f\"🧠 Avg token length of output: {avg_token_len} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUkommnVdr0e"
   },
   "source": [
    "## 📊 Mistral-7B + CounselChat Hyperparameter Tuning Tracker\n",
    "\n",
    "### ✅ Trial Summary Table\n",
    "\n",
    "| Trial | Dataset Size                  | LoRA `r` | Alpha | Dropout | Epochs | LR     | Prompt Style                                    | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore | Notes            |\n",
    "|-------|-------------------------------|----------|--------|----------|--------|--------|------------------------------------------------|--------|----------|----------|----------|------------|------------------|\n",
    "| 1     | 890 real                      | 8        | 16     | 0.05     | 3      | 2e-5   | Friendly therapist tone                         | 0.0203 | 0.2888   | 0.0429   | 0.1349   | 0.8311     | Baseline         |\n",
    "| 2     | 890 real                      | 16       | 32     | 0.05     | 4      | 2e-5   | Same prompt as Trial 1                          | 0.0173 | 0.2315   | 0.0334   | 0.1220   | 0.8076     | More stable LoRA |\n",
    "| 3     | 1,015 (real + 200 synthetic)  | 16       | 32     | 0.05     | 4      | 2e-5   | Same prompt as Trial 1                          | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058     | With GPT samples |\n",
    "| ✅ 4  | 2,116 smart-filtered (real)   | 16       | 32     | 0.05     | 5      | 2e-5   | Same prompt, full cleaned dataset               | **0.0222** | 0.2384   | **0.0387** | 0.1225   | **0.8144** | Best so far 🔥   |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Notes:\n",
    "- **BLEU** = n-gram overlap\n",
    "- **ROUGE-1/2/L** = word, bigram, sequence overlap\n",
    "- **BERTScore** = semantic similarity using RoBERTa\n",
    "- All trials used the same system prompt:  \n",
    "  `\"You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\"`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TF8t_OuPEVP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Ab1XpVfGAW"
   },
   "source": [
    "# Trial 4 Model (Chatbot Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELSW7UYEfnrn"
   },
   "source": [
    "Bulk message testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZPCRYgsfml4"
   },
   "outputs": [],
   "source": [
    "#Test prompts for Mistral-7B + CounselChat model\n",
    "test_prompts = [\n",
    "    \"I feel like I'm constantly overthinking every little thing. How do I make it stop?\",\n",
    "    \"Why do I get so anxious when I have to talk to people, even my friends?\",\n",
    "    \"I wake up with a knot in my stomach every day. What can I do about it?\",\n",
    "    \"Lately I’ve lost interest in everything. Nothing makes me happy anymore.\",\n",
    "    \"Is it normal to feel tired all the time even when I’m not physically active?\",\n",
    "    \"I feel numb, like nothing really matters. What should I do?\",\n",
    "    \"I had a panic attack at work and now I’m afraid it’ll happen again. How do I cope?\",\n",
    "    \"My heart races for no reason and I feel like I can't breathe. Am I losing control?\",\n",
    "    \"How can I calm down quickly when I feel overwhelmed in public?\",\n",
    "    \"No matter what I do, I always feel like I’m not good enough.\",\n",
    "    \"I compare myself to everyone and feel like I’m constantly failing.\",\n",
    "    \"Why do I always blame myself when something goes wrong?\",\n",
    "    \"I always say yes to people even when I’m exhausted. How do I set boundaries?\",\n",
    "    \"My partner doesn’t understand my anxiety. It’s hurting our relationship. What should I do?\",\n",
    "    \"I feel guilty for putting myself first. Is that selfish?\"\n",
    "]\n",
    "\n",
    "#Save to TXT\n",
    "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(\"\\n\".join(test_prompts))\n",
    "\n",
    "#Save to JSONL\n",
    "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\", \"w\") as jsonl_file:\n",
    "    for prompt in test_prompts:\n",
    "        jsonl_file.write(f'{{\"instruction\": \"{prompt}\"}}\\n')\n",
    "\n",
    "#Save to CSV\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"user_prompt\": test_prompts})\n",
    "df.to_csv(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files saved to Colab:\")\n",
    "print(\"- test_prompts_counselchat.txt\")\n",
    "print(\"- test_prompts_counselchat.jsonl\")\n",
    "print(\"- test_prompts_counselchat.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzHVmOSIfIDw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "#Load the fine-tuned model (Trial 4)\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "#Load test prompts from JSONL\n",
    "test_file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\"\n",
    "prompts = []\n",
    "with open(test_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        prompts.append(entry[\"instruction\"])\n",
    "\n",
    "#Define the response function\n",
    "def generate_response(prompt, max_tokens=150):\n",
    "    full_prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{prompt} [/INST]\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "#Generate and print responses\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n🧠 Prompt {i}: {prompt}\")\n",
    "    print(\"🤖 Response:\\n\", generate_response(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FP3YliQfKoX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdHO1TkZl-WF"
   },
   "source": [
    "# Tuning - Trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNIXH9igmCfE"
   },
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "#Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load filtered dataset (2,116 samples)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#Trial 5 Prompt Style: Updated for variety and tone\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] As a helpful mental health assistant, how would you support someone who says:\\n\\\"{example['instruction']}\\\" [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#LoRA Config — same structure as Trial 4\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Training arguments (Trial 5 updates applied)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,  # lower learning rate\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#Train!\n",
    "trainer.train()\n",
    "\n",
    "#Save model and tokenizer\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\")\n",
    "\n",
    "print(\"✅ Trial 5 complete! Model saved to Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO-eRBo6mVSZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R7gaMy1tvG8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WriMhs7ItulA"
   },
   "source": [
    "# Trial 5 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJshrX73tyWk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#Load 300-sample evaluation set\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
    "    split=\"train[:300]\"\n",
    ")\n",
    "\n",
    "#Load fine-tuned Trial 5 model + PEFT config\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\"\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
    "\n",
    "#Load evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "#Inference loop\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 5 (300 samples)\"):\n",
    "    prompt = f\"<s>[INST] As a helpful mental health assistant, how would you support someone who says:\\n\\\"{example['instruction']}\\\" [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "#Compute metrics\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "end = time.time()\n",
    "avg_gen_time = round((end - start) / len(predictions), 2)\n",
    "\n",
    "#Print results\n",
    "print(\"\\n📊 Trial 5 Evaluation Results (300 samples):\")\n",
    "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
    "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
    "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
    "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
    "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n",
    "print(f\"⏱️ Avg generation time/sample: {avg_gen_time}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5heZWUt1x9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwICqZkK6boJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S0CD2aN6b9V"
   },
   "source": [
    "# Merging - CounselChat + Augmented EmpatheticDialogues For Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "964gDO0268rF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\", \"r\") as f:\n",
    "    counsel_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"/content/drive/MyDrive/SIT782/datasets/empdiag_augmented_4100.jsonl\", \"r\") as f:\n",
    "    empathetic_data = [json.loads(line) for line in f]\n",
    "\n",
    "assert \"instruction\" in counsel_data[0] and \"output\" in counsel_data[0], \"❌ CounselChat format issue\"\n",
    "assert \"instruction\" in empathetic_data[0] and \"output\" in empathetic_data[0], \"❌ Empathetic format issue\"\n",
    "\n",
    "merged_data = counsel_data + empathetic_data\n",
    "unique_jsons = list({json.dumps(entry, sort_keys=True) for entry in merged_data})\n",
    "merged_cleaned = [json.loads(j) for j in unique_jsons]\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for sample in merged_cleaned:\n",
    "        json.dump(sample, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#final report\n",
    "print(f\"Merged dataset saved to: {output_path}\")\n",
    "print(f\"Total samples after deduplication: {len(merged_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQiJ60t_8h6P"
   },
   "source": [
    "dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAGO2Vk97HC0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load merged dataset\n",
    "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\", \"r\") as f:\n",
    "    merged_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Basic stats\n",
    "total_samples = len(merged_data)\n",
    "instruction_lengths = [len(entry[\"instruction\"].split()) for entry in merged_data]\n",
    "output_lengths = [len(entry[\"output\"].split()) for entry in merged_data]\n",
    "\n",
    "# Token length distributions\n",
    "avg_instruction_len = sum(instruction_lengths) / total_samples\n",
    "avg_output_len = sum(output_lengths) / total_samples\n",
    "\n",
    "min_instruction_len = min(instruction_lengths)\n",
    "max_instruction_len = max(instruction_lengths)\n",
    "\n",
    "min_output_len = min(output_lengths)\n",
    "max_output_len = max(output_lengths)\n",
    "\n",
    "# Extremely short responses\n",
    "short_outputs = sum(1 for length in output_lengths if length < 20)\n",
    "\n",
    "# Summary\n",
    "dataset_stats = {\n",
    "    \"Total Samples\": total_samples,\n",
    "    \"Avg Instruction Length (words)\": round(avg_instruction_len, 2),\n",
    "    \"Avg Output Length (words)\": round(avg_output_len, 2),\n",
    "    \"Min Instruction Length\": min_instruction_len,\n",
    "    \"Max Instruction Length\": max_instruction_len,\n",
    "    \"Min Output Length\": min_output_len,\n",
    "    \"Max Output Length\": max_output_len,\n",
    "    \"Samples with Output < 20 words\": short_outputs\n",
    "}\n",
    "\n",
    "dataset_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky0ccuca8qrF"
   },
   "source": [
    "Why We Removed Outputs with Fewer Than 20 Words (Trial 6 Dataset)\n",
    "\n",
    "In our merged dataset (counselchat + empathetic_dialogues), many samples had very short responses, often under 20 words. While these samples may be realistic, they:\n",
    "\n",
    "    Lack informative content or emotional depth\n",
    "\n",
    "    Hurt phrase overlap metrics like ROUGE-2 and BLEU\n",
    "\n",
    "    Introduce noise or overly generic replies into training\n",
    "\n",
    "To improve model quality, especially for long-form conversational replies, we created a cleaned dataset where each response (output) has at least 20 words.\n",
    "\n",
    "This refined dataset (Trial 6) will help:\n",
    "\n",
    "    Boost evaluation scores (ROUGE, BLEU, BERTScore)\n",
    "\n",
    "    Encourage more complete, helpful responses\n",
    "\n",
    "    Train the model to match empathetic, multi-step advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XXQdtHh8nE1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "merged_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\"\n",
    "with open(merged_path, \"r\") as f:\n",
    "    merged_data = [json.loads(line) for line in f]\n",
    "\n",
    "#Filter: Keep only samples with output ≥ 20 words\n",
    "filtered_data = [sample for sample in merged_data if len(sample[\"output\"].split()) >= 20]\n",
    "\n",
    "trial6_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\"\n",
    "with open(trial6_path, \"w\") as f:\n",
    "    for sample in filtered_data:\n",
    "        json.dump(sample, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Trial 6 dataset saved!\")\n",
    "print(f\"Total samples (after filter): {len(filtered_data)}\")\n",
    "print(f\"Samples removed (< 20 words): {len(merged_data) - len(filtered_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn17EGd988rl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf40swfAtidV"
   },
   "source": [
    "# Trial 6 Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CKq8D-B9xr-"
   },
   "source": [
    "🔁 Trial 6 Fine-Tuning Script (merged_counsel_empathetic_cleaned_trial6.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKw6ZLc_9yjK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "#Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load dataset (Trial 6)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "#Load model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False  # Prevent rotary crash\n",
    "\n",
    "#Preprocessing function (batched-safe)\n",
    "def preprocess(batch):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for instruction, output in zip(batch[\"instruction\"], batch[\"output\"]):\n",
    "        prompt = f\"You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{instruction}\"\n",
    "\n",
    "        prompt_tokens = tokenizer(prompt, truncation=True, max_length=256, padding=\"max_length\")\n",
    "        output_tokens = tokenizer(output, truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "        input_ids = prompt_tokens[\"input_ids\"] + output_tokens[\"input_ids\"]\n",
    "        attention_mask = prompt_tokens[\"attention_mask\"] + output_tokens[\"attention_mask\"]\n",
    "        labels = [-100] * len(prompt_tokens[\"input_ids\"]) + output_tokens[\"input_ids\"]\n",
    "\n",
    "        # Ensure total length is 512 tokens\n",
    "        input_ids_list.append(input_ids[:512])\n",
    "        attention_mask_list.append(attention_mask[:512])\n",
    "        labels_list.append(labels[:512])\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "#Apply preprocessing\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "#LoRA Config (rotary-safe)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    report_to=[],\n",
    "    run_name=\"mistral_trial6\"\n",
    ")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#Train\n",
    "trainer.train()\n",
    "\n",
    "#Save\n",
    "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\")\n",
    "\n",
    "print(\"✅ Trial 6 complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPrGsJuH96WD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQCaJjEGHsoY"
   },
   "source": [
    "# Trial 6 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w8FxhgMHvzV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "#Load evaluation dataset (300 samples for speed)\n",
    "eval_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\",\n",
    "    split=\"train[:300]\"\n",
    ")\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in tqdm(eval_dataset, desc=\"🔍 Evaluating Trial 6 (300 samples)\"):\n",
    "    input_text = f\"<s>[INST] You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{example['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "{\n",
    "    \"BLEU\": round(bleu_result[\"bleu\"], 4),\n",
    "    \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
    "    \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
    "    \"ROUGE-L\": round(rouge_result[\"rougeL\"], 4),\n",
    "    \"BERTScore (F1)\": round(sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]), 4)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch0ORDR0WeiR"
   },
   "source": [
    "📊 Trial 6 Evaluation Comparison: Mistral + CounselChat Trials\n",
    "\n",
    "| Trial   | Dataset                        | Samples | LoRA Params (r/α) | Epochs | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore (F1) |\n",
    "|---------|--------------------------------|---------|-------------------|--------|--------|----------|----------|----------|----------------|\n",
    "| Trial 1 | counselchat_cleaned.jsonl    | ~890    | 8 / 16            | 3      | 0.0173 | 0.2315   | 0.0334   | 0.1220   | 0.8076         |\n",
    "| Trial 2 | Same as Trial 1                | ~890    | 16 / 32           | 4      | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058         |\n",
    "| Trial 3 | Real + 200 Synthetic           | ~1,015  | 16 / 32           | 4      | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058         |\n",
    "| Trial 4 | Smart-filtered CounselChat     | ~2,116  | 16 / 32           | 5      | 0.0222 | 0.2384   | 0.0387   | 0.1225   | 0.8144         |\n",
    "| Trial 5 | Same dataset, prompt tweaks    | ~2,116  | 16 / 32           | 5      | 0.0243 | 0.3071   | 0.0416   | 0.1307   | 0.8344         |\n",
    "| **Trial 6** | Merged (filtered counsel) + Empathetic (augmented) | 2,277 | 8 / 16            | 4      | **0.0280** | **0.2967** | **0.0540** | **0.1442** | **0.8328**       |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dW7SssOVAJV"
   },
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets evaluate bert_score tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load 500 evaluation samples from MentalChat16K\n",
    "eval_data = load_dataset(\"ShenLab/MentalChat16K\", split=\"train\")\n",
    "eval_subset = eval_data.shuffle(seed=42).select(range(500))\n",
    "\n",
    "#Load fine-tuned model from Trial 12\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "#Response generator\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=350).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=180,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#Generate predictions with live progress\n",
    "preds, refs = [], []\n",
    "\n",
    "print(\"🧠 Generating responses on 500 MentalChat16K samples...\")\n",
    "for sample in tqdm(eval_subset, desc=\"Evaluating\"):\n",
    "    user_msg = sample[\"instruction\"]\n",
    "    ref_response = sample[\"output\"]\n",
    "    prompt = f\"<s>[INST] You are a kind and supportive mental health assistant who responds empathetically to this user message:\\n{user_msg} [/INST]\"\n",
    "    gen_response = generate_response(prompt)\n",
    "    preds.append(gen_response)\n",
    "    refs.append(ref_response)\n",
    "\n",
    "#Run evaluation metrics\n",
    "print(\"\\n📊 Running BLEU, ROUGE, and BERTScore evaluations...\\n\")\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "print(\"🔵 BLEU:\", bleu.compute(predictions=preds, references=refs)[\"bleu\"] * 100)\n",
    "\n",
    "rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "print(\"🟥 ROUGE-1:\", rouge_scores[\"rouge1\"] * 100)\n",
    "print(\"🟥 ROUGE-2:\", rouge_scores[\"rouge2\"] * 100)\n",
    "print(\"🟥 ROUGE-L:\", rouge_scores[\"rougeL\"] * 100)\n",
    "\n",
    "bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "print(\"🟩 BERTScore (F1):\", sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"]) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr4Fuv5wA3YX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 🔹 Load your fine-tuned model from a specific trial\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"  # change as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# 🔹 List of saved user questions (manually added or loaded from file)\n",
    "prompts = [\n",
    "    \"I feel like I'm constantly overthinking every little thing. How do I make it stop?\",\n",
    "    \"Why do I get so anxious when I have to talk to people, even my friends?\",\n",
    "    \"I wake up with a knot in my stomach every day. What can I do about it?\",\n",
    "    \"im being bullied in my college no one to talk\",\n",
    "    \"i like to dance and sing\"\n",
    "    # ...add more prompts as needed\n",
    "]\n",
    "\n",
    "# 🔹 Generate and print responses (no saving)\n",
    "for i, question in enumerate(prompts, 1):\n",
    "    full_prompt = f\"<s>[INST] You are a kind and supportive mental health assistant who responds empathetically to this user message:\\n{question} [/INST]\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.6, top_p=0.9, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"🧠 Prompt {i}: {question}\\n🤖 Response: {response}\\n{'-'*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ehtQNrAwSF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGhOWmjxWnIx"
   },
   "source": [
    "Trial 6 Model (Chatbot Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blX2zQphU_DT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Trial 6 model path\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
    "\n",
    "#Load model + tokenizer\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "#Load test prompts\n",
    "test_file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\"\n",
    "with open(test_file_path, \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "#Run inference\n",
    "results = []\n",
    "for i, sample in enumerate(tqdm(test_data, desc=\"🧠 Running Trial 6 Inference\")):\n",
    "    prompt = f\"<s>[INST] You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{sample['instruction']} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    results.append({\n",
    "        \"user_prompt\": sample[\"instruction\"],\n",
    "        \"model_response\": decoded.strip()\n",
    "    })\n",
    "\n",
    "#Display responses\n",
    "for idx, r in enumerate(results, 1):\n",
    "    print(f\"\\n🧠 Prompt {idx}: {r['user_prompt']}\\n🤖 Response: {r['model_response']}\")\n",
    "\n",
    "#Save to CSV\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"/content/trial6_test_outputs.csv\", index=False)\n",
    "print(\"\\n✅ Responses saved to trial6_test_outputs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQyO070zZo48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S3Vy-rYZHQP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHZbot9cZtXw"
   },
   "source": [
    "# FINAL ANDROID APP BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlL97JHca_a6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import os, threading\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "SAFETY_KEYWORDS = [\n",
    "    \"kill myself\", \"end my life\", \"suicide\", \"hurt myself\", \"self-harm\",\n",
    "    \"i want to die\", \"not worth living\", \"give up on life\"\n",
    "]\n",
    "\n",
    "SAFETY_RESPONSE = (\n",
    "    \"💙 I'm really sorry you're feeling this way. You're not alone. \"\n",
    "    \"Please seek help from a professional. You can contact Lifeline Australia at 13 11 14, \"\n",
    "    \"available 24/7 for support.\"\n",
    ")\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    user_input = data.get(\"message\", \"\")\n",
    "    mode = data.get(\"mode\", \"long\")  # 'short' or 'long'\n",
    "\n",
    "    if any(keyword in user_input.lower() for keyword in SAFETY_KEYWORDS):\n",
    "        return jsonify({\"response\": SAFETY_RESPONSE})\n",
    "\n",
    "    style_instruction = {\n",
    "        \"short\": \"Keep it brief, supportive, and to the point (under 100 words).\",\n",
    "        \"long\": \"Give a detailed and compassionate response with examples and encouragement (around 200 words).\"\n",
    "    }.get(mode, \"Give a helpful and supportive response.\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"<s>[INST] You are a kind and supportive mental health assistant.\\n\"\n",
    "        f\"{style_instruction}\\nUser: {user_input}\\n[/INST]\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
    "\n",
    "    max_tokens = 400 if mode == \"long\" else 120\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = decoded.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in decoded else decoded.strip()\n",
    "\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "\n",
    "ngrok.set_auth_token(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "def run_flask():\n",
    "    app.run()\n",
    "\n",
    "threading.Thread(target=run_flask).start()\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"Public URL to access chatbot API: {public_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwq9MXLUdttx"
   },
   "outputs": [],
   "source": [
    "!pkill -f uvicorn\n",
    "!pkill -f streamlit\n",
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CI_gyax9bFcu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgtmgEkerxNX"
   },
   "source": [
    "# Final Web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q98Qz1jZqpmt"
   },
   "outputs": [],
   "source": [
    "code = '''\n",
    "\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# --- Page Config ---\n",
    "st.set_page_config(page_title=\"🧠 CounselChat\", page_icon=\"💬\", layout=\"centered\")\n",
    "\n",
    "# --- Model Path ---\n",
    "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
    "\n",
    "# --- Bits & Bytes Config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "@st.cache_resource(show_spinner=True)\n",
    "def load_model():\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name_or_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "with st.spinner(\"🔄 Loading model...\"):\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "# --- Safety Filter ---\n",
    "SAFETY_KEYWORDS = [\n",
    "    \"kill myself\", \"end my life\", \"suicide\", \"hurt myself\", \"self-harm\",\n",
    "    \"i want to die\", \"not worth living\", \"give up on life\"\n",
    "]\n",
    "\n",
    "SAFETY_RESPONSE = (\n",
    "    \"💙 I'm really sorry you're feeling this way. You're not alone. \"\n",
    "    \"Please seek help from a professional. You can contact Lifeline Australia at 13 11 14, \"\n",
    "    \"available 24/7 for support.\"\n",
    ")\n",
    "\n",
    "# --- Chat History Container (Scrollable & Styled) ---\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "        .chat-container {\n",
    "            max-height: 500px;\n",
    "            overflow-y: auto;\n",
    "            padding-right: 10px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        .user-bubble {\n",
    "            background-color: #DCF8C6;\n",
    "            color: #000000;\n",
    "            padding: 10px;\n",
    "            border-radius: 15px;\n",
    "            text-align: right;\n",
    "            width: fit-content;\n",
    "            margin-left: auto;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .bot-bubble {\n",
    "            background-color: #F0F0F0;\n",
    "            color: #000000;\n",
    "            padding: 10px;\n",
    "            border-radius: 15px;\n",
    "            text-align: left;\n",
    "            width: fit-content;\n",
    "            margin-right: auto;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Header ---\n",
    "st.markdown(\"\"\"\n",
    "    <div style='text-align: center;'>\n",
    "        <h1>🧠 CounselChat</h1>\n",
    "        <p>Your empathetic mental health assistant</p>\n",
    "    </div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Chat Form ---\n",
    "mode = st.radio(\"Select Response Style:\", options=[\"short\", \"long\"], horizontal=True)\n",
    "user_input = st.text_input(\"💬 Type your message:\")\n",
    "\n",
    "# --- Trigger Chat ---\n",
    "if st.button(\"Send\") and user_input:\n",
    "    with st.container():\n",
    "        st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
    "\n",
    "        # Safety filter\n",
    "        if any(keyword in user_input.lower() for keyword in SAFETY_KEYWORDS):\n",
    "            st.markdown(f\"<div class='bot-bubble'>{SAFETY_RESPONSE}</div>\", unsafe_allow_html=True)\n",
    "        else:\n",
    "            with st.spinner(\"🤖 Thinking...\"):\n",
    "                style_instruction = {\n",
    "                    \"short\": \"Keep it brief, supportive, and to the point (under 100 words).\",\n",
    "                    \"long\": \"Give a detailed and compassionate response with examples and encouragement (around 200 words).\"\n",
    "                }.get(mode, \"Give a helpful and supportive response.\")\n",
    "\n",
    "                prompt = (\n",
    "                    f\"\"\"<s>[INST] You are a kind and supportive mental health assistant.\\n\"\"\"\n",
    "                    f\"\"\"{style_instruction}\\nUser: {user_input}\\n[/INST]\"\"\"\n",
    "                )\n",
    "\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
    "                max_tokens = 400 if mode == \"long\" else 120\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_tokens,\n",
    "                        temperature=0.6,\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                response = decoded.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in decoded else decoded.strip()\n",
    "\n",
    "                # Show conversation\n",
    "                st.markdown(f\"<div class='user-bubble'>{user_input}</div>\", unsafe_allow_html=True)\n",
    "                st.markdown(f\"<div class='bot-bubble'>🤖 {response}</div>\", unsafe_allow_html=True)\n",
    "\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "'''\n",
    "#Save to app.py\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUO2XDp0o2dQ"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSlKTLuio3ui"
   },
   "outputs": [],
   "source": [
    "!pkill -f streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxPwmZlGy71Z"
   },
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxvLAO3Ay-wL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxLJ-7qMAtjZ"
   },
   "source": [
    "# Evaluation Summary For Counsel Chat + Mistral 7b model trials\n",
    "\n",
    "\n",
    "###Evaluation Summary\n",
    "\n",
    "| Trial | Dataset Used                            | Synthetic Data | Prompt Style                                                | LoRA Config (r/α) | Epochs | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore (F1) | Notes                          |\n",
    "|-------|------------------------------------------|----------------|--------------------------------------------------------------|-------------------|--------|--------|----------|----------|----------|----------------|---------------------------------|\n",
    "| 1     | CounselChat (890)                        | ❌ No          | \"You are a professional mental health assistant...\"         | r=8 / α=16        | 3      | 0.0203 | 0.2888   | 0.0429   | 0.1349   | 0.8311         | Baseline trial                 |\n",
    "| 2     | CounselChat (same)                       | ❌ No          | Same as Trial 1                                              | r=16 / α=32       | 4      | 0.0173 | 0.2315   | 0.0334   | 0.122    | 0.8076         | Slight drop in scores          |\n",
    "| 3     | CounselChat + 200 synthetic              | ✅ Yes         | Same as Trial 1                                              | r=16 / α=32       | 4      | 0.0228 | 0.2984   | 0.0461   | 0.1387   | 0.8331         | Better than Trial 1 & 2        |\n",
    "| 4     | Smart-filtered CounselChat (2,116)       | ❌ No          | Same as Trial 1                                              | r=16 / α=32       | 5      | 0.0222 | 0.2384   | 0.0387   | 0.1225   | 0.8144         | Fast mode, improved token len  |\n",
    "| 5     | Same as Trial 4                          | ❌ No          | \"How would you support someone who says...\" variation        | r=16 / α=32       | 5      | 0.0243 | 0.3071   | 0.0416   | 0.1307   | 0.8344         | Most human-like tone           |\n",
    "| 6     | Merged CounselChat + Empathetic (cleaned)| ✅ Yes (merged)| \"You are a helpful mental health assistant...\" (rotary-safe) | r=8 / α=16        | 4      | 0.028  | 0.2967   | 0.054    | 0.1442   | 0.8328         | ✅ Best overall on clean eval   |\n",
    "| 6★    | Trial 6 on MentalChat16K (general test)  | ✅ Yes         | Same as above                                                | r=8 / α=16        | —      | 1.91   | 31.90    | 5.00     | 13.57    | 83.33%          | ✅ Best generalization ability  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4dNynhaA4_K"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avFjogHtkxXU"
   },
   "outputs": [],
   "source": [
    "from nbformat import read, write\n",
    "from nbformat import NO_CONVERT\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Colab Notebooks/Copy-of-counsel-mistral7b.ipynb\") as f:\n",
    "    nb = read(f, as_version=NO_CONVERT)\n",
    "\n",
    "# Remove invalid widget metadata\n",
    "if \"widgets\" in nb.metadata and \"state\" not in nb.metadata[\"widgets\"]:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Colab Notebooks/counsel-mistral7b-no_output-cleaned_notebook.ipynb\", \"w\") as f:\n",
    "    write(nb, f)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPiETYyc7MeidtxTzT2b7/X",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1CVE3tXbo9ffcINt71o6UOvveXGQK_65N",
     "timestamp": 1747507809804
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
