{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HDYaYgnUtRFj"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers\n",
        "!pip install -U accelerate datasets peft trl\n",
        "!pip install -U huggingface_hub\n",
        "!pip install -q evaluate bert-score\n",
        "!pip install rouge_score\n",
        "!pip install -q streamlit pyngrok\n",
        "!pip install -q flask flask-ngrok flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0qw4Oj_qwp-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "R0x0ttG1wszF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "env_path = Path(\"/content/drive/MyDrive/secrets/.env\")\n",
        "env_content = \"HF_TOKEN=ACTUAL_NGROK_TOKEN\"\n",
        "env_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "env_path.write_text(env_content)\n",
        "\n",
        "print(f\".env file created at {env_path}\")\n"
      ],
      "metadata": {
        "id": "ggbDC6T8gCC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(dotenv_path=env_path)\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(os.getenv(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "9iZWzKrAguBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Counsel Chat"
      ],
      "metadata": {
        "id": "Epq98bfm0IMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Load dataset\n",
        "file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/20200325_counsel_chat.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#Preview dataset\n",
        "print(\"üßæ Columns:\", df.columns)\n",
        "print(\"\\nüìå Sample rows:\")\n",
        "display(df.head(3))\n",
        "\n",
        "#Check for missing values\n",
        "print(\"\\nüßº Missing values per column:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "k8HwilSzwyQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Extract Best Q&A Pairs to JSONL (for LoRA Fine-Tuning)"
      ],
      "metadata": {
        "id": "xPyEtvqo0Tf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "#Group by questionID and select top upvoted answer\n",
        "top_answers_df = (\n",
        "    df.sort_values(\"upvotes\", ascending=False)\n",
        "      .groupby(\"questionID\", as_index=False)\n",
        "      .first()\n",
        ")\n",
        "\n",
        "#Format into JSONL format for instruction-output fine-tuning\n",
        "jsonl_data = []\n",
        "for _, row in top_answers_df.iterrows():\n",
        "    instruction = row[\"questionText\"].strip()\n",
        "    output = row[\"answerText\"].strip()\n",
        "\n",
        "    #Optional: Skip if too short\n",
        "    if len(instruction.split()) < 4 or len(output.split()) < 4:\n",
        "        continue\n",
        "\n",
        "    jsonl_data.append({\"instruction\": instruction, \"output\": output})\n",
        "\n",
        "#Save to JSONL file\n",
        "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for entry in jsonl_data:\n",
        "        json.dump(entry, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Saved {len(jsonl_data)} cleaned Q&A pairs to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "KoYkbj-t0UuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAhstEVv0ZIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with Mistral + Counsel Chat"
      ],
      "metadata": {
        "id": "7gKWui2D1Nvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#Load base model (or your fine-tuned model path)\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "#Load your CounselChat cleaned JSONL\n",
        "jsonl_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
        "samples = []\n",
        "\n",
        "with open(jsonl_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        samples.append(data)\n",
        "\n",
        "#Test first 3 prompts\n",
        "for i in range(3):\n",
        "    user_question = samples[i]['instruction']\n",
        "    expected_answer = samples[i]['output']\n",
        "\n",
        "    #Prompt style to guide model\n",
        "    prompt = f\"\"\"<s>[INST] You are a supportive mental health assistant. Provide thoughtful guidance.\n",
        "\n",
        "User: {user_question}\n",
        "\n",
        "Respond with empathy and practical steps. [/INST]\"\"\"\n",
        "\n",
        "    #Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=200)\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\nüìå Prompt: {user_question}\\nüîπ Expected: {expected_answer}\\nü§ñ Model: {response}\\n\" + \"=\"*80)\n"
      ],
      "metadata": {
        "id": "ru6fE__O1Sdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYy9lDcc11K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning -Trial 1"
      ],
      "metadata": {
        "id": "-lQIuZhc30Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "#Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\", split=\"train\")\n",
        "\n",
        "#Load tokenizer + Fix padding\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Fix padding token issue\n",
        "\n",
        "#Tokenization function\n",
        "def tokenize(example):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    result = tokenizer(prompt, text_target=example[\"output\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    return result\n",
        "\n",
        "#Tokenize dataset\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
        "\n",
        "#Load base model + apply LoRA\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "#Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=20,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "#Start fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "#Save model\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\")\n",
        "\n",
        "print(\"‚úÖ Trial 1 fine-tuning complete and saved to Drive!\")\n"
      ],
      "metadata": {
        "id": "RwTh9NvJ32ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 1 - Evaluation"
      ],
      "metadata": {
        "id": "JLFsGyBI8nXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "\n",
        "#Load the evaluation dataset (1000 samples)\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
        "    split=\"train[:300]\"\n",
        ")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial1\"\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "\n",
        "#Load evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "#Inference loop\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 1\"):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "\n",
        "#Compute metrics\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "#Print results\n",
        "print(\"\\nüìä Trial 1 Evaluation Results (1000 samples):\")\n",
        "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
        "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
        "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
        "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
        "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n"
      ],
      "metadata": {
        "id": "fSqCKfcb6hLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRvXJdDy83n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning trial 2"
      ],
      "metadata": {
        "id": "2N-Fe8D6D6Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import gc\n",
        "\n",
        "#Clear GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Load dataset (~890 samples)\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "#Load model + tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#Tokenization function\n",
        "def tokenize(example):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    output = f\" {example['output']}</s>\"\n",
        "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
        "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
        "    return result\n",
        "\n",
        "#Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
        "\n",
        "#LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training args (tested in Trial 9)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#Train\n",
        "trainer.train()\n",
        "\n",
        "#Save model + tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\")\n",
        "\n",
        "print(\"‚úÖ Trial 2 fine-tuning complete and saved to Drive!\")\n"
      ],
      "metadata": {
        "id": "lSNbU5tkD8UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mi7rvAjMF9mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 2 - evaluation"
      ],
      "metadata": {
        "id": "fe-bAMDYJ0JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Load evaluation dataset (300 or 1000 samples)\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
        "    split=\"train[:300]\"\n",
        ")\n",
        "\n",
        "#Load the PEFT config + base model\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial2\"  # üëà Change here\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "#Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "#Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "#Load metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "#Inference loop\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 2\"):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "\n",
        "#Evaluation\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "#Print results\n",
        "print(\"\\nüìä Trial 2 Evaluation Results (300 samples):\")\n",
        "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
        "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
        "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
        "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
        "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n"
      ],
      "metadata": {
        "id": "eraZ3CQpJ0sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQFK5gwjKHOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating synthetic GPT samples"
      ],
      "metadata": {
        "id": "ZHVquUAuWul8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Synthetic Sample Generation ‚Äì CounselChat Augmentation (Trial 3)\n",
        "To enhance the performance of our Mistral 7B fine-tuning for the mental health chatbot, we generate 200 high-quality synthetic Q&A samples. These samples are designed to:\n",
        "\n",
        "Match the tone, format, and structure of the original CounselChat dataset\n",
        "\n",
        "Improve generalization and bigram-level coherence (ROUGE-2)\n",
        "\n",
        "Stay lightweight to avoid exhausting Colab Pro A100 resources\n",
        "\n",
        "Each synthetic entry consists of:\n",
        "\n",
        "A realistic user query about anxiety, depression, or emotional distress (instruction)\n",
        "\n",
        "A thoughtful, structured, empathetic multi-step response (output) based on known therapeutic practices (e.g., CBT, DBT)\n",
        "\n",
        "These 200 samples will be merged with ~890 real samples to create a new training set (counselchat_augmented_1090.jsonl) for Trial 3 fine-tuning."
      ],
      "metadata": {
        "id": "2NJD2f1XXYbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "#Expanded set of ~60 varied emotional/mental health prompts\n",
        "user_issues = [\n",
        "    \"I feel anxious about everyday situations, even simple ones like sending emails.\",\n",
        "    \"I can't stop overanalyzing conversations after they happen.\",\n",
        "    \"I'm always expecting the worst-case scenario, even when things seem fine.\",\n",
        "    \"My heart races whenever I have to speak in front of others.\",\n",
        "    \"I feel guilty when I try to take time for myself.\",\n",
        "    \"I‚Äôm scared to share how I feel with my family because they might judge me.\",\n",
        "    \"Sometimes I just feel numb, like I‚Äôm not really here.\",\n",
        "    \"I constantly compare myself to others and feel like I‚Äôm falling behind.\",\n",
        "    \"I feel overwhelmed by my responsibilities and don't know where to start.\",\n",
        "    \"I'm always trying to please others and end up neglecting myself.\",\n",
        "    \"Even when I achieve something, I feel like it's not good enough.\",\n",
        "    \"I can‚Äôt stop thinking about things I did wrong years ago.\",\n",
        "    \"I‚Äôm scared I‚Äôll never feel truly happy again.\",\n",
        "    \"Sometimes I cry for no reason and I don't know how to explain it.\",\n",
        "    \"I get upset with myself when I‚Äôm not productive enough.\",\n",
        "    \"I feel like a burden to the people around me.\",\n",
        "    \"I hate asking for help because I don't want to seem weak.\",\n",
        "    \"I isolate myself even though I don‚Äôt want to be alone.\",\n",
        "    \"It feels like my mind is always racing and I can't shut it off.\",\n",
        "    \"I‚Äôve been eating less lately because I feel stressed and anxious.\",\n",
        "    \"I feel like my life lacks purpose or direction.\",\n",
        "    \"I try to sleep but my brain keeps bringing up bad memories.\",\n",
        "    \"I always feel like I‚Äôm being judged, even when no one is around.\",\n",
        "    \"Sometimes I fake being okay because I don‚Äôt want others to worry.\",\n",
        "    \"I struggle to get out of bed in the mornings lately.\",\n",
        "    \"I don‚Äôt know how to express my emotions without feeling ashamed.\",\n",
        "    \"I keep doubting my self-worth, even when others praise me.\",\n",
        "    \"I feel stuck in a loop of negative thinking.\",\n",
        "    \"It‚Äôs hard for me to enjoy things I used to love.\",\n",
        "    \"I feel afraid of failing, so I avoid trying new things.\",\n",
        "    \"I‚Äôve lost interest in socializing with others.\",\n",
        "    \"I get nervous even before small meetings or group calls.\",\n",
        "    \"I overthink everything I say and do.\",\n",
        "    \"I feel like I‚Äôm pretending to be okay all the time.\",\n",
        "    \"It feels like no one really understands me.\",\n",
        "    \"I panic when plans change unexpectedly.\",\n",
        "    \"I feel like I'm falling apart on the inside.\",\n",
        "    \"I worry a lot about things I can‚Äôt control.\",\n",
        "    \"I feel disconnected from my own life.\",\n",
        "    \"I‚Äôm trying to heal, but progress feels painfully slow.\",\n",
        "    \"I question if therapy is even helping me.\",\n",
        "    \"I want to trust people but I'm afraid of being hurt.\",\n",
        "    \"I avoid eye contact because I feel ashamed.\",\n",
        "    \"I feel anxious when someone compliments me.\",\n",
        "    \"I'm afraid people are pretending to like me.\",\n",
        "    \"I get angry at myself for feeling this way.\",\n",
        "    \"I often feel like an imposter, even with my accomplishments.\",\n",
        "    \"I‚Äôm overwhelmed and don‚Äôt know where to begin.\",\n",
        "    \"I worry that people secretly dislike me.\",\n",
        "    \"I sometimes wish I could just disappear for a while.\",\n",
        "    \"I get drained by social events, even short ones.\",\n",
        "    \"I feel like I can't be myself around others.\",\n",
        "    \"I don‚Äôt feel motivated to do anything lately.\",\n",
        "    \"I try to look strong, but inside I‚Äôm falling apart.\",\n",
        "    \"I‚Äôm scared of being vulnerable with people.\",\n",
        "    \"I feel emotionally exhausted by my own thoughts.\",\n",
        "    \"I want to feel better, but I don‚Äôt know how.\",\n",
        "    \"I get scared of being alone but also fear getting close to people.\"\n",
        "]\n",
        "\n",
        "# Same response components from earlier\n",
        "responses_intro = [\n",
        "    \"Thank you for being open and sharing this. You're not alone in feeling this way.\",\n",
        "    \"I hear you, and I want you to know that your feelings are valid.\",\n",
        "    \"What you're experiencing is difficult, and it's good that you're reaching out.\",\n",
        "    \"It takes courage to speak up about this ‚Äî you're already taking a positive step.\",\n",
        "    \"I'm really glad you asked this. Many people go through similar emotions.\"\n",
        "]\n",
        "\n",
        "responses_body = [\n",
        "    \"Anxiety can often lead to overthinking and self-doubt. It helps to practice grounding techniques like focused breathing or mindful journaling.\",\n",
        "    \"Cognitive Behavioral Therapy (CBT) has proven effective in managing negative thought loops. You might consider exploring CBT worksheets or speaking with a therapist trained in it.\",\n",
        "    \"Try to notice when these thoughts come up and gently challenge them by asking yourself if they're based on facts or assumptions.\",\n",
        "    \"Consider keeping a self-compassion journal where you write down moments you were kind to yourself or others.\",\n",
        "    \"Sometimes, talking to a counselor or even joining a peer support group can provide relief and perspective.\"\n",
        "]\n",
        "\n",
        "responses_close = [\n",
        "    \"Remember, healing is not linear. Small steps matter.\",\n",
        "    \"Be patient with yourself. You deserve support and care.\",\n",
        "    \"You are worthy of kindness ‚Äî from others and from yourself.\",\n",
        "    \"Don‚Äôt hesitate to seek professional help when needed. You‚Äôre not alone.\",\n",
        "    \"Keep going. You're doing better than you think.\"\n",
        "]\n",
        "\n",
        "# Generate and save 200 synthetic Q&A pairs\n",
        "synthetic_samples = []\n",
        "for _ in range(200):\n",
        "    instruction = random.choice(user_issues)\n",
        "    output = \"\\n\\n\".join([\n",
        "        random.choice(responses_intro),\n",
        "        random.choice(responses_body),\n",
        "        random.choice(responses_close)\n",
        "    ])\n",
        "    synthetic_samples.append({\n",
        "        \"instruction\": instruction,\n",
        "        \"output\": output\n",
        "    })\n",
        "\n",
        "# Save as JSONL\n",
        "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_synthetic_200.jsonl\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for item in synthetic_samples:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ 200 synthetic samples saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "Dd96pEXtWz2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MrHblaKRXq3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating new augmented dataset - synthetic200 + counsel chat dataset"
      ],
      "metadata": {
        "id": "qlqN7WsuX7Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "#File paths\n",
        "real_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\"\n",
        "synthetic_path = \"/content/dtasets/counselchat_augmented_1090.jsonl\"rive/MyDrive/chatbot/counsel/datasets/counselchat_synthetic_200.jsonl\"\n",
        "output_path = \"/content/drive/MyDrive/chatbot/counsel/da\n",
        "\n",
        "#Load real CounselChat data\n",
        "with open(real_path, \"r\") as f:\n",
        "    real_data = [json.loads(line.strip()) for line in f]\n",
        "\n",
        "#Load synthetic samples\n",
        "with open(synthetic_path, \"r\") as f:\n",
        "    synthetic_data = [json.loads(line.strip()) for line in f]\n",
        "\n",
        "#Combine and shuffle\n",
        "combined_data = real_data + synthetic_data\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "#Save merged dataset as JSONL\n",
        "with open(output_path, \"w\") as f:\n",
        "    for item in combined_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Merged dataset saved to: {output_path}\")\n",
        "print(f\"üìä Total samples: {len(combined_data)}\")\n"
      ],
      "metadata": {
        "id": "bIx7D2XzYEZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShAuC8aqYHWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 3 with augmented dataset"
      ],
      "metadata": {
        "id": "0LjBtIYTYf4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import gc\n",
        "\n",
        "#Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Load merged dataset (Counsel + 200 synthetic)\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_augmented_1090.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "#Load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#Tokenize\n",
        "def tokenize(example):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    output = f\" {example['output']}</s>\"\n",
        "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
        "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
        "\n",
        "#LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#Train the model\n",
        "trainer.train()\n",
        "\n",
        "#Save model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\")\n",
        "\n",
        "print(\"‚úÖ Trial 3 fine-tuning complete and saved to Drive!\")\n"
      ],
      "metadata": {
        "id": "Tjxwg2OyYkYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbqlO9z1Ymjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 3 - Evaluation"
      ],
      "metadata": {
        "id": "ggm_iDdeaO7c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMPysrNooULS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Load evaluation dataset\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_cleaned.jsonl\",\n",
        "    split=\"train[:300]\"  # Adjust to 1000 if needed\n",
        ")\n",
        "\n",
        "#Load the PEFT model and config\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial3\"\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "#Load base model and LoRA adapter\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "#Load evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "#Run generation\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 3\"):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "\n",
        "#Compute metrics\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "#Print results\n",
        "print(\"\\nüìä Trial 3 Evaluation Results (300 samples):\")\n",
        "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
        "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
        "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
        "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
        "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")"
      ],
      "metadata": {
        "id": "WrOxFh6qaUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdpW_y12ad0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Cleaning Updated"
      ],
      "metadata": {
        "id": "OUDCnfTzGrNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Smart Filtering of CounselChat Dataset (Explained)\n",
        "\n",
        "This dataset cleaning script was applied to the original CSV file `20200325_counsel_chat.csv` from CounselChat.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Step-by-Step Actions\n",
        "\n",
        "#### 1. **Loaded the original raw CSV**\n",
        "- File: `20200325_counsel_chat.csv`\n",
        "- Contains therapist responses to mental health questions.\n",
        "- Columns included: `questionText`, `answerText`, `topic`, `therapistInfo`, etc.\n",
        "\n",
        "#### 2. **Extracted Key Fields**\n",
        "Only two fields were kept to train the model:\n",
        "\n",
        "| CSV Field       | Transformed Field |\n",
        "|----------------|-------------------|\n",
        "| `questionText` | `instruction`     |\n",
        "| `answerText`   | `output`          |\n",
        "\n",
        "Example format:\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"I‚Äôve been feeling anxious a lot lately. How can I manage it?\",\n",
        "  \"output\": \"Anxiety is common. You can begin with deep breathing and journaling...\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Applied Smart Filters**\n",
        "The goal was to remove low-quality or misleading samples.\n",
        "\n",
        "| Filter Rule                                      | Reason                                              |\n",
        "|--------------------------------------------------|-----------------------------------------------------|\n",
        "| `instruction` and `output` must not be empty     | Skips blank entries                                 |\n",
        "| `instruction` must have ‚â• 5 words                | Too-short questions aren't useful                   |\n",
        "| `output` must have ‚â• 15 words                    | Short answers don‚Äôt help the model learn enough     |\n",
        "| No phrases like ‚ÄúI am an AI‚Äù, ‚ÄúI don‚Äôt understand‚Äù | Removes hallucinated or chatbot-like answers        |\n",
        "| `instruction` must not appear inside the `output`| Avoid parroting                                     |\n",
        "\n",
        "This ensures cleaner and more meaningful learning data.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Saved Cleaned Data**\n",
        "- Saved as JSONL to:\n",
        "  `/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl`\n",
        "- Each line contains one valid instruction-output pair\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Matters\n",
        "- Improves dataset quality for fine-tuning\n",
        "- Reduces junk samples and repetitive patterns\n",
        "- Leads to **better BLEU, ROUGE, and BERTScore results**"
      ],
      "metadata": {
        "id": "zA06ISrMVaTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/20200325_counsel_chat.csv\"  # Update path if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"Total rows:\", len(df))\n",
        "\n",
        "def format_row(row):\n",
        "    instruction = row.get(\"questionText\", \"\").strip()\n",
        "    output = row.get(\"answerText\", \"\").strip()\n",
        "    return {\"instruction\": instruction, \"output\": output}\n",
        "\n",
        "def is_valid(example):\n",
        "    instr = example[\"instruction\"]\n",
        "    out = example[\"output\"]\n",
        "\n",
        "    # Basic checks\n",
        "    if not instr or not out:\n",
        "        return False\n",
        "    if len(instr.split()) < 5 or len(out.split()) < 15:\n",
        "        return False\n",
        "    if \"I am an AI\" in out or \"I don't understand\" in out:\n",
        "        return False\n",
        "    if instr.lower() in out.lower():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "filtered = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted = format_row(row)\n",
        "    if is_valid(formatted):\n",
        "        filtered.append(formatted)\n",
        "\n",
        "print(f\"Total valid instruction-output pairs: {len(filtered)}\")\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for item in filtered:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(\"Cleaned JSONL saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "1mMhQy0hGsKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTDsBrHZI73V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning - Trial 4"
      ],
      "metadata": {
        "id": "f8oY57KlKQLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import gc\n",
        "\n",
        "#Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Load filtered dataset (2,116 samples)\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "#Load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#Tokenization function\n",
        "def tokenize(example):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    output = f\" {example['output']}</s>\"\n",
        "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=384)\n",
        "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=384)[\"input_ids\"]\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
        "\n",
        "#LoRA Config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#Train!\n",
        "trainer.train()\n",
        "\n",
        "#Save model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\")\n",
        "\n",
        "print(\"‚úÖ Trial 4 complete! Model saved to Drive.\")\n"
      ],
      "metadata": {
        "id": "k04Wps7YKUT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65PXSARyKbyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 4 - Evaluation"
      ],
      "metadata": {
        "id": "rzn6bNOaO_2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Enable TF32 for faster matrix operations\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "#Load smaller eval set (500 samples for quick test)\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
        "    split=\"train[:300]\"\n",
        ")\n",
        "\n",
        "#Load PEFT config and model\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\"\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "#Load evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "#Inference loop with timing\n",
        "predictions = []\n",
        "references = []\n",
        "token_lengths = []\n",
        "start_time = time.time()\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 4 (500 samples, fast mode)\"):\n",
        "    prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{example['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,  # ‚úÖ Reduced from 200\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "    token_lengths.append(len(tokenizer.tokenize(pred)))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "#Evaluation metrics\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "#Performance logs\n",
        "avg_gen_time = round((end_time - start_time) / len(eval_dataset), 2)\n",
        "avg_token_len = round(sum(token_lengths) / len(token_lengths), 2)\n",
        "\n",
        "print(\"\\nüìä Trial 4 Evaluation Results (500 samples, Fast Mode):\")\n",
        "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
        "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
        "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
        "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
        "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Avg generation time per sample: {avg_gen_time} seconds\")\n",
        "print(f\"üß† Avg token length of output: {avg_token_len} tokens\")\n"
      ],
      "metadata": {
        "id": "KQbzLHc2PDRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Mistral-7B + CounselChat Hyperparameter Tuning Tracker\n",
        "\n",
        "### ‚úÖ Trial Summary Table\n",
        "\n",
        "| Trial | Dataset Size                  | LoRA `r` | Alpha | Dropout | Epochs | LR     | Prompt Style                                    | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore | Notes            |\n",
        "|-------|-------------------------------|----------|--------|----------|--------|--------|------------------------------------------------|--------|----------|----------|----------|------------|------------------|\n",
        "| 1     | 890 real                      | 8        | 16     | 0.05     | 3      | 2e-5   | Friendly therapist tone                         | 0.0203 | 0.2888   | 0.0429   | 0.1349   | 0.8311     | Baseline         |\n",
        "| 2     | 890 real                      | 16       | 32     | 0.05     | 4      | 2e-5   | Same prompt as Trial 1                          | 0.0173 | 0.2315   | 0.0334   | 0.1220   | 0.8076     | More stable LoRA |\n",
        "| 3     | 1,015 (real + 200 synthetic)  | 16       | 32     | 0.05     | 4      | 2e-5   | Same prompt as Trial 1                          | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058     | With GPT samples |\n",
        "| ‚úÖ 4  | 2,116 smart-filtered (real)   | 16       | 32     | 0.05     | 5      | 2e-5   | Same prompt, full cleaned dataset               | **0.0222** | 0.2384   | **0.0387** | 0.1225   | **0.8144** | Best so far üî•   |\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Notes:\n",
        "- **BLEU** = n-gram overlap\n",
        "- **ROUGE-1/2/L** = word, bigram, sequence overlap\n",
        "- **BERTScore** = semantic similarity using RoBERTa\n",
        "- All trials used the same system prompt:  \n",
        "  `\"You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\"`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DUkommnVdr0e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TF8t_OuPEVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 4 Model (Chatbot Testing)"
      ],
      "metadata": {
        "id": "y-Ab1XpVfGAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bulk message testing"
      ],
      "metadata": {
        "id": "ELSW7UYEfnrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test prompts for Mistral-7B + CounselChat model\n",
        "test_prompts = [\n",
        "    \"I feel like I'm constantly overthinking every little thing. How do I make it stop?\",\n",
        "    \"Why do I get so anxious when I have to talk to people, even my friends?\",\n",
        "    \"I wake up with a knot in my stomach every day. What can I do about it?\",\n",
        "    \"Lately I‚Äôve lost interest in everything. Nothing makes me happy anymore.\",\n",
        "    \"Is it normal to feel tired all the time even when I‚Äôm not physically active?\",\n",
        "    \"I feel numb, like nothing really matters. What should I do?\",\n",
        "    \"I had a panic attack at work and now I‚Äôm afraid it‚Äôll happen again. How do I cope?\",\n",
        "    \"My heart races for no reason and I feel like I can't breathe. Am I losing control?\",\n",
        "    \"How can I calm down quickly when I feel overwhelmed in public?\",\n",
        "    \"No matter what I do, I always feel like I‚Äôm not good enough.\",\n",
        "    \"I compare myself to everyone and feel like I‚Äôm constantly failing.\",\n",
        "    \"Why do I always blame myself when something goes wrong?\",\n",
        "    \"I always say yes to people even when I‚Äôm exhausted. How do I set boundaries?\",\n",
        "    \"My partner doesn‚Äôt understand my anxiety. It‚Äôs hurting our relationship. What should I do?\",\n",
        "    \"I feel guilty for putting myself first. Is that selfish?\"\n",
        "]\n",
        "\n",
        "#Save to TXT\n",
        "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.txt\", \"w\") as txt_file:\n",
        "    txt_file.write(\"\\n\".join(test_prompts))\n",
        "\n",
        "#Save to JSONL\n",
        "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\", \"w\") as jsonl_file:\n",
        "    for prompt in test_prompts:\n",
        "        jsonl_file.write(f'{{\"instruction\": \"{prompt}\"}}\\n')\n",
        "\n",
        "#Save to CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"user_prompt\": test_prompts})\n",
        "df.to_csv(\"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Files saved to Colab:\")\n",
        "print(\"- test_prompts_counselchat.txt\")\n",
        "print(\"- test_prompts_counselchat.jsonl\")\n",
        "print(\"- test_prompts_counselchat.csv\")\n"
      ],
      "metadata": {
        "id": "DZPCRYgsfml4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "\n",
        "#Load the fine-tuned model (Trial 4)\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial4\"\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "#Load test prompts from JSONL\n",
        "test_file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\"\n",
        "prompts = []\n",
        "with open(test_file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        entry = json.loads(line)\n",
        "        prompts.append(entry[\"instruction\"])\n",
        "\n",
        "#Define the response function\n",
        "def generate_response(prompt, max_tokens=150):\n",
        "    full_prompt = f\"<s>[INST] You are a professional mental health assistant providing thoughtful, multi-step advice to help users with anxiety, stress, and emotional struggles.\\n{prompt} [/INST]\"\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "#Generate and print responses\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"\\nüß† Prompt {i}: {prompt}\")\n",
        "    print(\"ü§ñ Response:\\n\", generate_response(prompt))\n"
      ],
      "metadata": {
        "id": "GzHVmOSIfIDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8FP3YliQfKoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning - Trial 5"
      ],
      "metadata": {
        "id": "XdHO1TkZl-WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers datasets peft accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import gc\n",
        "\n",
        "#Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Load filtered dataset (2,116 samples)\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "#Load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#Trial 5 Prompt Style: Updated for variety and tone\n",
        "def tokenize(example):\n",
        "    prompt = f\"<s>[INST] As a helpful mental health assistant, how would you support someone who says:\\n\\\"{example['instruction']}\\\" [/INST]\"\n",
        "    output = f\" {example['output']}</s>\"\n",
        "    result = tokenizer(prompt + output, padding=\"max_length\", truncation=True, max_length=512)\n",
        "    result[\"labels\"] = tokenizer(output, padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"]\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
        "\n",
        "#LoRA Config ‚Äî same structure as Trial 4\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training arguments (Trial 5 updates applied)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=1e-5,  # lower learning rate\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#Train!\n",
        "trainer.train()\n",
        "\n",
        "#Save model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\")\n",
        "\n",
        "print(\"‚úÖ Trial 5 complete! Model saved to Drive.\")\n"
      ],
      "metadata": {
        "id": "nNIXH9igmCfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pO-eRBo6mVSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1R7gaMy1tvG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 5 - Evaluation"
      ],
      "metadata": {
        "id": "WriMhs7ItulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "#Load 300-sample evaluation set\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\",\n",
        "    split=\"train[:300]\"\n",
        ")\n",
        "\n",
        "#Load fine-tuned Trial 5 model + PEFT config\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial5\"\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, model_path, local_files_only=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, local_files_only=True)\n",
        "\n",
        "#Load evaluation metrics\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "#Inference loop\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 5 (300 samples)\"):\n",
        "    prompt = f\"<s>[INST] As a helpful mental health assistant, how would you support someone who says:\\n\\\"{example['instruction']}\\\" [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "\n",
        "#Compute metrics\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "end = time.time()\n",
        "avg_gen_time = round((end - start) / len(predictions), 2)\n",
        "\n",
        "#Print results\n",
        "print(\"\\nüìä Trial 5 Evaluation Results (300 samples):\")\n",
        "print(f\"BLEU: {round(bleu_result['bleu'], 4)}\")\n",
        "print(f\"ROUGE-1: {round(rouge_result['rouge1'], 4)}\")\n",
        "print(f\"ROUGE-2: {round(rouge_result['rouge2'], 4)}\")\n",
        "print(f\"ROUGE-L: {round(rouge_result['rougeL'], 4)}\")\n",
        "print(f\"BERTScore (F1): {round(sum(bertscore_result['f1']) / len(bertscore_result['f1']), 4)}\")\n",
        "print(f\"‚è±Ô∏è Avg generation time/sample: {avg_gen_time}s\")\n"
      ],
      "metadata": {
        "id": "rJshrX73tyWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QB5heZWUt1x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CwICqZkK6boJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging - CounselChat + Augmented EmpatheticDialogues For Trial 6"
      ],
      "metadata": {
        "id": "-S0CD2aN6b9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/counselchat_smartfiltered.jsonl\", \"r\") as f:\n",
        "    counsel_data = [json.loads(line) for line in f]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/SIT782/datasets/empdiag_augmented_4100.jsonl\", \"r\") as f:\n",
        "    empathetic_data = [json.loads(line) for line in f]\n",
        "\n",
        "assert \"instruction\" in counsel_data[0] and \"output\" in counsel_data[0], \"‚ùå CounselChat format issue\"\n",
        "assert \"instruction\" in empathetic_data[0] and \"output\" in empathetic_data[0], \"‚ùå Empathetic format issue\"\n",
        "\n",
        "merged_data = counsel_data + empathetic_data\n",
        "unique_jsons = list({json.dumps(entry, sort_keys=True) for entry in merged_data})\n",
        "merged_cleaned = [json.loads(j) for j in unique_jsons]\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for sample in merged_cleaned:\n",
        "        json.dump(sample, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "#final report\n",
        "print(f\"Merged dataset saved to: {output_path}\")\n",
        "print(f\"Total samples after deduplication: {len(merged_cleaned)}\")\n"
      ],
      "metadata": {
        "id": "964gDO0268rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset analysis"
      ],
      "metadata": {
        "id": "CQiJ60t_8h6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load merged dataset\n",
        "with open(\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\", \"r\") as f:\n",
        "    merged_data = [json.loads(line) for line in f]\n",
        "\n",
        "# Basic stats\n",
        "total_samples = len(merged_data)\n",
        "instruction_lengths = [len(entry[\"instruction\"].split()) for entry in merged_data]\n",
        "output_lengths = [len(entry[\"output\"].split()) for entry in merged_data]\n",
        "\n",
        "# Token length distributions\n",
        "avg_instruction_len = sum(instruction_lengths) / total_samples\n",
        "avg_output_len = sum(output_lengths) / total_samples\n",
        "\n",
        "min_instruction_len = min(instruction_lengths)\n",
        "max_instruction_len = max(instruction_lengths)\n",
        "\n",
        "min_output_len = min(output_lengths)\n",
        "max_output_len = max(output_lengths)\n",
        "\n",
        "# Extremely short responses\n",
        "short_outputs = sum(1 for length in output_lengths if length < 20)\n",
        "\n",
        "# Summary\n",
        "dataset_stats = {\n",
        "    \"Total Samples\": total_samples,\n",
        "    \"Avg Instruction Length (words)\": round(avg_instruction_len, 2),\n",
        "    \"Avg Output Length (words)\": round(avg_output_len, 2),\n",
        "    \"Min Instruction Length\": min_instruction_len,\n",
        "    \"Max Instruction Length\": max_instruction_len,\n",
        "    \"Min Output Length\": min_output_len,\n",
        "    \"Max Output Length\": max_output_len,\n",
        "    \"Samples with Output < 20 words\": short_outputs\n",
        "}\n",
        "\n",
        "dataset_stats\n"
      ],
      "metadata": {
        "id": "KAGO2Vk97HC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why We Removed Outputs with Fewer Than 20 Words (Trial 6 Dataset)\n",
        "\n",
        "In our merged dataset (counselchat + empathetic_dialogues), many samples had very short responses, often under 20 words. While these samples may be realistic, they:\n",
        "\n",
        "    Lack informative content or emotional depth\n",
        "\n",
        "    Hurt phrase overlap metrics like ROUGE-2 and BLEU\n",
        "\n",
        "    Introduce noise or overly generic replies into training\n",
        "\n",
        "To improve model quality, especially for long-form conversational replies, we created a cleaned dataset where each response (output) has at least 20 words.\n",
        "\n",
        "This refined dataset (Trial 6) will help:\n",
        "\n",
        "    Boost evaluation scores (ROUGE, BLEU, BERTScore)\n",
        "\n",
        "    Encourage more complete, helpful responses\n",
        "\n",
        "    Train the model to match empathetic, multi-step advice"
      ],
      "metadata": {
        "id": "ky0ccuca8qrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "merged_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic.jsonl\"\n",
        "with open(merged_path, \"r\") as f:\n",
        "    merged_data = [json.loads(line) for line in f]\n",
        "\n",
        "#Filter: Keep only samples with output ‚â• 20 words\n",
        "filtered_data = [sample for sample in merged_data if len(sample[\"output\"].split()) >= 20]\n",
        "\n",
        "trial6_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\"\n",
        "with open(trial6_path, \"w\") as f:\n",
        "    for sample in filtered_data:\n",
        "        json.dump(sample, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(\"Trial 6 dataset saved!\")\n",
        "print(f\"Total samples (after filter): {len(filtered_data)}\")\n",
        "print(f\"Samples removed (< 20 words): {len(merged_data) - len(filtered_data)}\")\n"
      ],
      "metadata": {
        "id": "9XXQdtHh8nE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qn17EGd988rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 6 Tuning"
      ],
      "metadata": {
        "id": "xf40swfAtidV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ Trial 6 Fine-Tuning Script (merged_counsel_empathetic_cleaned_trial6.jsonl)"
      ],
      "metadata": {
        "id": "9CKq8D-B9xr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Load dataset (Trial 6)\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "#Load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False  # Prevent rotary crash\n",
        "\n",
        "#Preprocessing function (batched-safe)\n",
        "def preprocess(batch):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for instruction, output in zip(batch[\"instruction\"], batch[\"output\"]):\n",
        "        prompt = f\"You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{instruction}\"\n",
        "\n",
        "        prompt_tokens = tokenizer(prompt, truncation=True, max_length=256, padding=\"max_length\")\n",
        "        output_tokens = tokenizer(output, truncation=True, max_length=256, padding=\"max_length\")\n",
        "\n",
        "        input_ids = prompt_tokens[\"input_ids\"] + output_tokens[\"input_ids\"]\n",
        "        attention_mask = prompt_tokens[\"attention_mask\"] + output_tokens[\"attention_mask\"]\n",
        "        labels = [-100] * len(prompt_tokens[\"input_ids\"]) + output_tokens[\"input_ids\"]\n",
        "\n",
        "        # Ensure total length is 512 tokens\n",
        "        input_ids_list.append(input_ids[:512])\n",
        "        attention_mask_list.append(attention_mask[:512])\n",
        "        labels_list.append(labels[:512])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"attention_mask\": attention_mask_list,\n",
        "        \"labels\": labels_list\n",
        "    }\n",
        "\n",
        "#Apply preprocessing\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "#LoRA Config (rotary-safe)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    target_modules=[\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=1e-5,\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=[],\n",
        "    run_name=\"mistral_trial6\"\n",
        ")\n",
        "\n",
        "#Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#Train\n",
        "trainer.train()\n",
        "\n",
        "#Save\n",
        "model.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\")\n",
        "\n",
        "print(\"‚úÖ Trial 6 complete and saved!\")"
      ],
      "metadata": {
        "id": "aKw6ZLc_9yjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPrGsJuH96WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trial 6 - Evaluation"
      ],
      "metadata": {
        "id": "wQCaJjEGHsoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "#Load evaluation dataset (300 samples for speed)\n",
        "eval_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"/content/drive/MyDrive/chatbot/counsel/datasets/merged_counsel_empathetic_cleaned_trial6.jsonl\",\n",
        "    split=\"train[:300]\"\n",
        ")\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in tqdm(eval_dataset, desc=\"üîç Evaluating Trial 6 (300 samples)\"):\n",
        "    input_text = f\"<s>[INST] You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{example['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(example[\"output\"])\n",
        "\n",
        "bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "{\n",
        "    \"BLEU\": round(bleu_result[\"bleu\"], 4),\n",
        "    \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
        "    \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
        "    \"ROUGE-L\": round(rouge_result[\"rougeL\"], 4),\n",
        "    \"BERTScore (F1)\": round(sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]), 4)\n",
        "}\n"
      ],
      "metadata": {
        "id": "8w8FxhgMHvzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Trial 6 Evaluation Comparison: Mistral + CounselChat Trials\n",
        "\n",
        "| Trial   | Dataset                        | Samples | LoRA Params (r/Œ±) | Epochs | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore (F1) |\n",
        "|---------|--------------------------------|---------|-------------------|--------|--------|----------|----------|----------|----------------|\n",
        "| Trial 1 | counselchat_cleaned.jsonl    | ~890    | 8 / 16            | 3      | 0.0173 | 0.2315   | 0.0334   | 0.1220   | 0.8076         |\n",
        "| Trial 2 | Same as Trial 1                | ~890    | 16 / 32           | 4      | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058         |\n",
        "| Trial 3 | Real + 200 Synthetic           | ~1,015  | 16 / 32           | 4      | 0.0160 | 0.2163   | 0.0291   | 0.1134   | 0.8058         |\n",
        "| Trial 4 | Smart-filtered CounselChat     | ~2,116  | 16 / 32           | 5      | 0.0222 | 0.2384   | 0.0387   | 0.1225   | 0.8144         |\n",
        "| Trial 5 | Same dataset, prompt tweaks    | ~2,116  | 16 / 32           | 5      | 0.0243 | 0.3071   | 0.0416   | 0.1307   | 0.8344         |\n",
        "| **Trial 6** | Merged (filtered counsel) + Empathetic (augmented) | 2,277 | 8 / 16            | 4      | **0.0280** | **0.2967** | **0.0540** | **0.1442** | **0.8328**       |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ch0ORDR0WeiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers datasets evaluate bert_score tqdm\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Load 500 evaluation samples from MentalChat16K\n",
        "eval_data = load_dataset(\"ShenLab/MentalChat16K\", split=\"train\")\n",
        "eval_subset = eval_data.shuffle(seed=42).select(range(500))\n",
        "\n",
        "#Load fine-tuned model from Trial 12\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "#Response generator\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=350).to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=180,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#Generate predictions with live progress\n",
        "preds, refs = [], []\n",
        "\n",
        "print(\"üß† Generating responses on 500 MentalChat16K samples...\")\n",
        "for sample in tqdm(eval_subset, desc=\"Evaluating\"):\n",
        "    user_msg = sample[\"instruction\"]\n",
        "    ref_response = sample[\"output\"]\n",
        "    prompt = f\"<s>[INST] You are a kind and supportive mental health assistant who responds empathetically to this user message:\\n{user_msg} [/INST]\"\n",
        "    gen_response = generate_response(prompt)\n",
        "    preds.append(gen_response)\n",
        "    refs.append(ref_response)\n",
        "\n",
        "#Run evaluation metrics\n",
        "print(\"\\nüìä Running BLEU, ROUGE, and BERTScore evaluations...\\n\")\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "print(\"üîµ BLEU:\", bleu.compute(predictions=preds, references=refs)[\"bleu\"] * 100)\n",
        "\n",
        "rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
        "print(\"üü• ROUGE-1:\", rouge_scores[\"rouge1\"] * 100)\n",
        "print(\"üü• ROUGE-2:\", rouge_scores[\"rouge2\"] * 100)\n",
        "print(\"üü• ROUGE-L:\", rouge_scores[\"rougeL\"] * 100)\n",
        "\n",
        "bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
        "print(\"üü© BERTScore (F1):\", sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"]) * 100)\n"
      ],
      "metadata": {
        "id": "4dW7SssOVAJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# üîπ Load your fine-tuned model from a specific trial\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"  # change as needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# üîπ List of saved user questions (manually added or loaded from file)\n",
        "prompts = [\n",
        "    \"I feel like I'm constantly overthinking every little thing. How do I make it stop?\",\n",
        "    \"Why do I get so anxious when I have to talk to people, even my friends?\",\n",
        "    \"I wake up with a knot in my stomach every day. What can I do about it?\",\n",
        "    \"im being bullied in my college no one to talk\",\n",
        "    \"i like to dance and sing\"\n",
        "    # ...add more prompts as needed\n",
        "]\n",
        "\n",
        "# üîπ Generate and print responses (no saving)\n",
        "for i, question in enumerate(prompts, 1):\n",
        "    full_prompt = f\"<s>[INST] You are a kind and supportive mental health assistant who responds empathetically to this user message:\\n{question} [/INST]\"\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.6, top_p=0.9, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"üß† Prompt {i}: {question}\\nü§ñ Response: {response}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "id": "Xr4Fuv5wA3YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l0ehtQNrAwSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trial 6 Model (Chatbot Testing)"
      ],
      "metadata": {
        "id": "GGhOWmjxWnIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Trial 6 model path\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
        "\n",
        "#Load model + tokenizer\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "#Load test prompts\n",
        "test_file_path = \"/content/drive/MyDrive/chatbot/counsel/datasets/test_prompts_counselchat.jsonl\"\n",
        "with open(test_file_path, \"r\") as f:\n",
        "    test_data = [json.loads(line) for line in f]\n",
        "\n",
        "#Run inference\n",
        "results = []\n",
        "for i, sample in enumerate(tqdm(test_data, desc=\"üß† Running Trial 6 Inference\")):\n",
        "    prompt = f\"<s>[INST] You are a helpful mental health assistant. Respond empathetically and with clarity:\\n{sample['instruction']} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
        "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"user_prompt\": sample[\"instruction\"],\n",
        "        \"model_response\": decoded.strip()\n",
        "    })\n",
        "\n",
        "#Display responses\n",
        "for idx, r in enumerate(results, 1):\n",
        "    print(f\"\\nüß† Prompt {idx}: {r['user_prompt']}\\nü§ñ Response: {r['model_response']}\")\n",
        "\n",
        "#Save to CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"/content/trial6_test_outputs.csv\", index=False)\n",
        "print(\"\\n‚úÖ Responses saved to trial6_test_outputs.csv\")\n"
      ],
      "metadata": {
        "id": "blX2zQphU_DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQyO070zZo48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7S3Vy-rYZHQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL ANDROID APP BACKEND"
      ],
      "metadata": {
        "id": "PHZbot9cZtXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import os, threading\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "config = PeftConfig.from_pretrained(model_path)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "SAFETY_KEYWORDS = [\n",
        "    \"kill myself\", \"end my life\", \"suicide\", \"hurt myself\", \"self-harm\",\n",
        "    \"i want to die\", \"not worth living\", \"give up on life\"\n",
        "]\n",
        "\n",
        "SAFETY_RESPONSE = (\n",
        "    \"üíô I'm really sorry you're feeling this way. You're not alone. \"\n",
        "    \"Please seek help from a professional. You can contact Lifeline Australia at 13 11 14, \"\n",
        "    \"available 24/7 for support.\"\n",
        ")\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    user_input = data.get(\"message\", \"\")\n",
        "    mode = data.get(\"mode\", \"long\")  # 'short' or 'long'\n",
        "\n",
        "    if any(keyword in user_input.lower() for keyword in SAFETY_KEYWORDS):\n",
        "        return jsonify({\"response\": SAFETY_RESPONSE})\n",
        "\n",
        "    style_instruction = {\n",
        "        \"short\": \"Keep it brief, supportive, and to the point (under 100 words).\",\n",
        "        \"long\": \"Give a detailed and compassionate response with examples and encouragement (around 200 words).\"\n",
        "    }.get(mode, \"Give a helpful and supportive response.\")\n",
        "\n",
        "    prompt = (\n",
        "        f\"<s>[INST] You are a kind and supportive mental health assistant.\\n\"\n",
        "        f\"{style_instruction}\\nUser: {user_input}\\n[/INST]\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
        "\n",
        "    max_tokens = 400 if mode == \"long\" else 120\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = decoded.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in decoded else decoded.strip()\n",
        "\n",
        "    return jsonify({\"response\": response})\n",
        "\n",
        "\n",
        "ngrok.set_auth_token(os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "def run_flask():\n",
        "    app.run()\n",
        "\n",
        "threading.Thread(target=run_flask).start()\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL to access chatbot API: {public_url}\")\n"
      ],
      "metadata": {
        "id": "HlL97JHca_a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f uvicorn\n",
        "!pkill -f streamlit\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "vwq9MXLUdttx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CI_gyax9bFcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Web UI"
      ],
      "metadata": {
        "id": "OgtmgEkerxNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# --- Page Config ---\n",
        "st.set_page_config(page_title=\"üß† CounselChat\", page_icon=\"üí¨\", layout=\"centered\")\n",
        "\n",
        "# --- Model Path ---\n",
        "model_path = \"/content/drive/MyDrive/chatbot/counsel/checkpoints/trial6\"\n",
        "\n",
        "# --- Bits & Bytes Config ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# --- Load Model & Tokenizer ---\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_model():\n",
        "    config = PeftConfig.from_pretrained(model_path)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.base_model_name_or_path,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "with st.spinner(\"üîÑ Loading model...\"):\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "# --- Safety Filter ---\n",
        "SAFETY_KEYWORDS = [\n",
        "    \"kill myself\", \"end my life\", \"suicide\", \"hurt myself\", \"self-harm\",\n",
        "    \"i want to die\", \"not worth living\", \"give up on life\"\n",
        "]\n",
        "\n",
        "SAFETY_RESPONSE = (\n",
        "    \"üíô I'm really sorry you're feeling this way. You're not alone. \"\n",
        "    \"Please seek help from a professional. You can contact Lifeline Australia at 13 11 14, \"\n",
        "    \"available 24/7 for support.\"\n",
        ")\n",
        "\n",
        "# --- Chat History Container (Scrollable & Styled) ---\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "        .chat-container {\n",
        "            max-height: 500px;\n",
        "            overflow-y: auto;\n",
        "            padding-right: 10px;\n",
        "            margin-bottom: 15px;\n",
        "        }\n",
        "        .user-bubble {\n",
        "            background-color: #DCF8C6;\n",
        "            color: #000000;\n",
        "            padding: 10px;\n",
        "            border-radius: 15px;\n",
        "            text-align: right;\n",
        "            width: fit-content;\n",
        "            margin-left: auto;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "        .bot-bubble {\n",
        "            background-color: #F0F0F0;\n",
        "            color: #000000;\n",
        "            padding: 10px;\n",
        "            border-radius: 15px;\n",
        "            text-align: left;\n",
        "            width: fit-content;\n",
        "            margin-right: auto;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# --- Header ---\n",
        "st.markdown(\"\"\"\n",
        "    <div style='text-align: center;'>\n",
        "        <h1>üß† CounselChat</h1>\n",
        "        <p>Your empathetic mental health assistant</p>\n",
        "    </div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# --- Chat Form ---\n",
        "mode = st.radio(\"Select Response Style:\", options=[\"short\", \"long\"], horizontal=True)\n",
        "user_input = st.text_input(\"üí¨ Type your message:\")\n",
        "\n",
        "# --- Trigger Chat ---\n",
        "if st.button(\"Send\") and user_input:\n",
        "    with st.container():\n",
        "        st.markdown(\"<div class='chat-container'>\", unsafe_allow_html=True)\n",
        "\n",
        "        # Safety filter\n",
        "        if any(keyword in user_input.lower() for keyword in SAFETY_KEYWORDS):\n",
        "            st.markdown(f\"<div class='bot-bubble'>{SAFETY_RESPONSE}</div>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            with st.spinner(\"ü§ñ Thinking...\"):\n",
        "                style_instruction = {\n",
        "                    \"short\": \"Keep it brief, supportive, and to the point (under 100 words).\",\n",
        "                    \"long\": \"Give a detailed and compassionate response with examples and encouragement (around 200 words).\"\n",
        "                }.get(mode, \"Give a helpful and supportive response.\")\n",
        "\n",
        "                prompt = (\n",
        "                    f\"\"\"<s>[INST] You are a kind and supportive mental health assistant.\\n\"\"\"\n",
        "                    f\"\"\"{style_instruction}\\nUser: {user_input}\\n[/INST]\"\"\"\n",
        "                )\n",
        "\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
        "                max_tokens = 400 if mode == \"long\" else 120\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=max_tokens,\n",
        "                        temperature=0.6,\n",
        "                        top_p=0.9,\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        eos_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                response = decoded.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in decoded else decoded.strip()\n",
        "\n",
        "                # Show conversation\n",
        "                st.markdown(f\"<div class='user-bubble'>{user_input}</div>\", unsafe_allow_html=True)\n",
        "                st.markdown(f\"<div class='bot-bubble'>ü§ñ {response}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "'''\n",
        "#Save to app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "Q98Qz1jZqpmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "id": "VUO2XDp0o2dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit"
      ],
      "metadata": {
        "id": "PSlKTLuio3ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "metadata": {
        "id": "mxPwmZlGy71Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxvLAO3Ay-wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Summary For Counsel Chat + Mistral 7b model trials\n",
        "\n",
        "\n",
        "###Evaluation Summary\n",
        "\n",
        "| Trial | Dataset Used                            | Synthetic Data | Prompt Style                                                | LoRA Config (r/Œ±) | Epochs | BLEU   | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore (F1) | Notes                          |\n",
        "|-------|------------------------------------------|----------------|--------------------------------------------------------------|-------------------|--------|--------|----------|----------|----------|----------------|---------------------------------|\n",
        "| 1     | CounselChat (890)                        | ‚ùå No          | \"You are a professional mental health assistant...\"         | r=8 / Œ±=16        | 3      | 0.0203 | 0.2888   | 0.0429   | 0.1349   | 0.8311         | Baseline trial                 |\n",
        "| 2     | CounselChat (same)                       | ‚ùå No          | Same as Trial 1                                              | r=16 / Œ±=32       | 4      | 0.0173 | 0.2315   | 0.0334   | 0.122    | 0.8076         | Slight drop in scores          |\n",
        "| 3     | CounselChat + 200 synthetic              | ‚úÖ Yes         | Same as Trial 1                                              | r=16 / Œ±=32       | 4      | 0.0228 | 0.2984   | 0.0461   | 0.1387   | 0.8331         | Better than Trial 1 & 2        |\n",
        "| 4     | Smart-filtered CounselChat (2,116)       | ‚ùå No          | Same as Trial 1                                              | r=16 / Œ±=32       | 5      | 0.0222 | 0.2384   | 0.0387   | 0.1225   | 0.8144         | Fast mode, improved token len  |\n",
        "| 5     | Same as Trial 4                          | ‚ùå No          | \"How would you support someone who says...\" variation        | r=16 / Œ±=32       | 5      | 0.0243 | 0.3071   | 0.0416   | 0.1307   | 0.8344         | Most human-like tone           |\n",
        "| 6     | Merged CounselChat + Empathetic (cleaned)| ‚úÖ Yes (merged)| \"You are a helpful mental health assistant...\" (rotary-safe) | r=8 / Œ±=16        | 4      | 0.028  | 0.2967   | 0.054    | 0.1442   | 0.8328         | ‚úÖ Best overall on clean eval   |\n",
        "| 6‚òÖ    | Trial 6 on MentalChat16K (general test)  | ‚úÖ Yes         | Same as above                                                | r=8 / Œ±=16        | ‚Äî      | 1.91   | 31.90    | 5.00     | 13.57    | 83.33%          | ‚úÖ Best generalization ability  |\n"
      ],
      "metadata": {
        "id": "hxLJ-7qMAtjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b4dNynhaA4_K"
      }
    }
  ]
}