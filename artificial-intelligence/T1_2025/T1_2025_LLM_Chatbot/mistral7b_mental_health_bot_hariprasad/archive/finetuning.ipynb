{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05803648",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U transformers accelerate datasets peft bitsandbytes trl\n",
    "!pip install -U huggingface_hub\n",
    "pip install rouge-score bert-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ae845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aae655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path = \"datasets/empathetic_dialogues.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"].select(range(300))\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"instruction\"],\n",
    "        text_target=example[\"output\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "trials = [\n",
    "    {\"lr\": 2e-4, \"r\": 4, \"bs\": 2},\n",
    "    {\"lr\": 3e-4, \"r\": 4, \"bs\": 2},\n",
    "]\n",
    "\n",
    "for idx, trial in enumerate(trials):\n",
    "    print(f\"\\nStarting Trial #{idx+1} — LR: {trial['lr']}, LoRA r: {trial['r']}, BS: {trial['bs']}\")\n",
    "    out_dir = f\"hparam2_trials/trial_{idx+1}_lr{trial['lr']}_r{trial['r']}_bs{trial['bs']}\"\n",
    "\n",
    "    if os.path.exists(os.path.join(out_dir, \"final\")):\n",
    "        print(\"Skipping: Already exists\")\n",
    "        continue\n",
    "\n",
    "    trainer = None  \n",
    "\n",
    "    try:\n",
    "  \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            r=trial['r'],\n",
    "            lora_alpha=trial['r'] * 2,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=out_dir,\n",
    "            num_train_epochs=3,  \n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=trial['bs'],\n",
    "            learning_rate=trial['lr'],\n",
    "            logging_dir=os.path.join(out_dir, \"logs\"),\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        model.save_pretrained(os.path.join(out_dir, \"final\"))\n",
    "        tokenizer.save_pretrained(os.path.join(out_dir, \"final\"))\n",
    "        print(\"Trial complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial #{idx+1} failed: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        #memory\n",
    "        del model\n",
    "        if trainer:\n",
    "            del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3277c",
   "metadata": {},
   "source": [
    "2 trial models will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6644d",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "model_path = \"hparam2_trials/trial_1_lr0.0002_r4_bs2/final\"  \n",
    "data_path = \"datasets/empathetic_dialogues.jsonl\"\n",
    "eval_output = \"hparam2_trials/eval2_trial1_300sample.json\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"].select(range(300))\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "all_bleu, all_rouge1, all_rouge2, all_rougeL = [], [], [], []\n",
    "predictions, references = [], []\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "for item in tqdm(dataset):\n",
    "    prompt, ref = item[\"instruction\"], item[\"output\"]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(ref)\n",
    "\n",
    "    all_bleu.append(sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth_fn))\n",
    "    rouge_scores = scorer.score(ref, pred)\n",
    "    all_rouge1.append(rouge_scores[\"rouge1\"].fmeasure)\n",
    "    all_rouge2.append(rouge_scores[\"rouge2\"].fmeasure)\n",
    "    all_rougeL.append(rouge_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"Calculating BERTScore...\")\n",
    "P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "\n",
    "results = {\n",
    "    \"BLEU\": sum(all_bleu) / len(all_bleu),\n",
    "    \"ROUGE-1\": sum(all_rouge1) / len(all_rouge1),\n",
    "    \"ROUGE-2\": sum(all_rouge2) / len(all_rouge2),\n",
    "    \"ROUGE-L\": sum(all_rougeL) / len(all_rougeL),\n",
    "    \"BERTScore_P\": P.mean().item(),\n",
    "    \"BERTScore_R\": R.mean().item(),\n",
    "    \"BERTScore_F1\": F1.mean().item(),\n",
    "}\n",
    "\n",
    "with open(eval_output, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Scores saved to:\", eval_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "model_path = \"hparam2_trials/trial_2_lr0.0003_r4_bs2/final\" \n",
    "data_path = \"datasets/empathetic_dialogues.jsonl\"\n",
    "eval_output = \"hparam2_trials/eval2_trial2_300sample.json\"  \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"].select(range(300))\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "all_bleu, all_rouge1, all_rouge2, all_rougeL = [], [], [], []\n",
    "predictions, references = [], []\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "for item in tqdm(dataset):\n",
    "    prompt, ref = item[\"instruction\"], item[\"output\"]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(ref)\n",
    "\n",
    "    all_bleu.append(sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth_fn))\n",
    "    rouge_scores = scorer.score(ref, pred)\n",
    "    all_rouge1.append(rouge_scores[\"rouge1\"].fmeasure)\n",
    "    all_rouge2.append(rouge_scores[\"rouge2\"].fmeasure)\n",
    "    all_rougeL.append(rouge_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"Calculating BERTScore...\")\n",
    "P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "\n",
    "results = {\n",
    "    \"BLEU\": sum(all_bleu) / len(all_bleu),\n",
    "    \"ROUGE-1\": sum(all_rouge1) / len(all_rouge1),\n",
    "    \"ROUGE-2\": sum(all_rouge2) / len(all_rouge2),\n",
    "    \"ROUGE-L\": sum(all_rougeL) / len(all_rougeL),\n",
    "    \"BERTScore_P\": P.mean().item(),\n",
    "    \"BERTScore_R\": R.mean().item(),\n",
    "    \"BERTScore_F1\": F1.mean().item(),\n",
    "}\n",
    "\n",
    "with open(eval_output, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Scores saved to:\", eval_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c9ea3",
   "metadata": {},
   "source": [
    "Testing the best model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"hparam_trials/trial_1_lr0.0002_r4_bs2/final\" \n",
    "use_memory = True  \n",
    "temperature = 0.7\n",
    "max_tokens = 150\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def clean_response(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"\\nWelcome to the Mental Health Chatbot (Mistral-7B)\")\n",
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "history = \"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Goodbye! Take care.\")\n",
    "        break\n",
    "\n",
    "    if use_memory:\n",
    "        history += f\"\\nUser: {user_input}\"\n",
    "        prompt = f\"<s>[INST] You are a kind and supportive mental health assistant.{history} [/INST]\"\n",
    "    else:\n",
    "        prompt = f\"<s>[INST] You are a kind and supportive mental health assistant.\\n{user_input} [/INST]\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    reply = decoded.split(\"[/INST]\")[-1].strip()\n",
    "    reply = clean_response(reply)\n",
    "\n",
    "    print(f\"Mistral: {reply}\\n\")\n",
    "\n",
    "    if use_memory:\n",
    "        history += f\"\\nMistral: {reply}\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
