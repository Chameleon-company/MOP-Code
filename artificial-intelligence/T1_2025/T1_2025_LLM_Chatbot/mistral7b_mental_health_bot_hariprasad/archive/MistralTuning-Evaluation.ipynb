{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate datasets peft bitsandbytes trl --quiet\n",
    "!pip install -U huggingface_hub\n",
    "!pip install -q evaluate transformers bert-score\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/train-conv.csv\")\n",
    "\n",
    "# Drop empty/junk columns\n",
    "df = df[[\"utterance\", \"context\", \"prompt\"]]\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=[\"utterance\", \"context\", \"prompt\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df.to_csv(\"datasets/train-conv-cleaned.csv\", index=False)\n",
    "\n",
    "print(f\"Cleaned file saved with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6706c",
   "metadata": {},
   "source": [
    "Conversation with chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "df = pd.read_csv(\"datasets/train-conv-cleaned.csv\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Response generator function\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Start interactive chatbot session\n",
    "print(\"\\nWelcome to the Mistral 7B Empathetic Chatbot\")\n",
    "print(\"Type your message or 'sample' to use a dataset example.\")\n",
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    if user_input.lower() == \"sample\":\n",
    "        sample = df.sample(1).iloc[0]\n",
    "        situation = sample[\"prompt\"]\n",
    "        emotion = sample[\"context\"]\n",
    "        user_msg = sample[\"utterance\"]\n",
    "\n",
    "        print(f\"\\nPrompt: {situation}\")\n",
    "        print(f\"Emotion: {emotion}\")\n",
    "        print(f\"User Message: {user_msg}\")\n",
    "\n",
    "        formatted_prompt = f\"<s>[INST] You are a supportive mental health assistant.\\nEmotion: {emotion}\\nSituation: {situation}\\nMessage: {user_msg} [/INST]\"\n",
    "    else:\n",
    "        formatted_prompt = f\"<s>[INST] You are a supportive mental health assistant.\\n{user_input} [/INST]\"\n",
    "\n",
    "    reply = generate_response(formatted_prompt)\n",
    "    print(f\"\\nMistral: {reply}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcd5a3",
   "metadata": {},
   "source": [
    "JSONL conversion of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "df = pd.read_csv(\"datasets/train-conv-cleaned.csv\")\n",
    "\n",
    "# Drop NaNs and shuffle for randomness\n",
    "df = df.dropna().sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_subset = df.iloc[:1000]\n",
    "\n",
    "records = []\n",
    "for i in range(0, len(df_subset) - 1, 2):  # pair every 2 utterances\n",
    "    instruction = df_subset.iloc[i][\"utterance\"]\n",
    "    output = df_subset.iloc[i + 1][\"utterance\"]\n",
    "    records.append({\n",
    "        \"instruction\": instruction.strip(),\n",
    "        \"output\": output.strip()\n",
    "    })\n",
    "\n",
    "with open(\"empdiag_1000.jsonl\", \"w\") as f:\n",
    "    for r in records:\n",
    "        json.dump(r, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#shutil.copy(\"empdiag_1000.jsonl\", \"/content/drive/MyDrive/SIT782/datasets/empdiag_1000.jsonl\")\n",
    "\n",
    "print(\"Saved: empdiag_1000.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e66bc",
   "metadata": {},
   "source": [
    "Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f69112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/empdiag_1000.jsonl\", split=\"train\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result[\"labels\"] = tokenizer(output, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mistral-lora-chatbot\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"checkpoints/mistral-lora-chatbot\")\n",
    "tokenizer.save_pretrained(\"checkpoints/mistral-lora-chatbot\")\n",
    "\n",
    "print(\"Fine-tuning complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea88ef",
   "metadata": {},
   "source": [
    "Trial 1 – Baseline LoRA Fine-Tuning Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/empdiag_1000.jsonl\", split=\"train\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result[\"labels\"] = tokenizer(output, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "# LoRA Configuration – Trial 1\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training Arguments – Trial 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/mistral-lora-baseline\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"checkpoints/mistral-lora-baseline\")\n",
    "tokenizer.save_pretrained(\"checkpoints/mistral-lora-baseline\")\n",
    "\n",
    "print(\"Trial 1 (Baseline) complete! Model saved to Drive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e0d8d",
   "metadata": {},
   "source": [
    "Trial 2: Lower Learning Rate + More Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/empdiag_1000.jsonl\", split=\"train\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result[\"labels\"] = tokenizer(output, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "#LoRA config (Trial 2)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#TrainingArguments (Trial 2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/mistral-lora-lr1e5_ep3\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"checkpoints/mistral-lora-lr1e5_ep3\")\n",
    "tokenizer.save_pretrained(\"checkpoints/mistral-lora-lr1e5_ep3\")\n",
    "\n",
    "print(\"Trial 2 complete! Model saved to Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1eb98",
   "metadata": {},
   "source": [
    "Trial 3: Fast Learning with Higher LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30153505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"datasets/empdiag_1000.jsonl\", split=\"train\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_enable_fp32_cpu_offload=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n",
    "    output = f\" {example['output']}</s>\"\n",
    "    result = tokenizer(prompt + output, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    result[\"labels\"] = tokenizer(output, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# TrainingArguments – Trial 3\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/mistral-lora-lr5e5_ep2\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(\"checkpoints/mistral-lora-lr5e5_ep2\")\n",
    "tokenizer.save_pretrained(\"checkpoints/mistral-lora-lr5e5_ep2\")\n",
    "\n",
    "print(\"Trial 3 complete! Model saved to Drive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27246e3",
   "metadata": {},
   "source": [
    "Evaluation for trial 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b50fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_path = \"checkpoints/mistral-lora-baseline\"\n",
    "eval_data_path = \"datasets/empdiag_1000.jsonl\" \n",
    "\n",
    "eval_data = []\n",
    "with open(eval_data_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        eval_data.append((r[\"instruction\"], r[\"output\"]))\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Evaluating: Trial 1 — Baseline\")\n",
    "\n",
    "for instruction, expected in tqdm(eval_data[:250]):\n",
    "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(expected)\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
    "    \"ROUGE-1\": round(rouge_score[\"rouge1\"], 4),\n",
    "    \"ROUGE-2\": round(rouge_score[\"rouge2\"], 4),\n",
    "    \"ROUGE-L\": round(rouge_score[\"rougeL\"], 4),\n",
    "    \"BERTScore (F1)\": round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]), 4)\n",
    "}\n",
    "\n",
    "print(\"\\nTrial 1 Results:\")\n",
    "print(pd.DataFrame([results]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b360051",
   "metadata": {},
   "source": [
    "Evaluation trial 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "model_path = \"checkpoints/mistral-lora-lr1e5_ep3\"\n",
    "eval_data_path = \"datasets/empdiag_1000.jsonl\" \n",
    "\n",
    "eval_data = []\n",
    "with open(eval_data_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        eval_data.append((r[\"instruction\"], r[\"output\"]))\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Evaluating: Trial 2 — LR 1e-5, Epochs 3\")\n",
    "\n",
    "for instruction, expected in tqdm(eval_data[:250]):\n",
    "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(expected)\n",
    "\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "results = {\n",
    "    \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
    "    \"ROUGE-1\": round(rouge_score[\"rouge1\"], 4),\n",
    "    \"ROUGE-2\": round(rouge_score[\"rouge2\"], 4),\n",
    "    \"ROUGE-L\": round(rouge_score[\"rougeL\"], 4),\n",
    "    \"BERTScore (F1)\": round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]), 4)\n",
    "}\n",
    "\n",
    "print(\"\\nTrial 2 Results:\")\n",
    "print(pd.DataFrame([results]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297de3f4",
   "metadata": {},
   "source": [
    "Evaluation trial 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_path = \"checkpoints/mistral-lora-lr5e5_ep2\"\n",
    "eval_data_path = \"datasets/empdiag_1000.jsonl\"  \n",
    "\n",
    "eval_data = []\n",
    "with open(eval_data_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        r = json.loads(line)\n",
    "        eval_data.append((r[\"instruction\"], r[\"output\"]))\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Evaluating: Trial 3 — LR 5e-5, Epochs 2\")\n",
    "\n",
    "for instruction, expected in tqdm(eval_data[:250]):\n",
    "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(expected)\n",
    "\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "results = {\n",
    "    \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
    "    \"ROUGE-1\": round(rouge_score[\"rouge1\"], 4),\n",
    "    \"ROUGE-2\": round(rouge_score[\"rouge2\"], 4),\n",
    "    \"ROUGE-L\": round(rouge_score[\"rougeL\"], 4),\n",
    "    \"BERTScore (F1)\": round(sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]), 4)\n",
    "}\n",
    "\n",
    "print(\"\\nTrial 3 Results:\")\n",
    "print(pd.DataFrame([results]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
