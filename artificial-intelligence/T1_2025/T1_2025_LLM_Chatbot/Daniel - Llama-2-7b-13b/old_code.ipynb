{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b3c656",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Using GPU: NVIDIA GeForce RTX 3060\n",
      "â–¶ Loading JSON dataset from datasets/no_gpu_limit_500.json...\n",
      "â–¶ Using 500 examples for training\n",
      "â–¶ Loading llama-2-13b-chat in 4-bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:29<00:00,  9.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Applying LoRA adapters...\n",
      "trainable params: 6,553,600 || all params: 13,022,417,920 || trainable%: 0.0503\n",
      "â–¶ Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 2309.54 examples/s]\n",
      "C:\\Users\\lephu\\AppData\\Local\\Temp\\ipykernel_28352\\879848848.py:132: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Initializing Trainer...\n",
      "â–¶ Starting training with hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 1:38:21, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.861600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.770200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Saving LoRA model to trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500...\n",
      "â–¶ Merging LoRA weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Training complete. Model saved in trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500, merged in trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500_merged\n",
      "â–¶ Next steps: Evaluate using ROUGE-L, ROUGE-1, ROUGE-2, BERTScore, and BLEU metrics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "########################\n",
    "# CONFIG\n",
    "########################\n",
    "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # Replace with your Hugging Face token\n",
    "MODEL_NAME = \"llama-2-13b-chat\"\n",
    "REPO_ID = f\"meta-llama/{MODEL_NAME}-hf\"\n",
    "CACHE_DIR = \"D:/huggingface_cache\"\n",
    "DATASET_PATH = \"datasets/no_gpu_limit_500.json\" # colab_gpu_limited_100.json # no_gpu_limit_500\n",
    "N_SAMPLES = 500 #! change to 100 for colab_gpu_limited_100.json\n",
    "OUTPUT_DIR = f\"trained_models/rerun_old_version/{MODEL_NAME}-lora-output-{N_SAMPLES}\"\n",
    "FINAL_DIR = f\"trained_models/rerun_old_version/{MODEL_NAME}-lora-final-{N_SAMPLES}\"\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "########################\n",
    "# DEVICE MANAGEMENT\n",
    "########################\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"â–¶ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "        torch.cuda.empty_cache()\n",
    "        return device\n",
    "    else:\n",
    "        raise RuntimeError(\"GPU not available, but required for this setup.\")\n",
    "\n",
    "########################\n",
    "# DATA LOADING\n",
    "########################\n",
    "def load_data(file_path, n_samples=N_SAMPLES):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    dataset = {\n",
    "        \"question\": [item[\"question\"] for item in data],\n",
    "        \"answer\": [item[\"answer\"] for item in data]\n",
    "    }\n",
    "    ds = Dataset.from_dict(dataset)\n",
    "    if len(ds) < n_samples:\n",
    "        print(f\"â–¶ Warning: Dataset has {len(ds)} samples, less than requested {n_samples}\")\n",
    "        n_samples = len(ds)\n",
    "    return ds.shuffle(seed=42).select(range(n_samples))\n",
    "\n",
    "########################\n",
    "# TRAINING FUNCTION\n",
    "########################\n",
    "def train_model(data_path=DATASET_PATH, n_samples=N_SAMPLES):\n",
    "    device = set_device()\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "    print(f\"â–¶ Loading JSON dataset from {data_path}...\")\n",
    "    train_ds = load_data(data_path, n_samples=n_samples)\n",
    "    print(f\"â–¶ Using {len(train_ds)} examples for training\")\n",
    "\n",
    "    expected_columns = {\"question\", \"answer\"}\n",
    "    if not all(col in train_ds.column_names for col in expected_columns):\n",
    "        raise ValueError(f\"Dataset must contain {expected_columns}, but found {train_ds.column_names}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(REPO_ID, token=HF_TOKEN, cache_dir=CACHE_DIR)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(f\"â–¶ Loading {MODEL_NAME} in 4-bit...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        REPO_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    print(\"â–¶ Applying LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Enhanced prompt engineering for better fine-tuning\n",
    "        texts = [f\"[INST] {q} [/INST] {a}\" for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "        tokenized = tokenizer(texts, truncation=True, max_length=512, padding=\"max_length\")\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    print(\"â–¶ Tokenizing dataset...\")\n",
    "    tokenized_train = train_ds.map(tokenize_function, batched=True, remove_columns=train_ds.column_names)\n",
    "\n",
    "    # Updated training arguments for hyperparameter tuning\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,  # Increased epochs for better tuning\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8, #! chaneg to \"16\" to reduce memory usage\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=2e-5,  # Adjusted learning rate\n",
    "        warmup_steps=50,     # Added warmup for stability\n",
    "        weight_decay=0.01    # Added regularization\n",
    "    )\n",
    "\n",
    "    print(\"â–¶ Initializing Trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        tokenizer=tokenizer\n",
    "        # processing_class=tokenizer  # Replace tokenizer=tokenizer #! do it later\n",
    "    )\n",
    "\n",
    "    print(\"â–¶ Starting training with hyperparameter tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"â–¶ Saving LoRA model to {FINAL_DIR}...\")\n",
    "    model.save_pretrained(FINAL_DIR)\n",
    "    tokenizer.save_pretrained(FINAL_DIR)\n",
    "\n",
    "    print(f\"â–¶ Merging LoRA weights...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_dir = FINAL_DIR + \"_merged\"\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    tokenizer.save_pretrained(merged_dir)\n",
    "\n",
    "    print(f\"â–¶ Training complete. Model saved in {FINAL_DIR}, merged in {merged_dir}\")\n",
    "    print(\"â–¶ Next steps: Evaluate using ROUGE-L, ROUGE-1, ROUGE-2, BERTScore, and BLEU metrics\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(data_path=DATASET_PATH, n_samples=N_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31c65c",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "### Evaluate the model after train with out is an excel file. also summary in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Using GPU: NVIDIA GeForce RTX 3060\n",
      "â–¶ Loading fine-tuned model from trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Loading test dataset from datasets/test_data.json...\n",
      "â–¶ Evaluating on 10 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.27k/6.27k [00:00<00:00, 16.4MB/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.94k/5.94k [00:00<00:00, 9.31MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 6.78MB/s]                   \n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.34k/3.34k [00:00<00:00, 5.54MB/s]\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Generating model predictions...\n",
      "â–¶ Computing ROUGE scores...\n",
      "â–¶ Computing BLEU score...\n",
      "â–¶ Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lephu\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.92 seconds, 10.87 sentences/sec\n",
      "â–¶ Exporting evaluation report to evaluation_report_20250506_040845.csv...\n",
      "\n",
      "â–¶ Evaluation Summary:\n",
      "  ROUGE-1: 0.3709\n",
      "  ROUGE-2: 0.1083\n",
      "  ROUGE-L: 0.1874\n",
      "  BLEU: 0.0539\n",
      "  BERTScore_Precision: 0.8523\n",
      "  BERTScore_Recall: 0.8746\n",
      "  BERTScore_F1: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "from bert_score import score as bert_score\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "########################\n",
    "# CONFIG\n",
    "########################\n",
    "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # Replace with your Hugging Face token\n",
    "MODEL_NAME = \"llama-2-13b-chat\"\n",
    "MERGED_MODEL_DIR = f\"trained_models/rerun_old_version/{MODEL_NAME}-lora-final-500_merged\" #!!! Change to your merged model path\n",
    "CACHE_DIR = \"D:/huggingface_cache\"\n",
    "TEST_DATASET_PATH = \"datasets/test_data.json\"  # Replace with your test dataset path\n",
    "REPORT_OUTPUT = f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "# Evaluation settings\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "########################\n",
    "# DEVICE MANAGEMENT\n",
    "########################\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"â–¶ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "        torch.cuda.empty_cache()\n",
    "        return device\n",
    "    else:\n",
    "        print(\"â–¶ No GPU available, using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "########################\n",
    "# DATA LOADING\n",
    "########################\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    dataset = {\n",
    "        \"question\": [item[\"question\"] for item in data],\n",
    "        \"answer\": [item[\"answer\"] for item in data]\n",
    "    }\n",
    "    return Dataset.from_dict(dataset)\n",
    "\n",
    "########################\n",
    "# EVALUATION FUNCTION\n",
    "########################\n",
    "def evaluate_model(test_data_path=TEST_DATASET_PATH):\n",
    "    device = set_device()\n",
    "\n",
    "    # Load the merged fine-tuned model and tokenizer\n",
    "    print(f\"â–¶ Loading fine-tuned model from {MERGED_MODEL_DIR}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_DIR, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MERGED_MODEL_DIR,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load test dataset\n",
    "    print(f\"â–¶ Loading test dataset from {test_data_path}...\")\n",
    "    test_ds = load_test_data(test_data_path)\n",
    "    print(f\"â–¶ Evaluating on {len(test_ds)} examples\")\n",
    "\n",
    "    # Prepare metrics\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"â–¶ Generating model predictions...\")\n",
    "    for example in test_ds:\n",
    "        input_text = f\"[INST] {example['question']} [/INST]\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip()\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"answer\"])\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    print(\"â–¶ Computing ROUGE scores...\")\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    print(\"â–¶ Computing BLEU score...\")\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    print(\"â–¶ Computing BERTScore...\")\n",
    "    P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "    bertscore_results = {\n",
    "        \"precision\": P.mean().item(),\n",
    "        \"recall\": R.mean().item(),\n",
    "        \"f1\": F1.mean().item()\n",
    "    }\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"BERTScore_Precision\": bertscore_results[\"precision\"],\n",
    "        \"BERTScore_Recall\": bertscore_results[\"recall\"],\n",
    "        \"BERTScore_F1\": bertscore_results[\"f1\"]\n",
    "    }\n",
    "\n",
    "    # Export to CSV\n",
    "    print(f\"â–¶ Exporting evaluation report to {REPORT_OUTPUT}...\")\n",
    "    results_df = pd.DataFrame([results])\n",
    "    results_df.to_csv(REPORT_OUTPUT, index=False)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nâ–¶ Evaluation Summary:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual test dataset path\n",
    "    evaluate_model(test_data_path=\"datasets/test_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2282e",
   "metadata": {},
   "source": [
    "# Chat Bot\n",
    "### Run and chat with the bot in terminal\n",
    "- Answer start with: \"Bot:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a027b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Using GPU: NVIDIA GeForce RTX 3060\n",
      "â–¶ Loading model and tokenizer from trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lephu\\anaconda3\\envs\\mop-ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Sorry to hear that you're feeling stressed and overwhelmed. It's completely normal to feel this way at times, and there are many things you can do to manage your stress and improve your well-being. Here are some suggestions that might help:\n",
      "\n",
      "1. Take a break: Sometimes, we just need a break from our daily routines to recharge and refocus. Take some time off from work or other responsibilities to relax and do something you enjoy.\n",
      "2. Practice self-care: Make sure you're taking care of yourself physically, emotionally, and mentally. This can include things like getting enough sleep, eating healthy foods, exercising regularly, and finding time for activities that bring you joy and relaxation.\n",
      "3. Identify the sources of your stress: Is there something specific that's causing you stress? If so, see if you can address it or find ways to manage it. Sometimes, just identifying the source of our stress can help us feel more in control.\n",
      "4. Seek support: Talk to a trusted friend, family member, or mental health professional about how you're feeling. Sometimes, just talking through our emotions with someone who cares about us can help us feel better.\n",
      "5. Try relaxation techniques: There are many relaxation techniques that can help us manage stress, such as deep breathing, progressive muscle relaxation, and mindfulness meditation. Experiment with different techniques to find what works best for you.\n",
      "6. Set boundaries: Learn to say no to things that drain your energy and say yes to things that nourish your mind, body, and spirit. Setting healthy boundaries can help us feel more balanced and in control.\n",
      "7. Practice gratitude: Make a conscious effort to focus on the things you're grateful for, rather than dwelling on the things that are causing you stress. Gratitude can help shift our perspective and improve our mood.\n",
      "\n",
      "Remember, it's okay to feel overwhelmed and stressed at times. It's important to take care of yourself and seek support when you need it. With time and effort, you can find ways to manage your stress and improve your well-being.\n",
      "Bot: I'm glad to hear that you're feeling better! It's important to take care of yourself and prioritize your well-being, especially when you're feeling overwhelmed or struggling with difficult emotions. Remember that it's okay to take breaks and focus on your own needs. You don't have to do everything at once, and it's important to pace yourself and prioritize your own well-being.\n",
      "\n",
      "If you ever need any more help or support, don't hesitate to reach out. I'm here to help in any way I can. Remember that you're not alone, and there are resources available to support you. Take care of yourself and keep moving forward, one step at a time.\n",
      "Bot: You're welcome! I'm glad to hear that you're feeling better. If you ever need any more help or have any other questions, don't hesitate to ask. Remember to take care of yourself and prioritize your well-being. It's important to take breaks and practice self-care when dealing with stressful situations. Sending you positive vibes and a big virtual hug! ðŸ¤—\n",
      "â–¶ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"trained_models/rerun_old_version/llama-2-13b-chat-lora-final-500_merged\"  # Path to your fine-tuned 13B model\n",
    "CACHE_DIR = \"D:/huggingface_cache\"\n",
    "MAX_LENGTH = 768  # Adjust based on memory constraints (256, 512, 765, 1024, 1280, 1536, ...)\n",
    "\n",
    "# Device setup\n",
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"â–¶ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "        torch.cuda.empty_cache()\n",
    "        return device\n",
    "    else:\n",
    "        print(\"â–¶ No GPU available, using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    device = set_device()\n",
    "    print(f\"â–¶ Loading model and tokenizer from {MODEL_PATH}...\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            cache_dir=CACHE_DIR,\n",
    "            local_files_only=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=CACHE_DIR,\n",
    "            local_files_only=True,\n",
    "            load_in_4bit=True,  # Use 4-bit quantization\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        ).to(device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "# Generate a response from the model\n",
    "def generate_response(model, tokenizer, device, user_input):\n",
    "    # Format the input as per the model's expected prompt\n",
    "    input_text = f\"[INST] {user_input} [/INST]\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=MAX_LENGTH,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Sorry, I encountered an error while generating a response.\"\n",
    "\n",
    "# Main chatbot loop\n",
    "def run_chatbot():\n",
    "    model, tokenizer, device = load_model_and_tokenizer()\n",
    "    print(\"â–¶ Chatbot is ready! Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"â–¶ Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = generate_response(model, tokenizer, device, user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mop-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
