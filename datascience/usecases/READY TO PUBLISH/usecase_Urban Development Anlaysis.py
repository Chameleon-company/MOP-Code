# -*- coding: utf-8 -*-
"""(CW) - MOP 2023 T3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1CENaWuUBQzfiKRhl0hVdyv8Uon6iJp
"""

from IPython.core.display import HTML

HTML(
    """
    <style>

    .usecase-title, .usecase-duration, .usecase-section-header {
      padding-left: 15px;
      padding-bottom: 10px;
      padding-top: 10px;
      padding-right: 15px;
      background-color: #0f9295;
      color: #fff;
      }

    .usecase-title {
      font-size: 1.7em;
      font-weight: bold;
      }

    .usecase-authors, .usecase-level, .usecase-skill {
      padding-left: 15px;
      padding-bottom: 7px;
      padding-top: 7px;
      background-color: #baeaeb;
      font-size: 1.4em;
      color: #121212;
      }

    .usecase-level-skill {
      display: flex;
      }

    .usecase-level, .usecase-skill {
      width: 50%;
      }

    .usecase-duration, .usecase-skill {
      text-align: right;
      padding-right: 15px;
      padding-bottom: 8px;
      font-size: 1.4em;
      }

    .usecase-section-header {
      font-weight: bold;
      font-size: 1.5em;
      }

    .usecase-subsection-header, .usecase-subsection-blurb {
      font-weight: bold;
      font-size: 1.2em;
      color: #121212;
      }

    .usecase-subsection-blurb {
      font-size: 1em;
      font-style: italic;
      }

    </style>
    """)

"""<div class="usecase-title"><b>Urban Development Analysis</b></div>

<div class="usecase-authors"><b>Authored by: </b> WING SUM WONG/JENNIFER YAU</div>

<div class="usecase-duration"><b>Duration:</b> 90 mins</div>

<div class="usecase-level-skill">
    <div class="usecase-level"><b>Level: </b>Intermediate</div>
    <div class="usecase-skill"><b>Pre-requisite Skills: </b>Python and basic Machine Learning</div>
</div>

<div class="usecase-section-header">Scenario</div>

As a council, I would like to find out the growth and development patterns in the Melbourne city. I would like to find out:

*   Which area of the city has had the most growth in recent years?
*   Which type of developments (residential, hotel, school)?
*   What are the trends in housing developments?
*   Forecast the future development trends.

<div class="usecase-section-header">What this use case will teach you</div>

At the end of this use case, you will:

*   have gained experienced in fetching datasets using APIs
*   understand what CLUE small areas represent
*   become familiar with data extraction, including data collection, consolidation, and initial assessment
*   have gained experience in performing data pre-processing, such as data cleaning and integration
*   have learnt how to visualize observations using mapping visualization techniques
*   become experienced in linear regression analysis
*   have experience with time series model analysis
*   have explored the development growth and forecasted future trends of CLUE small areas

<div class="usecase-section-header">Introduction</div>

To effectively discern trends in building and other commercial spaces development over past years, we adopted three datasets. These datasets encompass various types of data related to developments in dwellings and commercial spaces, including car space.

**DATASETS LIST**

1.   development-activity-monitor
2.   Residential dwellings
3.   Building information

Development Activity Monitor

This dataset tracks the establishment of new commercial and residential spaces, along with their geographic points in the CLUE areas, from 2002 to 2023. Property ID 5 served as unique identifier, known as the Quinary ID. For records lacking a property ID 5, Property ID 4 will be used, and so on, down to property ID.

Residential dwellings

Covering the period from 2002 to 2022, this dataset contains data on dwelling developments in the CLUE areas. It includes residential apartments, house/townhouses, and student accommodations. The Property ID is used as the key identifier in this dataset, acting as a second-tier individual key to uniquely identify properties within the boundary that have the same base property ID.

Building Information

This dataset includes data on accessibility, commercial and residential properties, location and the years of construction completion or refurbishment, ranged from 2002 to 2022. Property ID is used as the key identifier.

# **Part 1  - Import modules & fetch data from Open Data Melbourne**
"""

import requests
import pandas as pd
import os


# define function to fetch data from website using API

def fetch_data(base_url, dataset, api_key, num_records = 99, offset = 0):
    all_records = []
    max_offset = 9900  # Maximum number of requests

    while True:
        # maximum limit check
        if offset > max_offset:
            break

        # Create API request URL
        filters = f'{dataset}/records?limit={num_records}&offset={offset}'
        url = f'{base_url}{filters}&api_key={api_key}'

        # Start request
        try:
            result = requests.get(url, timeout=10)
            result.raise_for_status()
            records = result.json().get('results')
        except requests.exceptions.RequestException as e:
            raise Exception(f"API request failed: {e}")
        if records is None:
            break
        all_records.extend(records)
        if len(records) < num_records:
            break

        # next cycle offset
        offset += num_records

    # DataFrame all data
    df = pd.DataFrame(all_records)
    return df

API_KEY = os.environ.get('MELBOURNE_API_KEY', input("Please enter your API key: "))
BASE_URL = 'https://data.melbourne.vic.gov.au/api/explore/v2.1/catalog/datasets/'

"""# **Part 2 - Data Cleansing**

## **Dataset:** Development activity monitor

### Load dataset
"""

# get data
DEV_ACTIVITY = 'development-activity-monitor'

df_development = fetch_data(BASE_URL, DEV_ACTIVITY, API_KEY)

df_development.head()

df_development.isna().sum()

"""### Data cleaning

#### Fill 'property_id_5' and remove unwanted columns
"""

# save as a new dataset to preserve the original data

df_dev_filled = df_development.copy()

# fill 'property_id_5' with values from 'property_id_4', then 'property_id_3', then 'property_id_2', then 'property_id'

df_dev_filled['property_id_5'] = df_dev_filled['property_id_5'].fillna(df_dev_filled['property_id_4']).fillna(df_dev_filled['property_id_3']).fillna(df_dev_filled['property_id_2']).fillna(df_dev_filled['property_id'])

# drop 'property_id_4' to 'property_id', and other unwanted columns

df_dev_filled.drop(['property_id_4', 'property_id_3', 'property_id_2', 'property_id', 'data_format', 'development_key', 'town_planning_application'], axis = 1, inplace = True)

len(df_dev_filled)

df_dev_filled.head()

df_dev_filled.isna().sum()

# rename 'property_id_5'
df_dev_filled.rename(columns = {'property_id_5': 'property_id'}, inplace = True)

# check the dataframe
df_dev_filled.head()

"""#### Fix duplicated value in 'property_id_5' if any"""

# identify duplicates in 'property_id' if any
dup = df_dev_filled[df_dev_filled.duplicated(subset = 'property_id', keep = False)]

# sort these duplicates by 'property_id' and then by 'year_completed' in descending order
dup_sorted = dup.sort_values(by = ['property_id', 'year_completed'], ascending = [True, False])

# display rows where 'property_id' has duplicates
print(dup_sorted)

len(dup_sorted)

# drop duplicates and those are under construnction
# keeping the first (which has the latest year) if years are different

df_dev_clean = df_dev_filled.drop(dup_sorted[dup_sorted.duplicated(subset = 'property_id', keep = 'first')].index)

print(df_dev_clean)

## double check if there still have duplicated values in 'property_id'

dup_2 = df_dev_clean[df_dev_clean.duplicated(subset = 'property_id', keep = False)]

print(dup_2)

len(dup_2)

df_dev_clean.isna().sum()

"""## **Dataset:** Residential dwellings

### Load dataset
"""

# get data
RES_DWELLINGS = 'residential-dwellings'

df_residential = fetch_data(BASE_URL, RES_DWELLINGS, API_KEY)

df_residential.head()

df_residential.isna().sum()   # check if there are nulls

len(df_residential)

"""### Data cleaning"""

# drop 'base_property_id'
df_res_dropped = df_residential.drop(['base_property_id'], axis = 1)

df_res_dropped.head()

# duplicate ID checking on 'property_id'
# if any, keep row with the latest year

# sort dataframe by 'property_id' then 'census_year' (latest year first for each group of 'property_id')
df_res_sorted = df_res_dropped.sort_values(by = ['property_id', 'census_year'], ascending = [True, False])

# drop duplicates in 'property_id', keeping only the row with the latest year in 'census_year'
df_res_cleaned = df_res_sorted.drop_duplicates(subset = 'property_id', keep = 'first')

print(df_res_cleaned)

df_res_cleaned.isna().sum()

"""## **Dataset:** Building information

### Load dataset
"""

# get data
BUILDING_INFO = 'buildings-with-name-age-size-accessibility-and-bicycle-facilities'

df_bldg_info = fetch_data(BASE_URL, BUILDING_INFO, API_KEY)

df_bldg_info.head()

df_bldg_info.isna().sum()   # check if there are nulls

len(df_bldg_info)

"""### Data cleaning"""

# drop 'base_property_id' for 'df_bldg_info' dataset
df_bldg_dropped = df_bldg_info.drop(['base_property_id', 'building_name'], axis = 1)

df_bldg_dropped.head()

# duplicate ID checking on 'property_id'
# if any, keep row with the latest year

# sort dataframe by 'property_id' then 'census_year' (latest year first for each group of 'property_id')
df_bldg_sorted = df_bldg_dropped.sort_values(by = ['property_id', 'census_year'], ascending = [True, False])

# drop duplicates in 'property_id', keeping only the row with the latest year in 'census_year'
df_bldg_cleaned = df_bldg_sorted.drop_duplicates(subset = 'property_id', keep = 'first')

print(df_bldg_cleaned)

df_bldg_cleaned.isna().sum()

"""**LIMITATION**


Upon examining the three datasets, it was observed that only the ‘Development Activity Monitor’ dataset includes data on building status. To mitigate the risk of dealing with massive amounts of missing data in the dataset and to simplify the analysis, this study will focus only on the properties whose construction has been completed. Records labelled as under construction or lacking information on the completion year will be excluded.

# **Part 3 - Merging**

### Merge 'Development activity monitor' and 'Residential dwellings'
"""

list(df_dev_clean)

list(df_res_cleaned)

## skip duplicated columns while merging:
## 'block_id', 'clue_small_area', 'street_address', 'longitude', 'latitude' and 'geopoint'

## dev_exclude = ['street_address', 'longitude', 'latitude', 'geopoint']
## df_dev_selected = df_dev_clean.drop(dev_exclude, axis = 1)

## df_res_selected = df_res_cleaned[['census_year', 'property_id', 'building_address', 'dwelling_type', 'dwelling_number', 'longitude', 'latitude', 'location']]

## ---

df_res_selected = df_res_cleaned[['census_year', 'property_id', 'dwelling_type', 'dwelling_number']]

# inner join

inner_dev_res = pd.merge(df_dev_clean, df_res_selected, on = 'property_id', how = 'inner')

len(inner_dev_res)

# outer join

outer_dev_res = pd.merge(df_dev_clean, df_res_selected, on = 'property_id', how = 'outer')

len(outer_dev_res)

# right join

right_dev_res = pd.merge(df_dev_clean, df_res_selected, on = 'property_id', how = 'right')

len(right_dev_res)

# left join

left_dev_res = pd.merge(df_dev_clean, df_res_selected, on = 'property_id', how = 'left')
len(left_dev_res)

list(left_dev_res.columns)

left_dev_res.head()

left_dev_res.isna().sum()

"""### Merge 'Development & Residential' with 'Building information'"""

list(df_bldg_cleaned)

# skip column 'floors_above' from dataframe 'left_dev_res'

dev_res_exclude = ['floors_above']
left_dev_res = left_dev_res.drop(dev_res_exclude, axis = 1)

# skip columns from 'df_bldg_cleaned':
# 'census_year', 'block_id', 'street_address', 'clue_small_area', 'longitude', 'latitude' and 'location'

bldg_exclude = ['census_year', 'block_id', 'street_address', 'clue_small_area', 'longitude', 'latitude', 'location']
df_bldg_selected = df_bldg_cleaned.drop(bldg_exclude, axis = 1)

# left join

left_dev_res_bldg = pd.merge(left_dev_res, df_bldg_selected, on = 'property_id', how = 'left')

len(left_dev_res_bldg)

list(left_dev_res_bldg.columns)

left_dev_res_bldg.head()

left_dev_res_bldg.isna().sum()   # check if there are nulls

"""# **Part 4 - Observations**

## CUBE Small Area Trends in Building and Space Constructions

**Aggregation calculation**
"""

# total number of buildings / spaces labelled as 'completed', group by CLUE small area

completed_bldg = left_dev_res_bldg[left_dev_res_bldg['status'] == 'COMPLETED']

selected = ['resi_dwellings', 'student_apartments', 'hotels_serviced_apartments', 'childcare_places', 'car_spaces', 'bike_spaces', 'bicycle_spaces']

grouped_CLUE = completed_bldg.groupby(['clue_small_area', 'year_completed'])[selected].sum(numeric_only = True)

grouped_CLUE

# sum across the rows to get the total sum of the selected columns for each clue_small_area and completed year
grouped_CLUE['total'] = grouped_CLUE.sum(axis = 1)

# reset the index to have `clue_small_area` and `year_completed` as columns
grouped_reset = grouped_CLUE.reset_index()

# select only `clue_small_area` and `year_completed`, and `total` columns for final output
year_bldg_sum = grouped_reset[['clue_small_area', 'year_completed', 'total']]

year_bldg_sum

"""### Bar Chart"""

# draw the bar chart

import matplotlib.pyplot as plt
import seaborn as sns

# set style of seaborn
sns.set(style = "whitegrid")

# create a bar plot
plt.figure(figsize = (20, 6))
barplot = sns.barplot(x = 'year_completed', y = 'total', hue = 'clue_small_area', data = year_bldg_sum)

# Set the plot title and labels
plt.title('Sum of completed buildings and spaces by CLUE small area and completed year')
plt.xlabel('Completed year')
plt.ylabel('Total')
plt.legend(loc = 'upper left')

# Display the plot
plt.show()

"""**Interactive Bar Chart**"""

!pip install plotly

import plotly.express as px

# interactive bar plot
fig_barChart = px.bar(year_bldg_sum, x = 'year_completed', y = 'total', color = 'clue_small_area',
                      title = 'Sum of completed buildings and spaces by CLUE small area and completed year',
                      labels = {'total': 'Total', 'year': 'Year', 'clue_small_area': 'CLUE small area'})

fig_barChart.update_layout(
    legend = dict(
        yanchor = "top",
        y = 1,
        xanchor = "left",
        x = 0
        ),
    barmode = 'group',
    xaxis_tickangle = -45
    )

# show the plot
fig_barChart.show()

"""### Choropleth Map"""

# get CLUE small area data
SMALL_AREA_FOR_CLUE = 'small-areas-for-census-of-land-use-and-employment-clue'

df_sm_area_for_CLUE = fetch_data(BASE_URL, SMALL_AREA_FOR_CLUE, API_KEY)

df_sm_area_for_CLUE

# Commented out IPython magic to ensure Python compatibility.
!pip install qeds

import geopandas as gpd
import qeds
from shapely.geometry import MultiPolygon


# apply qeds style for plotting if available
try:
    qeds.themes.mpl_style();
except:
    # if qeds style does not work, use a default matplotlib style
    plt.style.use('seaborn')

# %matplotlib inline

# defiine function to transform the data into a multipolygon
def create_multipolygon(row):

    # extract coordinates from row
    coordinates = row['geometry']['coordinates']

    # create a multipolygon object
    return MultiPolygon(coordinates)

# apply function to transform geo_shape to geometry
df_sm_area_for_CLUE["geometry"] = df_sm_area_for_CLUE["geo_shape"].apply(create_multipolygon)

# create GeoDataFrame
gdf_shapes = gpd.GeoDataFrame(df_sm_area_for_CLUE, geometry = 'geometry')

gdf_shapes.head()

"""**Map of CLUE small area**"""

# plotting the map

fig_CLUE, gax = plt.subplots(figsize = (8, 8))

gdf_shapes.plot(ax = gax, edgecolor = 'white', color = 'grey', alpha = 0.8)

gax.set_xlabel('Longitude')
gax.set_ylabel('Latitude')

plt.show()

# sum of each type of buildings / spaces for each CLUE area across all years

CLUE_bldg_totals = completed_bldg.groupby(['clue_small_area'])[selected].sum()

CLUE_bldg_totals['total'] = CLUE_bldg_totals.sum(axis = 1)

CLUE_bldg_totals

# merge with 'CLUE_bldg_totals' dataframe

geo_total = gdf_shapes.merge(CLUE_bldg_totals, left_on = 'featurenam', right_on = 'clue_small_area')

geo_total

"""**Choropleth Map**"""

fig_choroplethmap, ax = plt.subplots(1, 1, figsize = (8, 8))

geo_total.plot(
    column = 'total',
    ax = ax,
    legend = True,
    cmap = 'Blues',
    vmin = 300,
    vmax = 71000,
    legend_kwds = {
        'label': "Total Developments in CLUE Small Area",
        'orientation': "horizontal"
        },
    linewidth = 0.2,
    edgecolor = 'black')

# remove the axis
ax.set_axis_off()

ax.set_title("Choropleth Map of Developments in CLUE small areas")

plt.show()

"""**Interactive Choropleth Map**"""

!pip install folium

import folium
from folium.features import GeoJsonTooltip

# convert the GeoDataFrame to GeoJSON format
geojson = geo_total.to_json()

# use central point to create a base map
map_center = [geo_total['geo_point_2d'][0]['lat'], geo_total['geo_point_2d'][0]['lon']]
I_cMap = folium.Map(location = map_center, zoom_start = 13)

# define the fields to include in the tooltip
tooltip_fields = ['featurenam', 'total']

# add the choropleth layer
I_choropleth = folium.Choropleth(
    geo_data = geojson,
    name = 'choropleth',
    data = geo_total,
    columns = ['featurenam', 'total'],
    key_on = 'feature.properties.featurenam',
    fill_color = 'PuBuGn',
    fill_opacity = 0.7,
    line_opacity = 1,
    legend_name = 'Total Developments in CLUE small areas'
).add_to(I_cMap)

# create a GeoJsonTooltip
tooltip = GeoJsonTooltip(
    fields = tooltip_fields,
    aliases = ['CLUE area: ', 'Total Developments: '],  # These are the labels for the tooltip
    localize = True,
    sticky = False,
    labels = True,
    style = """
      background-color: #F0EFEF;
      border: 2px solid black;
      border-radius: 3px;
      box-shadow: 3px;
      """,
    max_width=800,
)

# add the tooltip to the choropleth layer
I_choropleth.geojson.add_child(tooltip)

# add a layer control
folium.LayerControl().add_to(I_cMap)

# display the map
I_cMap

"""### Treemap"""

# melting the dataframe into a long format suitable for the treemap
CLUE_totals_long = geo_total.melt(
    id_vars = ['featurenam'],
    value_vars = ['resi_dwellings',
                  'student_apartments',
                  'hotels_serviced_apartments',
                  'childcare_places',
                  'car_spaces',
                  'bike_spaces',
                  'bicycle_spaces'],
    var_name = 'DevelopmentType',
    value_name = 'Count'
    )

# creating the treemap
fig_tree = px.treemap(CLUE_totals_long,
                      path = ['featurenam', 'DevelopmentType'],
                      values = 'Count'
                      )

fig_tree.update_layout(
    margin = dict(t = 50, l = 25, r = 25, b = 25),
    title = {'text': "Treemap of Developments by CLUE areas",
             'y': 0.93,
             'x': 0.5,
             'xanchor': 'center',
             'yanchor': 'top'
             }
    )

# Show the figure
fig_tree.show()

CLUE_totals_long.head()

"""## Growth Patterns in Housing Developments

**Aggregation Calculation**
"""

# sum of residential dwellings (residential dwellings & student apartments) for each CLUE area across all years

grouped_CLUE_res = grouped_CLUE.groupby(['clue_small_area', 'year_completed'])[['resi_dwellings', 'student_apartments']].sum()

grouped_CLUE_res['total'] = grouped_CLUE_res.sum(axis = 1)

grouped_CLUE_res

"""### Line Chart"""

# create the figure and specify the size
fig_line = plt.subplots(figsize = (18, 6))

# plot the data
sns.lineplot(x = 'year_completed', y = 'total', hue = 'clue_small_area', data = grouped_CLUE_res)

# set theme
sns.set_theme(style = 'whitegrid')

# move legend
plt.legend(loc = 'upper left')

plt.show()

"""**Interactive Line Chart**"""

# create interactive line plot
fig_line_interactive = px.line(grouped_CLUE_res.reset_index(), x = 'year_completed', y = 'total', color = 'clue_small_area', title = 'Trends in Housing Developments Over the Years')

# improve layout
fig_line_interactive.update_layout(
    title = {
        'text': 'Trends in Housing Developments Over the Years',
        'y': 0.9,
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top'
        },
    hovermode = 'closest',  # show tooltip for the closest point
    legend_title_text = 'clue_small_area'
)

# Show the plot
fig_line_interactive.show()

# from google.colab import drive
# drive.mount('/content/gdrive')

# df_dev_filled.to_csv('/content/gdrive/My Drive/Colab Notebooks/df_dev_filled.csv', index = False)

"""# Part 5 - Forecast

## Linear Regression
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.metrics import r2_score

left_dev_res_bldg

"""### Data Clean and Pre-processing"""

### Data Clean and Pre-processing

# handle missing column for "year_completed" with median
left_dev_res_bldg['year_completed'].fillna(left_dev_res_bldg['year_completed'].median(), inplace=True)

# handle missing column for "construction_year" with median
left_dev_res_bldg['construction_year'].fillna(left_dev_res_bldg['construction_year'].median(), inplace=True)

# handle missing column for "refurbished_year" with median
left_dev_res_bldg['refurbished_year'].fillna(left_dev_res_bldg['refurbished_year'].median(), inplace=True)

# drop column "census_year", "dweliing_type", "dweliing_number", "has_showers" , "number_of_floors_above_ground "
left_dev_res_bldg.drop('census_year', axis=1, inplace=True)

left_dev_res_bldg.drop('street_address', axis=1, inplace=True)
left_dev_res_bldg.drop('geopoint', axis=1, inplace=True)
left_dev_res_bldg.drop('dwelling_type', axis=1, inplace=True)
left_dev_res_bldg.drop('dwelling_number', axis=1, inplace=True)
left_dev_res_bldg.drop('has_showers', axis=1, inplace=True)
left_dev_res_bldg.drop('property_id', axis=1, inplace=True)

left_dev_res_bldg.drop('number_of_floors_above_ground', axis=1, inplace=True)
left_dev_res_bldg.drop('predominant_space_use', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_type', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_type_description', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_rating', axis=1, inplace=True)
left_dev_res_bldg.drop('bicycle_spaces', axis=1, inplace=True)

# put column "clue_small_area", "status" into label encoding
from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()
left_dev_res_bldg['clue_small_area'] = labelencoder.fit_transform(left_dev_res_bldg['clue_small_area'])
left_dev_res_bldg['status'] = labelencoder.fit_transform(left_dev_res_bldg['status'])

print(left_dev_res_bldg.describe())

left_dev_res_bldg

"""### Model Training

##### Target variable : resi_dwellings
"""

# group by 'year_completed' and calculate the sum of 'resi_dwellings'
yearly_sum = left_dev_res_bldg.groupby('year_completed')['resi_dwellings'].sum().reset_index()

# rename the column
yearly_sum.rename(columns={'resi_dwellings': 'total_resi_dwellings'}, inplace=True)

# use 'year_completed' and 'total_resi_dwellings'
X = yearly_sum[['year_completed']]
y = yearly_sum['total_resi_dwellings']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is a bit high in the result. In result shows that the prediction for residential dwelling unit will be increasing in the 2024- 2028. The R-squared show negative value which indicate it is a non linear relationship.

#### Target variable : studio_dwe
"""

# group by 'year_completed' and calculate the sum of 'stuido_dwe'
yearly_sum2 = left_dev_res_bldg.groupby('year_completed')['studio_dwe'].sum().reset_index()

# rename the column
yearly_sum2.rename(columns={'studio_dwe': 'total_studio_dwe'}, inplace=True)

# use 'year_completed' and 'total_studio_dwe'
X = yearly_sum2[['year_completed']]
y = yearly_sum2['total_studio_dwe']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is a bit high in the result.
In result shows that the prediction for studio will be increasing in the 2024- 2028.
The R-squared show negative value which indicate it is a non linear relationship.

#### Target variable : one_bdrm_dwe
"""

# group by 'year_completed' and calculate the sum of 'one_bdrm_dwe'
yearly_sum3 = left_dev_res_bldg.groupby('year_completed')['one_bdrm_dwe'].sum().reset_index()

# rename the column
yearly_sum3.rename(columns={'one_bdrm_dwe': 'total_one_bdrm_dwe'}, inplace=True)

# use 'year_completed' and 'total_one_bdrm_dwe'
X = yearly_sum3[['year_completed']]
y = yearly_sum3['total_one_bdrm_dwe']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is a extremely high in the result.
In result shows that the prediction for one bedroom will be increasing in the 2024- 2028.
The R-squared show negative value which indicate it is a non linear relationship.

#### Target variable: two_bdrm_dwe
"""

# group by 'year_completed' and calculate the sum of 'two_bdrm_dwe'
yearly_sum4 = left_dev_res_bldg.groupby('year_completed')['two_bdrm_dwe'].sum().reset_index()

# rename the column
yearly_sum4.rename(columns={'two_bdrm_dwe': 'total_two_bdrm_dwe'}, inplace=True)

# use 'year_completed' and 'total_two_bdrm_dwe'
X = yearly_sum4[['year_completed']]
y = yearly_sum4['total_two_bdrm_dwe']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is a bit high in the result.
In result shows that the prediction for two bedroom will be increasing in the 2024- 2028.
The R-squared show negative value which indicate it is a non linear relationship.

#### Target variable: three_bdrm_dwe
"""

# group by 'year_completed' and calculate the sum of 'three_bdrm_dwe'
yearly_sum5 = left_dev_res_bldg.groupby('year_completed')['three_bdrm_dwe'].sum().reset_index()

# rename the column
yearly_sum5.rename(columns={'three_bdrm_dwe': 'total_three_bdrm_dwe'}, inplace=True)

# use 'year_completed' and 'total_three_bdrm_dwe'
X = yearly_sum5[['year_completed']]
y = yearly_sum5['total_three_bdrm_dwe']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is a bit high in the result.
In result shows that the prediction for three bedroom will be increasing in the 2024- 2028.
The R-squared show negative value which indicate it is a non linear relationship.

#### Target variable: student_apartments
"""

# group by 'year_completed' and calculate the sum of 'student_apartments'
yearly_sum6 = left_dev_res_bldg.groupby('year_completed')['student_apartments'].sum().reset_index()

# rename the column
yearly_sum6.rename(columns={'student_apartments': 'total_student_apartments'}, inplace=True)

# use 'year_completed' and 'total_student_apartments'
X = yearly_sum6[['year_completed']]
y = yearly_sum6['total_student_apartments']

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions
y_pred = model.predict(X_test)

# evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# predict future values
future_years = pd.DataFrame({'year_completed': np.arange(2024, 2028)})
future_predictions = model.predict(future_years)
print(future_predictions)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print(f"R-squared: {r_squared}")

"""The MSE is extremly high compare with previous result.
In result shows that the prediction for student apartment will be increasing in the 2024- 2028.
The R-squared show negative value which indicate it is a non linear relationship.

In conclusion, the linear model is not suitable for this dataset, therefore we are going to use another model for forecast.

## Time Series Model

### Model Training
"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.preprocessing import StandardScaler
import seaborn as sns

left_dev_res_bldg

print(left_dev_res_bldg.columns)

left_dev_res_bldg.isna().sum()

# handle missing column for "year_completed" with median
left_dev_res_bldg['year_completed'].fillna(left_dev_res_bldg['year_completed'].median(), inplace=True)

# handle missing column for "construction_year" with median
left_dev_res_bldg['construction_year'].fillna(left_dev_res_bldg['construction_year'].median(), inplace=True)

# handle missing column for "refurbished_year" with median
left_dev_res_bldg['refurbished_year'].fillna(left_dev_res_bldg['refurbished_year'].median(), inplace=True)

# drop column "census_year", "dweliing_type", "dweliing_number", "has_showers" , "number_of_floors_above_ground "
left_dev_res_bldg.drop('census_year', axis=1, inplace=True)

left_dev_res_bldg.drop('street_address', axis=1, inplace=True)

left_dev_res_bldg.drop('geopoint', axis=1, inplace=True)

left_dev_res_bldg.drop('dwelling_type', axis=1, inplace=True)

left_dev_res_bldg.drop('dwelling_number', axis=1, inplace=True)

left_dev_res_bldg.drop('has_showers', axis=1, inplace=True)

left_dev_res_bldg.drop('property_id', axis=1, inplace=True)

left_dev_res_bldg.drop('number_of_floors_above_ground', axis=1, inplace=True)
left_dev_res_bldg.drop('predominant_space_use', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_type', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_type_description', axis=1, inplace=True)
left_dev_res_bldg.drop('accessibility_rating', axis=1, inplace=True)
left_dev_res_bldg.drop('bicycle_spaces', axis=1, inplace=True)

# put column "clue_small_area", "status" into label encoding
from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()
left_dev_res_bldg['clue_small_area'] = labelencoder.fit_transform(left_dev_res_bldg['clue_small_area'])
left_dev_res_bldg['status'] = labelencoder.fit_transform(left_dev_res_bldg['status'])

# Print unique values in the 'clue_small_area' column
print(left_dev_res_bldg['clue_small_area'].unique())

"""['North Melbourne' 'West Melbourne (Residential)' 'Carlton'
 'Melbourne (Remainder)' 'Melbourne (CBD)' 'West Melbourne (Industrial)'
 'South Yarra' 'Kensington' 'Docklands' 'Parkville' 'East Melbourne'
 'Southbank' 'Port Melbourne']

### Target variable : resi_dwellings

#### Clue small area : 0 - Carlton
"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 0

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 1 - Docklands"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 1

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 2 - East Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 2

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 3 - Kensington"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 3

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 4 - Melbourne (CBD)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 4

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 5 - Melbourne (Remainder)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 5

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 6 - North Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 6

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 7 - Parkville"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 7

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 8 - Port Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 8

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 9 - South Yarra"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 9

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 10 - Southbank"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 10

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 11 - West Melbourne(Industrial)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 11

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 12 - West Melbourne(Residential)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'resi_dwellings'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['resi_dwellings'].sum().reset_index()

# Select a specific area
specific_area = 12

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'resi_dwellings' by year
annual_data = area_data['resi_dwellings'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""### Target variable : studio_dwe

#### Clue small area : 0 - Carlton
"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 0

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 1 - Docklands"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 1

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 2 - East Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 2

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 3 - Kensington"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 3

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 4 - Melbourne (CBD)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 4

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 5 - Melbourne (Remainder)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 5

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 6 - North Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 6

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 7 - Parkville"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 7

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 8 - Pot Melbourne"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 8

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 9 - South Yarra"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 9

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 10 - Southbank"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 10

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 11 - West Melbourne(Industrial)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 11

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""#### Clue small area : 12 - West Melbourne(Residential)"""

# Convert 'year_completed' to datetime format for grouping by year
left_dev_res_bldg['year_completed'] = pd.to_datetime(left_dev_res_bldg['year_completed'], format='%Y')

# Group by 'clue_small_area' and 'year_completed', then sum 'studio_dwe'
grouped_data = left_dev_res_bldg.groupby(['clue_small_area', 'year_completed'])['studio_dwe'].sum().reset_index()

# Select a specific area
specific_area = 12

# Filter data for the specific area
area_data = grouped_data[grouped_data['clue_small_area'] == specific_area]

# Convert 'year_completed' to datetime and set as index
area_data['year_completed'] = pd.to_datetime(area_data['year_completed'], format='%Y')
area_data.set_index('year_completed', inplace=True)

# Aggregate 'studio_dwe' by year
annual_data = area_data['studio_dwe'].resample('A').sum()

# Fit the ARIMA model
model = ARIMA(annual_data, order=(0, 1, 0))
model_fit = model.fit()

# Forecast for future years
forecast = model_fit.forecast(steps=5)
print(forecast)

"""## Visualization"""

import folium
import pandas as pd
import ipywidgets as widgets
from ipyleaflet import Map, Marker, MarkerCluster
from IPython.display import display

ln

ln.rename(columns={'Forecast ': 'Forecast'}, inplace=True)

import pandas as pd
import matplotlib.pyplot as plt

ln.rename(columns=lambda x: x.strip(), inplace=True)

# Filter the data for the year 2024
year_to_plot = 2024
ln_filtered = ln[ln['Year'] == year_to_plot]

# Aggregate forecast data by area for the filtered year
aggregated_data = ln_filtered.groupby('area')['Forecast'].sum()

# Plotting
plt.figure(figsize=(12, 6))
aggregated_data.plot(kind='bar')
plt.title(f'Total Forecast Data by Area for the Year {year_to_plot}')
plt.xlabel('Area')
plt.ylabel('Total Forecast')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# Filter the data for the year 2025
year_to_plot = 2025
ln_filtered = ln[ln['Year'] == year_to_plot]

# Aggregate forecast data by area for the filtered year
aggregated_data = ln_filtered.groupby('area')['Forecast'].sum()

# Plotting
plt.figure(figsize=(12, 6))
aggregated_data.plot(kind='bar')
plt.title(f'Total Forecast Data by Area for the Year {year_to_plot}')
plt.xlabel('Area')
plt.ylabel('Total Forecast')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# Filter the data for the year 2026
year_to_plot = 2026
ln_filtered = ln[ln['Year'] == year_to_plot]

# Aggregate forecast data by area for the filtered year
aggregated_data = ln_filtered.groupby('area')['Forecast'].sum()

# Plotting
plt.figure(figsize=(12, 6))
aggregated_data.plot(kind='bar')
plt.title(f'Total Forecast Data by Area for the Year {year_to_plot}')
plt.xlabel('Area')
plt.ylabel('Total Forecast')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# Filter the data for the year 2027
year_to_plot = 2027
ln_filtered = ln[ln['Year'] == year_to_plot]

# Aggregate forecast data by area for the filtered year
aggregated_data = ln_filtered.groupby('area')['Forecast'].sum()

# Plotting
plt.figure(figsize=(12, 6))
aggregated_data.plot(kind='bar')
plt.title(f'Total Forecast Data by Area for the Year {year_to_plot}')
plt.xlabel('Area')
plt.ylabel('Total Forecast')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

# Filter the data for the year 2028
year_to_plot = 2028
ln_filtered = ln[ln['Year'] == year_to_plot]

# Aggregate forecast data by area for the filtered year
aggregated_data = ln_filtered.groupby('area')['Forecast'].sum()

# Plotting
plt.figure(figsize=(12, 6))
aggregated_data.plot(kind='bar')
plt.title(f'Total Forecast Data by Area for the Year {year_to_plot}')
plt.xlabel('Area')
plt.ylabel('Total Forecast')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.show()

"""In result, we can find that for the upcoming five years forecast 2024-2028 Melbourne (CBD) will be the most fastest growth within the area.

# Conclusion

This analysis of the urban development covers the period from 2002 to 2023 and includes only completed developments. Consequently, this scope may not fully capture the comprehensive trends necessary to forecast future developments in the CLUE areas accurately. To enhance the predictive accuracy of the future urban development trends, it would be beneficial to incorporate additional datasets into the model training process. This could include more recent data or projections that consider ongoing and planned developments.
"""