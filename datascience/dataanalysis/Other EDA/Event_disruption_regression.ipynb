{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# from jupyter_dash import JupyterDash\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Investigating the Pedestrian Sensor Data.\n",
    "### Pedestrian sensor data - basic analysis.\n",
    "\n",
    "This next dataset is pulled from the Melbourne Open Playground. The code block in the next cell needs to be uncommented to extract the data and copy it into a csv file that gets stored on your local drive.\n",
    "\n",
    "This section of code will see whether the data in this dataset can be used to make predictions of the pedestrian counts.\n",
    "\n",
    "Further sections will introduce new datasets and see if the extra information can help to make better, more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get Sensor count history data\n",
    "# def sensor_count():\n",
    "#     client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "#     sensor_data_id = \"b2ak-trbp\"\n",
    "#     results = client.get(sensor_data_id, limit=5000000)\n",
    "#     df = pd.DataFrame.from_records(results)\n",
    "#     df = df[['year', 'month', 'mdate', 'day', 'time', 'sensor_id', 'sensor_name', 'hourly_counts']]\n",
    "#     return df\n",
    "\n",
    "# sensor_history = sensor_count()\n",
    "\n",
    "# sensor_history.to_csv('sensor_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_history = pd.read_csv('sensor_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function grabs the location (longitude and latitude) of the pedestrian sensors\n",
    "def sensor_location():\n",
    "    client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "    sensor_location_data_id = \"h57g-5234\"\n",
    "    results = client.get(sensor_location_data_id)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    sensor_location = df[[\"sensor_id\", \"sensor_description\", \"latitude\", \"longitude\"]]\n",
    "    sensor_location.columns = [\"Sensor ID\", \"Sensor Description\", \"lat\", \"lon\"]\n",
    "    sensor_location[\"lat\"] = sensor_location[\"lat\"].apply(lambda x: float(x))\n",
    "    sensor_location[\"lon\"] = sensor_location[\"lon\"].apply(lambda x: float(x))\n",
    "    return sensor_location\n",
    "\n",
    "sensor_location = sensor_location()\n",
    "sensor_location['sensor_id'] = sensor_location['Sensor ID'].astype(int)\n",
    "sensor_location = sensor_location.drop(['Sensor ID', 'Sensor Description'], axis=1)\n",
    "sensor_history = sensor_history.merge(sensor_location, on=('sensor_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do some exploration of the dataset we just imported.\n",
    "print(sensor_history.info())\n",
    "print(\"\")\n",
    "print(sensor_history.head())\n",
    "print(\"\")\n",
    "print(sensor_history.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a quick linear regression to see how well we can model the relationships\n",
    "#contained within the pedestrian sensor network data.\n",
    "x = sensor_history.drop(columns='hourly_counts')\n",
    "y = sensor_history.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "X = X.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Adding new datasets:\n",
    "### Add a new dataset: climate microsensors.\n",
    "\n",
    "We didn't get a great result from the previous model. The score it output is the 'R-squared' score. These scores range from 0 to 1, with 1 being a perfect score and 0 being the worst possible score.\n",
    "\n",
    "Let's see if we can improve on this score by adding other datasets.\n",
    "\n",
    "This next dataset is also pulled from the Melbourne Open Playground. The code block in the next cell needs to be uncommented to extract the data and copy it into a csv file that gets stored on your local drive.\n",
    "\n",
    "The dataset is based on climate microsensors in Melbourne's CBD. For this analysis, I am only trying to get an idea of what the climate is like in Melbourne's city as a whole, not going into the detail of each sensor location. So I am only grabbing the data from one sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get Sensor count history data\n",
    "# def micro_count():\n",
    "#     client = Socrata('data.melbourne.vic.gov.au', 'nlPM0PQJSjzCsbVqntjPvjB1f', None)\n",
    "#     micro_data_id = \"u4vh-84j8\"\n",
    "#     results = client.get(micro_data_id, limit=4000000)\n",
    "#     if results:\n",
    "#         df = pd.DataFrame.from_records(results)\n",
    "#     return df\n",
    "\n",
    "# micro_history = micro_count()\n",
    "\n",
    "# micro_history.to_csv('micro_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_history = pd.read_csv('micro_history.csv')\n",
    "\n",
    "micro_history = micro_history[(micro_history.sensor_id == '5a') | (micro_history.sensor_id == '5b') |\n",
    "                             (micro_history.sensor_id == '5c') | (micro_history.sensor_id == '0a') |\n",
    "                             (micro_history.sensor_id == '0b') | (micro_history.sensor_id == '6')]\n",
    "\n",
    "micro_history = micro_history[(micro_history.site_id == 1003) | (micro_history.site_id == 1009)]\n",
    "\n",
    "micro_history = micro_history.drop(['id', 'gateway_hub_id', 'type', 'units'], axis=1)\n",
    "\n",
    "micro_history.loc[micro_history.sensor_id == '5a', 'temp'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '5b', 'humidity'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '5c', 'pressure'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '0a', 'part_2p5'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '0b', 'part_10'] = micro_history.value\n",
    "micro_history.loc[micro_history.sensor_id == '6', 'wind'] = micro_history.value\n",
    "\n",
    "micro_history.local_time = pd.to_datetime(micro_history.local_time, format='%Y-%m-%d')\n",
    "micro_history['year'] = micro_history.local_time.dt.year\n",
    "micro_history['month'] = micro_history.local_time.dt.month_name()\n",
    "micro_history['mdate'] = micro_history.local_time.dt.day\n",
    "micro_history['time'] = micro_history.local_time.dt.hour\n",
    "\n",
    "micro_history = micro_history.drop(['site_id', 'sensor_id', 'value', 'local_time'], axis=1)\n",
    "micro_history = micro_history.groupby(by=['year', 'month', 'mdate', 'time']).max()\n",
    "\n",
    "ped_climate = sensor_history.merge(micro_history, on=('year', 'month', 'mdate', 'time'), how='inner')\n",
    "\n",
    "x = ped_climate.drop(columns='hourly_counts')\n",
    "y = ped_climate.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "X = X.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a different dataset: school and public holidays:\n",
    "\n",
    "Adding in the climate data resulted in an imrpovement of the R-squared score. Let's see if we can find other datasets to add in to the model and get that score even higher. The next dataset was one that was created manually - by looking up the details online, then entering them into a csv.\n",
    "You will need to have this csv downloaded into your local directory for this to work.\n",
    "\n",
    "This dataset has details of which dates are public holidays and which are school holidays. This could have an impact on how many people are walking around, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_holidays = pd.read_csv('vic_holidays.csv')\n",
    "\n",
    "ped_holidays = sensor_history.merge(vic_holidays, on=('year', 'month', 'mdate'), how='left')\n",
    "ped_holidays =  ped_holidays.fillna(0)\n",
    "\n",
    "x = ped_holidays.drop(columns='hourly_counts')\n",
    "y = ped_holidays.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a little look at this dataset.\n",
    "ped_holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then do the prediction.\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Covid-19?:\n",
    "\n",
    "The holiday dataset only had a small, but consistent, positive effect on the scoring. Let's keep looking. The next dataset was found on the internet. You will need to have this csv downloaded into your local directory for this to work.\n",
    "\n",
    "It contains statistics about covid-19, including historical data. It is expected that these numbers could have a reasonably large impact on the numbers of pedestrians.\n",
    "\n",
    "**Source:** https://govtstats.covid19nearme.com.au/data/all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = pd.read_csv('covid_data.csv')\n",
    "\n",
    "covid_data = covid_data[['DATE', 'VIC_CASES_LOCAL_LAST_24H', 'VIC_CASES_ACTIVE', \n",
    "                    'VIC_CASES_LOCAL_LAST_7D', 'VIC_CASES_OVERSEAS_ACQUIRED_LAST_24H', 'VIC_CASES_OVERSEAS_ACQUIRED_LAST_7D',\n",
    "                    'VIC_CASES_UNDER_INVESTIGATION_LAST_24H', 'VIC_CASES_UNDER_INVESTIGATION_LAST_7D',\n",
    "                    'VIC_TESTS_LAST_7D', 'VIC_TESTS_PER_100K_LAST_7D']]\n",
    "\n",
    "covid_data.fillna(0)\n",
    "\n",
    "covid_data.DATE = pd.to_datetime(covid_data.DATE, format='%Y-%m-%d')\n",
    "\n",
    "covid_data['year'] = covid_data.DATE.dt.year\n",
    "covid_data['month'] = covid_data.DATE.dt.month_name()\n",
    "covid_data['mdate'] = covid_data.DATE.dt.day\n",
    "covid_data['mon'] = covid_data.DATE.dt.month\n",
    "\n",
    "sensor_covid = sensor_history.merge(covid_data, on=('year', 'month', 'mdate'), how='inner')\n",
    "\n",
    "x = sensor_covid.drop(columns='hourly_counts')\n",
    "y = sensor_covid.hourly_counts\n",
    "\n",
    "x_day = pd.get_dummies(x.day)\n",
    "x_month = pd.get_dummies(x.month)\n",
    "x_sensor = pd.get_dummies(x.sensor_name)\n",
    "\n",
    "x_drop = x.drop(['month', 'day', 'sensor_name', 'sensor_id', 'mon', 'DATE'], axis=1)\n",
    "X = pd.concat([x_drop, x_day, x_month, x_sensor],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the newly created dataset:\n",
    "print(sensor_covid.head())\n",
    "print(\"\")\n",
    "print(sensor_covid.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then do the prediction.\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Making the final dataset:\n",
    "\n",
    "The Covid data also seems to have a small, positive impact on the scoring. What if we add all of these datasets together? Will the sum of the parts be greater, or will the different datasets just confuse the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = ped_climate.merge(sensor_covid, on=('year', 'month', 'mdate', 'day', 'time', 'sensor_id'\n",
    "                                            , 'sensor_name', 'hourly_counts'), how='inner')\n",
    "\n",
    "merged = merge_1.merge(ped_holidays, on=('year', 'month', 'mdate', 'day', 'time', 'sensor_id'\n",
    "                                            , 'sensor_name', 'hourly_counts'), how='inner')\n",
    "\n",
    "merge_days = pd.get_dummies(merged.day)\n",
    "merge_months = pd.get_dummies(merged.month)\n",
    "merge_sensor = pd.get_dummies(merged.sensor_name)\n",
    "merge_drop = merged.drop(['month', 'day', 'sensor_name', 'sensor_id', 'DATE'], axis=1)\n",
    "\n",
    "merged_final = pd.concat([merge_drop, merge_days, merge_months, merge_sensor],axis=1)\n",
    "\n",
    "X = merged_final.drop(columns='hourly_counts')\n",
    "y = merged_final.hourly_counts\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "#And then do the prediction.\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The R-squared score is: \", LR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating the predictive models:\n",
    "\n",
    "Ok, so now we have some datasets and we have had a look at their individual impacts - before finding that they are stronger when combined together. We have a better understanding of how different events effect the number of pedestrians going past different sensors.\n",
    "\n",
    "But are we using the best model? There are other alternatives such as Decision Tree regressors, Random Forest regressors, Support Vector Machine regressors and even Deep Learning regression models.\n",
    "\n",
    "However, Support Vector Machine regressors can take a long time to run when the data has many dimensions, and Deep Learning models are also resource intensive. Below we will limit ourselves to adding in a Decision Tree regressor and a Random Forest regressor. Even these can take a long time to run, but the results will hopefully be worth it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "LR = LinearRegression(fit_intercept=False)\n",
    "LR.fit(X_train, y_train)\n",
    "print(\"The basic Linear Regression R-squared score: \", LR.score(X_test, y_test))\n",
    "\n",
    "#Decision Tree Regressor\n",
    "DT = DecisionTreeRegressor(max_depth = 75)\n",
    "DT.fit(X_train, y_train)\n",
    "print(\"The Decision Tree regressor's R-squared score: \", DT.score(X_test, y_test))\n",
    "\n",
    "#Random Forest Regressor\n",
    "RFR = RandomForestRegressor(n_estimators=150, max_depth=100, n_jobs= -1, max_features=100)\n",
    "RFR.fit(X_train, y_train)\n",
    "print(\"The Random Forest regressor's R-squared score: ', RFR.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interacting with our predictive models:\n",
    "\n",
    "The R-squared scores we have managed to create now are much better, with the Decision Tree being a huge jump over the basic Linear Regression, and the Random Forest being even better again.\n",
    "\n",
    "So now we have these cool models, we need a really easy, intuitive way to investigate them. For that, we build an interactive interface using Plotly Dash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(external_stylesheets=[dbc.themes.SOLAR])\n",
    "\n",
    "fig = px.scatter_mapbox(merged_final, lat=merged_final.lat, lon=merged_final.lon\n",
    "                        , zoom = 12.5\n",
    "                       , size = merged_final.hourly_counts)\n",
    "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_center_lon=144.96\n",
    "                      , mapbox_center_lat = -37.81)\n",
    "fig.update_layout(margin={\"r\":5,\"t\":5,\"l\":5,\"b\":5})\n",
    "\n",
    "app.layout = html.Div(id='parent', children=[ #main Div\n",
    "    html.Div(id='header', children=[ #header Div\n",
    "        \n",
    "        html.Div([ #calender selector Div\n",
    "            html.P('Choose a date for analysis:'),\n",
    "            dcc.DatePickerSingle(\n",
    "                id = 'selector_date',\n",
    "                month_format = 'MMMM Y',\n",
    "                calendar_orientation = 'horizontal',\n",
    "                placeholder = 'Select a date',\n",
    "                date = date(2021, 6, 21),\n",
    "                display_format = 'DD/MM/YYYY')\n",
    "        ],\n",
    "        style={'width': '15%', 'display': 'inline-block', 'verticalAlign': 'top', 'padding': '20px 20px 20px 20px'}),\n",
    "\n",
    "        html.Div([ #hour slider Div\n",
    "            html.P('Select the hour of the day:'),\n",
    "            dcc.Slider(\n",
    "                id='selector_hour',\n",
    "                min=0,\n",
    "                max=23,\n",
    "                step=1,\n",
    "                value=0,\n",
    "                marks={0: 'midnight', 3: '3am', 6: '6am', 9: '9am', 12: 'midday',\n",
    "                       15: '3pm', 18: '6pm', 21: '9pm'}\n",
    "        )], style={'width': '85%', 'display': 'inline-block', 'textAlign':'left', 'verticalAlign': 'top', 'padding': '20px 20px 20px 20px'}),\n",
    "    ]), #end of 'header' Div\n",
    "\n",
    "    html.Div([ #map and various selectors Div\n",
    "        \n",
    "            html.Div([ #various selectors Div\n",
    "\n",
    "                html.Hr(),\n",
    "                html.P('Temperature:'),\n",
    "                dcc.Slider(\n",
    "                    id='selector_temp',\n",
    "                    min=0,\n",
    "                    max=50,\n",
    "                    value=25,\n",
    "                    marks = {0: '0C', 10: '10C', 20: '20C', 30: '30C', 40: '40C', 50: '50C'}\n",
    "                    ),\n",
    "                html.P('Humidity:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_humid',\n",
    "                    min=0,\n",
    "                    max=100,\n",
    "                    value=0,\n",
    "                    marks = {0: '0%', 25: '25%', 50: '50%', 75: '75%', 100: '100%'}\n",
    "                    ),\n",
    "                html.P('Wind speed:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_wind',\n",
    "                    min=0,\n",
    "                    max=100,\n",
    "                    value=0,\n",
    "                    marks = {0: 'Calm', 20: '20km/h', 40: '40km/h', 60: '60km/h', 80: '80km/h', 100: '100km/h'}\n",
    "                    ),\n",
    "                html.P('Air pressure:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_pressure',\n",
    "                    min=975,\n",
    "                    max=1050,\n",
    "                    value=975,\n",
    "                    marks = {975: '975hPa', 1000: '1000hPa', 1025: '1025hPa', 1050: '1050hPa'}\n",
    "                    ),\n",
    "                html.P('Particulate concentration 2.5 microns:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_part2p5',\n",
    "                    min=0,\n",
    "                    max=500,\n",
    "                    value=0,\n",
    "                    marks = {0: '0', 100: '100', 200: '200', 300: '300', 400: '400', 500: '500'}\n",
    "                    ),\n",
    "                html.P('Particulate concentration 10 microns:'),\n",
    "                 dcc.Slider(\n",
    "                    id='selector_part10',\n",
    "                    min=0,\n",
    "                    max=1000,\n",
    "                    value=0,\n",
    "                    marks = {0: '0', 250: '200', 500: '500', 750: '750', 1000: '1000'}\n",
    "                    ),\n",
    "                html.Hr(),\n",
    "                html.P('Holiday type:'),\n",
    "                dcc.RadioItems(id='selector_holiday', \n",
    "                   options=[\n",
    "                       {'label': 'School holiday ', 'value': 'SCH'},\n",
    "                       {'label': 'Public holiday ', 'value': 'PUB'},\n",
    "                       {'label': 'Both ', 'value': 'BOTH'},\n",
    "                       {'label': 'Neither ', 'value': 'NONE'}\n",
    "                   ],\n",
    "                   value='NONE'\n",
    "                ),\n",
    "                html.Hr(),\n",
    "\n",
    "                html.P('Covid cases under investigation in the previous 7 days:'),\n",
    "                dcc.Input(id='selector_covid', type='number', min=0, max=10000, step=100, value=0),\n",
    "\n",
    "            ],\n",
    "            style={'height': '49%', 'width': '49%', 'display': 'inline-block', 'padding': '20px 20px 20px 20px'}), #various selectors Div\n",
    "\n",
    "            html.Div(id = 'right_panel', children=[ #format and place the right side panel\n",
    "\n",
    "            html.Div(id='map', children=[ #map div\n",
    "                    dcc.Graph(id = 'world_map', figure = fig)\n",
    "            ],\n",
    "                    style={'textAlign':'center', 'verticalAlign': 'top', 'display': 'inline-block', 'padding': '0px 20px 20px 50px'}), #map selector Div\n",
    "            html.Br(),\n",
    "            html.Hr(),\n",
    "            html.P('Select which predictor model to use:'),\n",
    "            html.Div(id='predictor', children=[ #prediction model div\n",
    "                    dcc.Dropdown(id = 'selector_model', #pick the predictive model to use\n",
    "                        options = [\n",
    "                            {'label':'Linear Regression', 'value':'LR' },\n",
    "                            {'label': 'Decision Tree Regression', 'value':'DT'},\n",
    "                            {'label': 'Random Forest Regression', 'value':'RFR'},\n",
    "                        ],\n",
    "                        value = 'LR'),                        \n",
    "            ],\n",
    "                    style={'textAlign':'center', 'width': '35%', 'display': 'inline-block'}), \n",
    "\n",
    "        ], style={'textAlign':'center', 'width': '49%', 'display': 'inline-block'})\n",
    "    ]) #end of 'map and various selectors' Div\n",
    "]) #end of 'parent' Div\n",
    "\n",
    "@app.callback(Output(component_id='world_map', component_property='figure'),\n",
    "            Input(component_id='selector_temp', component_property='value'),\n",
    "            Input(component_id='selector_humid', component_property='value'),\n",
    "            Input(component_id='selector_wind', component_property='value'),\n",
    "            Input(component_id='selector_part2p5', component_property='value'),\n",
    "            Input(component_id='selector_part10', component_property='value'),\n",
    "            Input(component_id='selector_pressure', component_property='value'),\n",
    "            Input(component_id='selector_holiday', component_property='value'),\n",
    "            Input(component_id='selector_covid', component_property='value'),\n",
    "            Input(component_id='selector_date', component_property='date'),\n",
    "            Input(component_id='selector_hour', component_property='value'),\n",
    "            Input(component_id='selector_model', component_property='value'))\n",
    "\n",
    "def selectors(temp, humid, wind, part_2p5, part_10, pressure, holiday, covid, indate, time, model):\n",
    "\n",
    "    date_object = date.fromisoformat(indate)\n",
    "    year = date_object.year\n",
    "    mon = date_object.month\n",
    "    mdate = date_object.day\n",
    "\n",
    "    scenario = merged_final[(merged_final.year == year) & (merged_final.mdate == mdate)\n",
    "                    & (merged_final.mon == mon) & (merged_final.time == time)]\n",
    "\n",
    "    scenario.school_hol = 0\n",
    "    scenario.pub_hol = 0\n",
    "    \n",
    "    if holiday == 'BOTH':\n",
    "        scenario.school_hol = 1\n",
    "        scenario.pub_hol = 1\n",
    "    if holiday == 'SCH':\n",
    "        scenario.school_hol = 1\n",
    "    if holiday == 'PUB':\n",
    "        scenario.pub_hol = 1\n",
    "    \n",
    "    scenario.temp = temp\n",
    "    scenario.humidity = humid\n",
    "    scenario.wind = wind\n",
    "    scenario.time = time\n",
    "    scenario.pressure = pressure\n",
    "    scenario.part_2p5 = part_2p5\n",
    "    scenario.part_10 = part_10\n",
    "    scenario.VIC_CASES_UNDER_INVESTIGATION_LAST_7D = covid\n",
    "   \n",
    "    if model == 'LR':\n",
    "        guess = LR.predict(scenario.drop(columns='hourly_counts'))\n",
    "    if model == 'DT':\n",
    "        guess = DT.predict(scenario.drop(columns='hourly_counts'))\n",
    "    if model == 'RFR':\n",
    "        guess = RFR.predict(scenario.drop(columns='hourly_counts'))\n",
    "    \n",
    "    scenario.guess = guess.astype(int)\n",
    "\n",
    "    fig = px.scatter_mapbox(scenario, lat=scenario.lat, size = scenario.guess, lon=scenario.lon)\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\", uirevision='scenario')\n",
    "    fig.update_layout(margin={\"r\":5,\"t\":5,\"l\":5,\"b\":5})\n",
    "\n",
    "    return fig\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. An awesome, fun way to interact with our predictive models!\n",
    "\n",
    "Of course, these models and their predictions can always be improved. You can find new datasets to add, or you can look to make the models better through feature selection, feature reduction or feature engineering. The possibilities are endless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoP",
   "language": "python",
   "name": "mop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
