{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score, max_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def prepare_data_for_prediction(new_data):\n",
    "    # Handle age ranges\n",
    "    new_data['age_numeric'] = new_data['age'].apply(age_to_numeric)\n",
    "    \n",
    "    # Feature engineering\n",
    "    new_data['sample_size_log'] = np.log1p(new_data['sample_size'])\n",
    "    \n",
    "    # Add safety likelihood feature\n",
    "    new_data['safety_likelihood'] = new_data.apply(safety_likelihood, axis=1)\n",
    "    \n",
    "    # Select features\n",
    "    X_new = new_data[numeric_features + categorical_features]\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "def make_predictions(model, new_data):\n",
    "    X_new = prepare_data_for_prediction(new_data)\n",
    "    predictions = model.predict(X_new)\n",
    "    return predictions\n",
    "\n",
    "def identify_high_risk_areas(data, threshold=0.7):\n",
    "    data['risk_score'] = data['predicted_likelihood'] * (1 - data['safety_likelihood'])\n",
    "    high_risk_areas = data[data['risk_score'] > threshold]\n",
    "    return high_risk_areas.sort_values('risk_score', ascending=False)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('HB1.csv')\n",
    "\n",
    "# Filter data for smoking behavior only\n",
    "data = data[data['behavior'] == 'Smoking']\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Handle age ranges\n",
    "def age_to_numeric(age_range):\n",
    "    if isinstance(age_range, str):\n",
    "        if '-' in age_range:\n",
    "            return int(age_range.split('-')[0])\n",
    "        elif '+' in age_range:\n",
    "            return int(age_range.replace('+', ''))\n",
    "        else:\n",
    "            return int(age_range)\n",
    "    return age_range\n",
    "\n",
    "data['age_numeric'] = data['age'].apply(age_to_numeric)\n",
    "\n",
    "# Feature engineering\n",
    "data['sample_size_log'] = np.log1p(data['sample_size'])\n",
    "\n",
    "# Add safety likelihood feature\n",
    "def safety_likelihood(row):\n",
    "    base_safety = 0.8\n",
    "    safety = base_safety - 0.1 if row['gender'] == 'Female' else base_safety\n",
    "    safety -= 0.05 if 'CBD' in str(row['location']) or 'Melbourne' in str(row['location']) else 0\n",
    "    safety -= 0.05 if row['age_numeric'] < 25 or row['age_numeric'] > 65 else 0\n",
    "    return max(0, min(safety, 1))\n",
    "\n",
    "data['safety_likelihood'] = data.apply(safety_likelihood, axis=1)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = ['age_numeric', 'sample_size', 'sample_size_log', 'safety_likelihood']\n",
    "categorical_features = ['gender', 'location']\n",
    "\n",
    "# Split features and target\n",
    "X = data[numeric_features + categorical_features]\n",
    "y = data['likelihood_percent']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', None)  # This will be set in the loop\n",
    "])\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'Linear Regression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Define parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'regressor__n_estimators': [50, 100, 200],\n",
    "        'regressor__max_depth': [None, 10, 20, 30]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'regressor__C': [0.1, 1, 10],\n",
    "        'regressor__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'Linear Regression': {}\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Set the regressor in the pipeline\n",
    "    full_pipeline.set_params(regressor=model)\n",
    "\n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(full_pipeline, param_grids[name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "    print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "    print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n",
    "    print(f\"Explained Variance Score: {explained_variance_score(y_test, y_pred)}\")\n",
    "    print(f\"Max Error: {max_error(y_test, y_pred)}\")\n",
    "\n",
    "    # Feature importance (only for Random Forest)\n",
    "    if name == 'Random Forest':\n",
    "        # Get feature names for numeric features\n",
    "        numeric_feature_names = numeric_features\n",
    "\n",
    "        # Get feature names for categorical features after one-hot encoding\n",
    "        onehot_encoder = best_model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_feature_names = onehot_encoder.get_feature_names_out(categorical_features).tolist()\n",
    "\n",
    "        # Combine all feature names\n",
    "        feature_names = numeric_feature_names + cat_feature_names\n",
    "\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.named_steps['regressor'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(feature_importance)\n",
    "\n",
    "\n",
    "# Analyze safety likelihood\n",
    "print(\"\\nSafety Likelihood Analysis:\")\n",
    "print(f\"Average safety likelihood: {data['safety_likelihood'].mean()}\")\n",
    "print(f\"Safety likelihood by gender:\")\n",
    "print(data.groupby('gender')['safety_likelihood'].mean())\n",
    "\n",
    "# Correlation between safety likelihood and smoking likelihood\n",
    "correlation = data['safety_likelihood'].corr(data['likelihood_percent'])\n",
    "print(f\"\\nCorrelation between safety likelihood and smoking likelihood: {correlation}\")\n",
    "\n",
    "# Analyze smoking likelihood by location and gender\n",
    "print(\"\\nSmoking Likelihood Analysis:\")\n",
    "smoking_by_location_gender = data.groupby(['location', 'gender'])['likelihood_percent'].mean().unstack()\n",
    "print(smoking_by_location_gender)\n",
    "\n",
    "# Identify high-risk areas (high smoking likelihood, low safety)\n",
    "data['risk_score'] = data['likelihood_percent'] * (1 - data['safety_likelihood'])\n",
    "high_risk_areas = data.groupby('location')['risk_score'].mean().sort_values(ascending=False).head(5)\n",
    "print(\"\\nTop 5 High-Risk Areas:\")\n",
    "print(high_risk_areas)\n",
    "\n",
    "# Select the best model (SVR in this case)\n",
    "best_model = grid_search.best_estimator_  # Make sure this is the GridSearchCV object for the SVR model\n",
    "\n",
    "# Example of making predictions on new data\n",
    "new_data = pd.DataFrame({\n",
    "    'age': ['25-34', '45-54', '65+'],\n",
    "    'gender': ['Male', 'Female', 'Male'],\n",
    "    'location': ['CBD', 'Suburban', 'Rural'],\n",
    "    'sample_size': [100, 150, 200]\n",
    "})\n",
    "\n",
    "predictions = make_predictions(best_model, new_data)\n",
    "new_data['predicted_likelihood'] = predictions\n",
    "\n",
    "print(\"\\nPredictions for new data:\")\n",
    "print(new_data)\n",
    "\n",
    "# Identify high-risk areas\n",
    "high_risk_areas = identify_high_risk_areas(new_data)\n",
    "print(\"\\nHigh-risk areas:\")\n",
    "print(high_risk_areas)\n",
    "\n",
    "# Add visualizations #Heat Map\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(smoking_by_location_gender, annot=True, cmap='YlOrRd')\n",
    "plt.title('Smoking Likelihood by Location and Gender')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance #Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the relationship between safety likelihood and smoking likelihood #scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='safety_likelihood', y='likelihood_percent', hue='gender', data=data)\n",
    "plt.title('Safety Likelihood vs Smoking Likelihood')\n",
    "plt.xlabel('Safety Likelihood')\n",
    "plt.ylabel('Smoking Likelihood (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of risk scores #histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['risk_score'], kde=True)\n",
    "plt.title('Distribution of Risk Scores')\n",
    "plt.xlabel('Risk Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize top 5 high-risk areas #bargraph\n",
    "plt.figure(figsize=(10, 6))\n",
    "high_risk_areas.plot(kind='bar')\n",
    "plt.title('Top 5 High-Risk Areas')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize model performance comparison\n",
    "model_names = list(models.keys())\n",
    "mse_scores = []\n",
    "\n",
    "for name in model_names:\n",
    "    pipeline = full_pipeline.set_params(regressor=models[name])\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_scores.append(-scores.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=model_names, y=mse_scores)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Squared Error (Cross-Validation)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions for new data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='location', y='predicted_likelihood', hue='gender', data=new_data)\n",
    "plt.title('Predicted Smoking Likelihood for New Data')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Predicted Likelihood (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize high-risk areas\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='location', y='risk_score', data=high_risk_areas)\n",
    "plt.title('Risk Scores for High-Risk Areas')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saha_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
