{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install runtime deps\n",
        "!pip install -q fastapi uvicorn nest_asyncio pyngrok\n",
        "\n",
        "#  mount Google Drive to load checkpoints from /content/drive/MyDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "CKPT_CNN  = \"/content/drive/MyDrive/cnn_model.pth\"\n",
        "CKPT_LSTM = \"/content/drive/MyDrive/lstm_model.pth\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7MH5LBI2FXA",
        "outputId": "fcc2609b-b856-47b1-e95d-647971b07d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# ---- Models  ----\n",
        "class EEG_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(14, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3)\n",
        "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1   = nn.Linear(64, 32)\n",
        "        self.fc2   = nn.Linear(32, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class EEG_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=14, hidden_size=64, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers  = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1  = nn.Linear(hidden_size, 32)\n",
        "        self.fc2  = nn.Linear(32, num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = F.relu(self.fc1(out[:, -1, :]))\n",
        "        return self.fc2(out)\n",
        "\n",
        "# ---- Normalization (per sample, per channel; same as eval) ----\n",
        "def normalize_per_sample(sample_14_T: np.ndarray) -> np.ndarray:\n",
        "    mean = sample_14_T.mean(axis=1, keepdims=True)\n",
        "    std  = sample_14_T.std(axis=1, keepdims=True)\n",
        "    std[std == 0] = 1\n",
        "    return (sample_14_T - mean) / std\n",
        "\n",
        "# ---- Model loading ----\n",
        "def load_cnn(ckpt_path: str) -> nn.Module:\n",
        "    m = EEG_CNN().to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "def load_lstm(ckpt_path: str) -> nn.Module:\n",
        "    m = EEG_LSTM().to(DEVICE)\n",
        "    m.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    m.eval()\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "0p9mH7qY2PFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- FastAPI ----------------\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"status\": \"ok\", \"message\": \"EEG API running. Visit /docs for Swagger UI.\"}\n",
        "\n",
        "@app.post(\"/predict_cnn\")\n",
        "async def predict_cnn(file: UploadFile = File(...)):\n",
        "    data = np.load(file.file)   # must be (14, T)\n",
        "    result = predict_model(cnn_model, data, \"CNN\")\n",
        "    return {\"model\": \"CNN\", \"prediction\": result}\n",
        "\n",
        "@app.post(\"/predict_lstm\")\n",
        "async def predict_lstm(file: UploadFile = File(...)):\n",
        "    data = np.load(file.file)   # must be (14, T)\n",
        "    result = predict_model(lstm_model, data, \"LSTM\")\n",
        "    return {\"model\": \"LSTM\", \"prediction\": result}\n"
      ],
      "metadata": {
        "id": "41P_TKyH6Z8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"330PuAWTfTyPYHXGy5iVtC1ajhx_2URNNdK974QAAXTpwG62C\""
      ],
      "metadata": {
        "id": "BDXeKjm63mRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "\n",
        "token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "if token:\n",
        "    ngrok.set_auth_token(token)\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start public tunnel to port 8000\n",
        "public_url = ngrok.connect(8000, \"http\")\n",
        "print(\" Public URL:\", public_url)\n",
        "\n",
        "# Run FastAPI app in a background thread\n",
        "def run_app():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_app, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "print(\" Server started. Open the Public URL above, then add /docs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOyxaVTr2YTJ",
        "outputId": "9b1d72d2-7a1f-4740-ab02-e1a9e6aceda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Public URL: NgrokTunnel: \"https://429d7e864b3f.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            " Server started. Open the Public URL above, then add /docs\n"
          ]
        }
      ]
    }
  ]
}