{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iTkuGxkyAER",
        "outputId": "ab8d0540-e18d-4bbc-9b39-ea6907654868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RpblWjyHpw",
        "outputId": "88279df1-b1cd-471f-e5ad-8ef76605ce2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BValkYsyMIr",
        "outputId": "7d7c8f63-c64e-4d74-99d3-584827c82c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/app.py\n",
        "class EEG_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(14, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3)\n",
        "        self.pool  = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1   = nn.Linear(64, 32)\n",
        "        self.fc2   = nn.Linear(32, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opc_ZvLVyTMz",
        "outputId": "bb3ed4e8-8b4c-4202-e43f-e087ca7f507f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/app.py\n",
        "class EEG_LSTM(nn.Module):\n",
        "    def __init__(self, input_size=14, hidden_size=64, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc1  = nn.Linear(hidden_size, 32)\n",
        "        self.fc2  = nn.Linear(32, num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
        "        c0 = torch.zeros(2, x.size(0), 64).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = F.relu(self.fc1(out[:,-1,:]))\n",
        "        return self.fc2(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKQAmZ3AyX9W",
        "outputId": "3a068b7b-28c9-45a0-cc42-0736953a5b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/app.py\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def normalize_per_sample(sample):\n",
        "    mean = sample.mean(axis=1, keepdims=True)\n",
        "    std  = sample.std(axis=1, keepdims=True)\n",
        "    std[std==0] = 1\n",
        "    return (sample - mean)/std\n",
        "\n",
        "def load_model(model_type, ckpt_path):\n",
        "    if model_type==\"CNN\":\n",
        "        m = EEG_CNN().to(DEVICE)\n",
        "    else:\n",
        "        m = EEG_LSTM().to(DEVICE)\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    m.load_state_dict(state)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "def predict(sample, model, model_type):\n",
        "    x = normalize_per_sample(sample).astype(np.float32)\n",
        "    if model_type==\"CNN\":\n",
        "        xt = torch.from_numpy(x).unsqueeze(0).to(DEVICE)      # [1,14,T]\n",
        "    else:\n",
        "        xt = torch.from_numpy(x).unsqueeze(0).permute(0,2,1).to(DEVICE)  # [1,T,14]\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(model(xt), dim=1).detach().cpu().numpy()[0]\n",
        "    return int(np.argmax(probs)), float(probs[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxTynhznyfUh",
        "outputId": "913550b0-d5df-4c90-e92f-347c44c8dda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/app.py\n",
        "st.title(\"üß† EEG Stress-Level Demo\")\n",
        "\n",
        "model_type = st.radio(\"Choose Model:\", [\"CNN\",\"LSTM\"])\n",
        "ckpt = st.text_input(\n",
        "    \"Model checkpoint path\",\n",
        "    value=\"/content/drive/MyDrive/cnn_model.pth\" if model_type==\"CNN\" else \"/content/drive/MyDrive/lstm_model.pth\",\n",
        "    help=\"Full path to your trained weights (.pth)\"\n",
        ")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload EEG sample (.npy)\", type=[\"npy\"])\n",
        "st.caption(\"Expected shape: (14, T). We z-score per channel over time.\")\n",
        "\n",
        "if uploaded and ckpt:\n",
        "    try:\n",
        "        arr = np.load(uploaded)\n",
        "        assert arr.ndim==2 and arr.shape[0]==14, \"Sample must be (14, T).\"\n",
        "        model = load_model(model_type, ckpt)\n",
        "        pred, prob = predict(arr, model, model_type)\n",
        "        label = \"High Stress\" if pred==1 else \"Low Stress\"\n",
        "        st.metric(\"Prediction\", label)\n",
        "        st.progress(prob)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo5-kfIzymz0",
        "outputId": "42998a2b-3fe4-4a03-eebd-5f699cd4c3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a /content/app.py\n",
        "\n",
        "# -------------------- Week 9: Dashboard --------------------\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.header(\"üìä Model Optimization Dashboard\")\n",
        "\n",
        "# Loads results CSV\n",
        "results_path = \"/content/drive/MyDrive/week9_results_all.csv\"\n",
        "df = pd.read_csv(results_path)\n",
        "\n",
        "# ---- Plot 1: Accuracy vs Latency ----\n",
        "fig1, ax1 = plt.subplots(figsize=(6,4))\n",
        "for model in [\"CNN\", \"LSTM\"]:\n",
        "    subset = df[df[\"Model\"] == model]\n",
        "    ax1.scatter(subset[\"Latency_ms\"], subset[\"Accuracy\"], label=model)\n",
        "    for _, row in subset.iterrows():\n",
        "        ax1.text(row[\"Latency_ms\"], row[\"Accuracy\"], row[\"Variant\"], fontsize=8)\n",
        "ax1.set_xlabel(\"Latency (ms)\")\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_title(\"Accuracy vs Latency\")\n",
        "ax1.legend()\n",
        "ax1.grid()\n",
        "st.pyplot(fig1)\n",
        "\n",
        "# ---- Plot 2: F1 vs Model Size ----\n",
        "fig2, ax2 = plt.subplots(figsize=(6,4))\n",
        "for model in [\"CNN\", \"LSTM\"]:\n",
        "    subset = df[df[\"Model\"] == model]\n",
        "    ax2.scatter(subset[\"Size_MB\"], subset[\"F1\"], label=model)\n",
        "    for _, row in subset.iterrows():\n",
        "        ax2.text(row[\"Size_MB\"], row[\"F1\"], row[\"Variant\"], fontsize=8)\n",
        "ax2.set_xlabel(\"Model Size (MB)\")\n",
        "ax2.set_ylabel(\"F1 Score\")\n",
        "ax2.set_title(\"F1 vs Model Size\")\n",
        "ax2.legend()\n",
        "ax2.grid()\n",
        "st.pyplot(fig2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPoM5SgbvcxA",
        "outputId": "09bf6634-677b-4ff4-dc15-5a5bd1f0ccf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to /content/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloudflared\n",
        "!streamlit run /content/app.py & npx cloudflared tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "strtDExO0HXw",
        "outputId": "79a757ca-01e5-4823-f0dd-8bb208cbce6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloudflared\n",
            "  Downloading cloudflared-1.0.0.2.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setuptools_scm (from cloudflared)\n",
            "  Downloading setuptools_scm-9.2.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from setuptools_scm->cloudflared) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from setuptools_scm->cloudflared) (75.2.0)\n",
            "Downloading setuptools_scm-9.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cloudflared\n",
            "  Building wheel for cloudflared (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cloudflared: filename=cloudflared-1.0.0.2-py3-none-any.whl size=2983 sha256=506524d2cca06f176db5d86caa3c49ff8c88905fc795b3e0bb0f363e97762e93\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ec/09/c3bcd3470be046ec77a9c0cb9d8bb6ceed49c831460878ab0a\n",
            "Successfully built cloudflared\n",
            "Installing collected packages: setuptools_scm, cloudflared\n",
            "Successfully installed cloudflared-1.0.0.2 setuptools_scm-9.2.0\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "cloudflared@0.7.1\n",
            "Ok to proceed? (y) \u001b[20G\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.16.248.137:8501\u001b[0m\n",
            "\u001b[0m\n",
            "y\n",
            "\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\u001b[90m2025-09-21T10:08:24Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-09-21T10:08:24Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m |  https://jpg-gsm-geology-consent.trycloudflare.com                                         |\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.9.0 (Checksum 4f48c9fc205ed7d200a35f24170e795171189b77a696e3f8b5704d07a6801957)\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: e9dc911c-7379-43ae-973c-6755c5aed819\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.57\n",
            "2025/09/21 10:08:28 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-09-21T10:08:28Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0md4c3c477-3042-45ab-a017-a13de3674f42 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.57 \u001b[36mlocation=\u001b[0mlax08 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}